# Ingestion å±‚ä½¿ç”¨æŒ‡å—

## ğŸ“¦ æ¦‚è¿°

Ingestion å±‚è´Ÿè´£ä»å„ç§æ•°æ®æºé‡‡é›†åŸå§‹æ•°æ®ï¼Œå¹¶è‡ªåŠ¨å®Œæˆï¼š
- **æ–‡ä»¶ä¸‹è½½** - ä»æ•°æ®æºä¸‹è½½åŸå§‹æ–‡æ¡£
- **MinIO ä¸Šä¼ ** - è‡ªåŠ¨ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨ï¼ˆBronze å±‚ï¼‰
- **å…ƒæ•°æ®è®°å½•** - è‡ªåŠ¨è®°å½•åˆ° PostgreSQL
- **æ–‡ä»¶éªŒè¯** - è‡ªåŠ¨è®¡ç®—å“ˆå¸Œã€æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

```
src/ingestion/
â”œâ”€â”€ base/                   # åŸºç¡€æ¨¡å—
â”‚   â”œâ”€â”€ base_crawler.py     # çˆ¬è™«åŸºç±»ï¼ˆé›†æˆ storage å±‚ï¼‰
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ a_share/                # Aè‚¡çˆ¬è™«
â”‚   â”œâ”€â”€ cninfo_crawler.py   # CNINFO çˆ¬è™«å®ç°
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ hk_stock/               # æ¸¯è‚¡çˆ¬è™«ï¼ˆå¾…å®ç°ï¼‰
â”œâ”€â”€ us_stock/               # ç¾è‚¡çˆ¬è™«ï¼ˆå¾…å®ç°ï¼‰
â””â”€â”€ __init__.py
```

### ç±»å±‚æ¬¡ç»“æ„

```
BaseCrawler (æŠ½è±¡åŸºç±»)
â”œâ”€â”€ é›†æˆ MinIOClientï¼ˆå¯¹è±¡å­˜å‚¨ï¼‰
â”œâ”€â”€ é›†æˆ PostgreSQLClientï¼ˆå…ƒæ•°æ®ï¼‰
â”œâ”€â”€ é›†æˆ PathManagerï¼ˆè·¯å¾„ç®¡ç†ï¼‰
â””â”€â”€ æä¾› crawl() å’Œ crawl_batch() æ–¹æ³•

CninfoAShareCrawler (Aè‚¡å®ç°)
â”œâ”€â”€ ç»§æ‰¿ BaseCrawler
â”œâ”€â”€ å®ç° _download_file() æ–¹æ³•
â”œâ”€â”€ å¤ç”¨ç°æœ‰ src/crawler/zh/main.py é€»è¾‘
â””â”€â”€ æ”¯æŒå•ä»»åŠ¡å’Œå¤šè¿›ç¨‹æ‰¹é‡çˆ¬å–
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…ä¾èµ–ï¼š

```bash
pip install requests beautifulsoup4 tqdm minio psycopg2-binary sqlalchemy pymilvus pydantic-settings
```

è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå‚è€ƒ `env.example`ï¼‰ï¼š

```bash
# MinIO
export MINIO_ENDPOINT=localhost:9000
export MINIO_ACCESS_KEY=admin
export MINIO_SECRET_KEY=admin123456
export MINIO_BUCKET=company-datalake

# PostgreSQL
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5432
export POSTGRES_DB=finnet
export POSTGRES_USER=finnet
export POSTGRES_PASSWORD=finnet123456
```

### 2. åŸºæœ¬ä½¿ç”¨

```python
from src.ingestion import CninfoAShareCrawler, CrawlTask
from src.common.constants import Market, DocType

# åˆ›å»ºçˆ¬è™«å®ä¾‹
crawler = CninfoAShareCrawler(
    enable_minio=True,      # å¯ç”¨ MinIO ä¸Šä¼ 
    enable_postgres=True,   # å¯ç”¨ PostgreSQL è®°å½•
    workers=4               # å¤šè¿›ç¨‹æ•°é‡
)

# åˆ›å»ºçˆ¬å–ä»»åŠ¡
task = CrawlTask(
    stock_code="000001",
    company_name="å¹³å®‰é“¶è¡Œ",
    market=Market.A_SHARE,
    doc_type=DocType.QUARTERLY_REPORT,
    year=2023,
    quarter=3
)

# æ‰§è¡Œçˆ¬å–
result = crawler.crawl(task)

# æ£€æŸ¥ç»“æœ
if result.success:
    print(f"âœ… æˆåŠŸï¼")
    print(f"æœ¬åœ°è·¯å¾„: {result.local_file_path}")
    print(f"MinIOè·¯å¾„: {result.minio_object_path}")
    print(f"æ•°æ®åº“ID: {result.document_id}")
else:
    print(f"âŒ å¤±è´¥ï¼š{result.error_message}")
```

---

## ğŸ“š è¯¦ç»†ç”¨æ³•

### å•ä»»åŠ¡çˆ¬å–

```python
from src.ingestion import CninfoAShareCrawler, CrawlTask
from src.common.constants import Market, DocType

# åˆ›å»ºçˆ¬è™«
crawler = CninfoAShareCrawler()

# åˆ›å»ºä»»åŠ¡
task = CrawlTask(
    stock_code="000001",
    company_name="å¹³å®‰é“¶è¡Œ",
    market=Market.A_SHARE,
    doc_type=DocType.QUARTERLY_REPORT,
    year=2023,
    quarter=3,
    metadata={"source": "manual"}  # è‡ªå®šä¹‰å…ƒæ•°æ®
)

# æ‰§è¡Œçˆ¬å–ï¼ˆè‡ªåŠ¨å®Œæˆä¸‹è½½ã€ä¸Šä¼ ã€è®°å½•ï¼‰
result = crawler.crawl(task)
```

### æ‰¹é‡çˆ¬å–ï¼ˆå¤šè¿›ç¨‹ï¼‰

```python
from src.ingestion import CninfoAShareCrawler, CrawlTask
from src.common.constants import Market, DocType

# åˆ›å»ºçˆ¬è™«ï¼ˆå¯ç”¨å¤šè¿›ç¨‹ï¼‰
crawler = CninfoAShareCrawler(workers=8)

# æ‰¹é‡åˆ›å»ºä»»åŠ¡
tasks = [
    CrawlTask(
        stock_code="000001",
        company_name="å¹³å®‰é“¶è¡Œ",
        market=Market.A_SHARE,
        doc_type=DocType.QUARTERLY_REPORT,
        year=2023,
        quarter=3
    ),
    CrawlTask(
        stock_code="600519",
        company_name="è´µå·èŒ…å°",
        market=Market.A_SHARE,
        doc_type=DocType.QUARTERLY_REPORT,
        year=2023,
        quarter=3
    ),
    # ... æ›´å¤šä»»åŠ¡
]

# æ‰¹é‡çˆ¬å–ï¼ˆ8ä¸ªè¿›ç¨‹å¹¶è¡Œï¼‰
results = crawler.crawl_batch(tasks)

# ç»Ÿè®¡ç»“æœ
success_count = sum(1 for r in results if r.success)
print(f"æˆåŠŸ: {success_count}/{len(tasks)}")
```

### ä» CSV æ–‡ä»¶æ‰¹é‡çˆ¬å–

```python
import csv
from src.ingestion import CninfoAShareCrawler, CrawlTask
from src.common.constants import Market, DocType

# è¯»å– CSV
tasks = []
with open("companies.csv", "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        task = CrawlTask(
            stock_code=row["code"],
            company_name=row["name"],
            market=Market.A_SHARE,
            doc_type=DocType.QUARTERLY_REPORT,
            year=int(row["year"]),
            quarter=int(row["quarter"])
        )
        tasks.append(task)

# æ‰¹é‡çˆ¬å–
crawler = CninfoAShareCrawler(workers=8)
results = crawler.crawl_batch(tasks)
```

### éªŒè¯çˆ¬å–ç»“æœ

```python
result = crawler.crawl(task)

# éªŒè¯ç»“æœ
is_valid, error_msg = crawler.validate_result(result)

if is_valid:
    print("âœ… éªŒè¯é€šè¿‡")
else:
    print(f"âŒ éªŒè¯å¤±è´¥ï¼š{error_msg}")
```

---

## ğŸ”§ é«˜çº§é…ç½®

### ç¦ç”¨ MinIO æˆ– PostgreSQL

```python
# ä»…ä¸‹è½½æ–‡ä»¶ï¼Œä¸ä¸Šä¼ åˆ° MinIO
crawler = CninfoAShareCrawler(
    enable_minio=False,
    enable_postgres=False
)
```

### æŒ‡å®šæ—§æ–‡ä»¶ç›®å½•ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰

```python
# å¦‚æœæ–‡ä»¶å·²å­˜åœ¨äºæ—§ç›®å½•ï¼Œè·³è¿‡ä¸‹è½½
crawler = CninfoAShareCrawler(
    old_pdf_dir="/path/to/old/reports"
)
```

### è‡ªå®šä¹‰æ–‡æ¡£ç±»å‹

```python
from src.common.constants import DocType

# å¹´æŠ¥
task = CrawlTask(
    stock_code="000001",
    company_name="å¹³å®‰é“¶è¡Œ",
    market=Market.A_SHARE,
    doc_type=DocType.ANNUAL_REPORT,  # å¹´æŠ¥
    year=2023,
    quarter=None  # å¹´æŠ¥ä¸éœ€è¦å­£åº¦
)

# åŠå¹´æŠ¥
task = CrawlTask(
    stock_code="000001",
    company_name="å¹³å®‰é“¶è¡Œ",
    market=Market.A_SHARE,
    doc_type=DocType.INTERIM_REPORT,  # åŠå¹´æŠ¥
    year=2023,
    quarter=2  # ç¬¬äºŒå­£åº¦ = åŠå¹´æŠ¥
)

# å­£æŠ¥
task = CrawlTask(
    stock_code="000001",
    company_name="å¹³å®‰é“¶è¡Œ",
    market=Market.A_SHARE,
    doc_type=DocType.QUARTERLY_REPORT,  # å­£æŠ¥
    year=2023,
    quarter=1  # ç¬¬ä¸€å­£åº¦
)
```

---

## ğŸ“Š æ•°æ®æµ

```
1. ç”¨æˆ·åˆ›å»º CrawlTask
   â†“
2. CninfoAShareCrawler._download_file()
   - è°ƒç”¨ç°æœ‰ src/crawler/zh/main.py é€»è¾‘
   - ä¸‹è½½ PDF åˆ°ä¸´æ—¶ç›®å½•
   â†“
3. BaseCrawler.crawl()
   - è®¡ç®—æ–‡ä»¶å“ˆå¸Œå’Œå¤§å°
   - ç”Ÿæˆ MinIO è·¯å¾„
   â†“
4. MinIOClient.upload_file()
   - ä¸Šä¼ åˆ° Bronze å±‚
   - è·¯å¾„: bronze/a_share/quarterly_reports/2023/Q3/000001/000001_2023_Q3.pdf
   â†“
5. PostgreSQLClient + crud.create_document()
   - è®°å½•æ–‡æ¡£å…ƒæ•°æ®
   - çŠ¶æ€: crawled
   â†“
6. è¿”å› CrawlResult
   - success: True
   - local_file_path: æœ¬åœ°è·¯å¾„
   - minio_object_path: MinIO å¯¹è±¡è·¯å¾„
   - document_id: æ•°æ®åº“ID
   - file_size: æ–‡ä»¶å¤§å°
   - file_hash: æ–‡ä»¶å“ˆå¸Œ
```

---

## ğŸ—ƒï¸ å­˜å‚¨è·¯å¾„è§„èŒƒ

### MinIO è·¯å¾„

éµå¾ª plan.md 5.2 è§„èŒƒï¼š

```
bronze/a_share/quarterly_reports/2023/Q3/000001/000001_2023_Q3.pdf
â””â”€â”¬â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”¬â”˜ â””â”¬â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
  â”‚      â”‚             â”‚           â”‚   â”‚     â”‚            â”‚
  â”‚      â”‚             â”‚           â”‚   â”‚     â”‚            â””â”€ æ–‡ä»¶å
  â”‚      â”‚             â”‚           â”‚   â”‚     â””â”€ è‚¡ç¥¨ä»£ç ç›®å½•
  â”‚      â”‚             â”‚           â”‚   â””â”€ å­£åº¦ç›®å½•
  â”‚      â”‚             â”‚           â””â”€ å¹´ä»½ç›®å½•
  â”‚      â”‚             â””â”€ æ–‡æ¡£ç±»å‹ç›®å½•
  â”‚      â””â”€ å¸‚åœºç›®å½•
  â””â”€ æ•°æ®å±‚æ¬¡ï¼ˆBronze åŸå§‹æ•°æ®ï¼‰
```

### PostgreSQL è¡¨ç»“æ„

ä¸»è¦è¡¨ï¼š

- **documents** - æ–‡æ¡£å…ƒæ•°æ®
  - id, stock_code, company_name, market, doc_type
  - year, quarter, minio_object_path
  - status, crawled_at, file_size, file_hash
  - metadata (JSONB)

- **document_chunks** - æ–‡æ¡£åˆ†å—ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
- **crawl_tasks** - çˆ¬å–ä»»åŠ¡è®°å½•
- **validation_logs** - éªŒè¯æ—¥å¿—

---

## âš™ï¸ æ€§èƒ½ä¼˜åŒ–

### å¤šè¿›ç¨‹é…ç½®

```python
# æ ¹æ® CPU æ ¸å¿ƒæ•°å’Œç½‘ç»œå¸¦å®½è°ƒæ•´
crawler = CninfoAShareCrawler(workers=8)  # 8ä¸ªå¹¶è¡Œè¿›ç¨‹

# å»ºè®®ï¼š
# - CPU å¯†é›†å‹ï¼šworkers = CPUæ ¸å¿ƒæ•°
# - I/O å¯†é›†å‹ï¼šworkers = CPUæ ¸å¿ƒæ•° * 2
# - ç½‘ç»œçˆ¬è™«ï¼šworkers = 4-8ï¼ˆé¿å…è¢«å°IPï¼‰
```

### æ–­ç‚¹ç»­çˆ¬

ç°æœ‰çˆ¬è™«å·²æ”¯æŒ checkpoint æœºåˆ¶ï¼Œè‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½çš„æ–‡ä»¶ã€‚

---

## ğŸ› æ•…éšœæ’æŸ¥

### MinIO è¿æ¥å¤±è´¥

```python
# æ£€æŸ¥ MinIO æ˜¯å¦è¿è¡Œ
# docker ps | grep minio

# æµ‹è¯•è¿æ¥
from src.storage.object_store.minio_client import MinIOClient
minio_client = MinIOClient()
files = minio_client.list_files()
```

### PostgreSQL è¿æ¥å¤±è´¥

```python
# æ£€æŸ¥ PostgreSQL æ˜¯å¦è¿è¡Œ
# docker ps | grep postgres

# æµ‹è¯•è¿æ¥
from src.storage.metadata.postgres_client import get_postgres_client
pg_client = get_postgres_client()
pg_client.test_connection()
```

### çˆ¬å–å¤±è´¥

```python
# å¯ç”¨è°ƒè¯•æ—¥å¿—
import logging
logging.basicConfig(level=logging.DEBUG)

# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
result = crawler.crawl(task)
if not result.success:
    print(result.error_message)
    print(result.metadata)  # æŸ¥çœ‹é¢å¤–ä¿¡æ¯
```

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹

å‚è€ƒ `examples/ingestion_demo.py`ï¼š

```bash
# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
python examples/ingestion_demo.py --mode all

# ä»…è¿è¡Œå•ä»»åŠ¡ç¤ºä¾‹
python examples/ingestion_demo.py --mode single

# ä»…è¿è¡Œæ‰¹é‡ä»»åŠ¡ç¤ºä¾‹
python examples/ingestion_demo.py --mode batch

# ä»…æ£€æŸ¥å­˜å‚¨è¿æ¥
python examples/ingestion_demo.py --mode storage
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [STORAGE_LAYER_GUIDE.md](./STORAGE_LAYER_GUIDE.md) - Storage å±‚è¯¦ç»†æ–‡æ¡£
- [ARCHITECTURE.md](./ARCHITECTURE.md) - ç³»ç»Ÿæ¶æ„è®¾è®¡
- [plan.md](../plan.md) - å®Œæ•´è®¾è®¡è§„èŒƒ

---

*æœ€åæ›´æ–°: 2025-01-13*
