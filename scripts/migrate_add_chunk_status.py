#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“è¿ç§»è„šæœ¬ï¼šä¸º DocumentChunk æ·»åŠ  status å­—æ®µå’Œå¤±è´¥è¿½è¸ªæœºåˆ¶

å˜æ›´å†…å®¹ï¼š
1. æ·»åŠ  status å­—æ®µï¼ˆVARCHAR(50)ï¼Œé»˜è®¤ 'pending'ï¼‰
2. æ·»åŠ  vectorization_error å­—æ®µï¼ˆTEXTï¼Œå¯ç©ºï¼‰
3. æ·»åŠ  vectorization_retry_count å­—æ®µï¼ˆINTEGERï¼Œé»˜è®¤ 0ï¼‰
4. åˆ›å»º idx_chunk_status ç´¢å¼•
5. åˆå§‹åŒ–ç°æœ‰æ•°æ®çš„ statusï¼ˆå·²å‘é‡åŒ–çš„è®¾ä¸º 'vectorized'ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
  # è¯•è¿è¡Œï¼ˆé»˜è®¤ï¼‰
  python scripts/migrate_add_chunk_status.py

  # å®é™…æ‰§è¡Œ
  python scripts/migrate_add_chunk_status.py --force
"""

import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.storage.metadata.postgres_client import get_postgres_client
from src.common.logger import get_logger
from sqlalchemy import text

logger = get_logger(__name__)


def check_migration_status(session):
    """æ£€æŸ¥è¿ç§»çŠ¶æ€"""
    logger.info("=" * 80)
    logger.info("æ£€æŸ¥ document_chunks è¡¨è¿ç§»çŠ¶æ€")
    logger.info("=" * 80)

    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    result = session.execute(text("""
        SELECT table_name
        FROM information_schema.tables
        WHERE table_name = 'document_chunks'
    """)).fetchone()

    if not result:
        logger.error("âŒ document_chunks è¡¨ä¸å­˜åœ¨")
        return False, None

    logger.info("âœ“ document_chunks è¡¨å­˜åœ¨")

    # æ£€æŸ¥å­—æ®µæ˜¯å¦å·²å­˜åœ¨
    columns = session.execute(text("""
        SELECT column_name, data_type, column_default
        FROM information_schema.columns
        WHERE table_name = 'document_chunks'
        AND column_name IN ('status', 'vectorization_error', 'vectorization_retry_count')
        ORDER BY column_name
    """)).fetchall()

    existing_columns = {col[0] for col in columns}

    logger.info(f"\nç°æœ‰å­—æ®µ:")
    for col in columns:
        logger.info(f"  {col[0]}: {col[1]} (é»˜è®¤å€¼: {col[2]})")

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
    index_exists = session.execute(text("""
        SELECT indexname
        FROM pg_indexes
        WHERE tablename = 'document_chunks'
        AND indexname = 'idx_chunk_status'
    """)).fetchone()

    logger.info(f"\nidx_chunk_status ç´¢å¼•: {'å­˜åœ¨' if index_exists else 'ä¸å­˜åœ¨'}")

    # ç»Ÿè®¡å½“å‰æ•°æ®çŠ¶æ€
    result = session.execute(text("""
        SELECT
            COUNT(*) as total,
            COUNT(vectorized_at) as has_vectorized_at
        FROM document_chunks
    """)).fetchone()

    total_chunks = result[0] if result else 0
    vectorized_chunks = result[1] if result else 0

    logger.info(f"\nå½“å‰æ•°æ®çŠ¶æ€:")
    logger.info(f"  æ€»åˆ†å—æ•°: {total_chunks:,}")
    logger.info(f"  å·²å‘é‡åŒ–: {vectorized_chunks:,} (vectorized_at IS NOT NULL)")
    logger.info(f"  å¾…å‘é‡åŒ–: {total_chunks - vectorized_chunks:,}")

    # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿ç§»
    needs_migration = (
        'status' not in existing_columns or
        'vectorization_error' not in existing_columns or
        'vectorization_retry_count' not in existing_columns or
        not index_exists
    )

    return True, {
        'needs_migration': needs_migration,
        'has_status': 'status' in existing_columns,
        'has_error': 'vectorization_error' in existing_columns,
        'has_retry': 'vectorization_retry_count' in existing_columns,
        'has_index': bool(index_exists),
        'total_chunks': total_chunks,
        'vectorized_chunks': vectorized_chunks,
    }


def migrate_add_fields(session, dry_run=True):
    """æ·»åŠ æ–°å­—æ®µ"""
    logger.info("=" * 80)
    logger.info("æ­¥éª¤ 1: æ·»åŠ æ–°å­—æ®µ")
    logger.info("=" * 80)

    fields_to_add = [
        ('status', "VARCHAR(50) DEFAULT 'pending'", "å‘é‡åŒ–çŠ¶æ€"),
        ('vectorization_error', "TEXT", "å‘é‡åŒ–å¤±è´¥åŸå› "),
        ('vectorization_retry_count', "INTEGER DEFAULT 0", "å‘é‡åŒ–é‡è¯•æ¬¡æ•°"),
    ]

    for field_name, field_def, field_desc in fields_to_add:
        # æ£€æŸ¥å­—æ®µæ˜¯å¦å·²å­˜åœ¨
        result = session.execute(text("""
            SELECT column_name
            FROM information_schema.columns
            WHERE table_name = 'document_chunks'
            AND column_name = :field_name
        """), {"field_name": field_name}).fetchone()

        if result:
            logger.info(f"â­ï¸  å­—æ®µ {field_name} å·²å­˜åœ¨ï¼Œè·³è¿‡")
            continue

        if dry_run:
            logger.info(f"ğŸ” DRY RUN: å°†æ·»åŠ å­—æ®µ {field_name} ({field_desc})")
            logger.info(f"   SQL: ALTER TABLE document_chunks ADD COLUMN {field_name} {field_def}")
        else:
            logger.info(f"æ­£åœ¨æ·»åŠ å­—æ®µ {field_name} ({field_desc})...")
            try:
                session.execute(text(f"""
                    ALTER TABLE document_chunks
                    ADD COLUMN IF NOT EXISTS {field_name} {field_def}
                """))
                session.commit()
                logger.info(f"âœ“ å­—æ®µ {field_name} æ·»åŠ æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ æ·»åŠ å­—æ®µ {field_name} å¤±è´¥: {e}")
                session.rollback()
                return False

    return True


def migrate_create_index(session, dry_run=True):
    """åˆ›å»ºç´¢å¼•"""
    logger.info("=" * 80)
    logger.info("æ­¥éª¤ 2: åˆ›å»ºç´¢å¼•")
    logger.info("=" * 80)

    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
    result = session.execute(text("""
        SELECT indexname
        FROM pg_indexes
        WHERE tablename = 'document_chunks'
        AND indexname = 'idx_chunk_status'
    """)).fetchone()

    if result:
        logger.info("â­ï¸  idx_chunk_status ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡")
        return True

    if dry_run:
        logger.info("ğŸ” DRY RUN: å°†åˆ›å»ºç´¢å¼• idx_chunk_status")
        logger.info("   SQL: CREATE INDEX idx_chunk_status ON document_chunks(status)")
    else:
        logger.info("æ­£åœ¨åˆ›å»ºç´¢å¼• idx_chunk_status...")
        try:
            session.execute(text("""
                CREATE INDEX IF NOT EXISTS idx_chunk_status
                ON document_chunks(status)
            """))
            session.commit()
            logger.info("âœ“ ç´¢å¼• idx_chunk_status åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            session.rollback()
            return False

    return True


def migrate_initialize_data(session, dry_run=True):
    """åˆå§‹åŒ–ç°æœ‰æ•°æ®çš„ status"""
    logger.info("=" * 80)
    logger.info("æ­¥éª¤ 3: åˆå§‹åŒ–ç°æœ‰æ•°æ®")
    logger.info("=" * 80)

    # æ£€æŸ¥ status å­—æ®µæ˜¯å¦å­˜åœ¨
    result = session.execute(text("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'document_chunks'
        AND column_name = 'status'
    """)).fetchone()

    if not result:
        if dry_run:
            # åœ¨ dry-run æ¨¡å¼ä¸‹ï¼Œä¼°ç®—éœ€è¦æ›´æ–°çš„æ•°æ®
            result = session.execute(text("""
                SELECT COUNT(*)
                FROM document_chunks
                WHERE vectorized_at IS NOT NULL
            """)).fetchone()
            need_update = result[0] if result else 0
            logger.info(f"ğŸ” DRY RUN: å°†æ›´æ–° {need_update:,} æ¡è®°å½•")
            logger.info("   å°†å·²å‘é‡åŒ–çš„è®°å½• (vectorized_at IS NOT NULL) çš„ status è®¾ä¸º 'vectorized'")
            return True
        else:
            logger.error("âŒ status å­—æ®µä¸å­˜åœ¨ï¼Œè¯·å…ˆæ‰§è¡Œæ­¥éª¤ 1")
            return False

    # ç»Ÿè®¡éœ€è¦åˆå§‹åŒ–çš„æ•°æ®
    result = session.execute(text("""
        SELECT COUNT(*)
        FROM document_chunks
        WHERE vectorized_at IS NOT NULL
        AND (status IS NULL OR status = 'pending')
    """)).fetchone()

    need_update = result[0] if result else 0

    if need_update == 0:
        logger.info("â­ï¸  æ‰€æœ‰æ•°æ®å·²æ­£ç¡®åˆå§‹åŒ–ï¼Œè·³è¿‡")
        return True

    if dry_run:
        logger.info(f"ğŸ” DRY RUN: å°†æ›´æ–° {need_update:,} æ¡è®°å½•")
        logger.info("   å°†å·²å‘é‡åŒ–çš„è®°å½• (vectorized_at IS NOT NULL) çš„ status è®¾ä¸º 'vectorized'")
    else:
        logger.info(f"æ­£åœ¨åˆå§‹åŒ– {need_update:,} æ¡è®°å½•...")
        try:
            session.execute(text("""
                UPDATE document_chunks
                SET status = 'vectorized'
                WHERE vectorized_at IS NOT NULL
                AND (status IS NULL OR status = 'pending')
            """))
            session.commit()
            logger.info(f"âœ“ æˆåŠŸæ›´æ–° {need_update:,} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–æ•°æ®å¤±è´¥: {e}")
            session.rollback()
            return False

    return True


def verify_migration(session):
    """éªŒè¯è¿ç§»ç»“æœ"""
    logger.info("=" * 80)
    logger.info("éªŒè¯è¿ç§»ç»“æœ")
    logger.info("=" * 80)

    # æ£€æŸ¥å­—æ®µ
    columns = session.execute(text("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'document_chunks'
        AND column_name IN ('status', 'vectorization_error', 'vectorization_retry_count')
        ORDER BY column_name
    """)).fetchall()

    logger.info(f"\nå­—æ®µæ£€æŸ¥:")
    for col in columns:
        logger.info(f"  âœ“ {col[0]}")

    # æ£€æŸ¥ç´¢å¼•
    index = session.execute(text("""
        SELECT indexname
        FROM pg_indexes
        WHERE tablename = 'document_chunks'
        AND indexname = 'idx_chunk_status'
    """)).fetchone()

    logger.info(f"\nç´¢å¼•æ£€æŸ¥:")
    if index:
        logger.info(f"  âœ“ idx_chunk_status")
    else:
        logger.warning(f"  âš ï¸  idx_chunk_status ä¸å­˜åœ¨")

    # ç»Ÿè®¡æ•°æ®åˆ†å¸ƒ
    result = session.execute(text("""
        SELECT
            status,
            COUNT(*) as count
        FROM document_chunks
        GROUP BY status
        ORDER BY status
    """)).fetchall()

    logger.info(f"\næ•°æ®åˆ†å¸ƒ:")
    total = 0
    for status, count in result:
        logger.info(f"  {status or 'NULL'}: {count:,}")
        total += count
    logger.info(f"  æ€»è®¡: {total:,}")

    # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
    result = session.execute(text("""
        SELECT COUNT(*)
        FROM document_chunks
        WHERE vectorized_at IS NOT NULL
        AND status != 'vectorized'
    """)).fetchone()

    inconsistent = result[0] if result else 0
    if inconsistent > 0:
        logger.warning(f"\nâš ï¸  æ•°æ®ä¸ä¸€è‡´: {inconsistent:,} æ¡è®°å½• vectorized_at ä¸ä¸ºç©ºä½† status ä¸æ˜¯ 'vectorized'")
    else:
        logger.info(f"\nâœ“ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ä¸º document_chunks æ·»åŠ  status å­—æ®µçš„æ•°æ®åº“è¿ç§»å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¯•è¿è¡Œï¼ˆé»˜è®¤ï¼‰- åªæ£€æŸ¥ä¸æ‰§è¡Œ
  python scripts/migrate_add_chunk_status.py

  # å®é™…æ‰§è¡Œè¿ç§»
  python scripts/migrate_add_chunk_status.py --force
        """
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=True,
        help="è¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…æ‰§è¡Œï¼Œé»˜è®¤å¯ç”¨ï¼‰"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶æ‰§è¡Œè¿ç§»ï¼ˆç¦ç”¨è¯•è¿è¡Œï¼‰"
    )

    args = parser.parse_args()

    dry_run = not args.force

    try:
        logger.info("=" * 80)
        logger.info("DocumentChunk Status å­—æ®µè¿ç§»å·¥å…·")
        logger.info("=" * 80)
        logger.info(f"æ¨¡å¼: {'ğŸ” DRY RUN (è¯•è¿è¡Œ)' if dry_run else 'â–¶ï¸  å®é™…æ‰§è¡Œ'}")
        logger.info(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("=" * 80)
        logger.info("")

        pg_client = get_postgres_client()

        with pg_client.get_session() as session:
            # æ£€æŸ¥è¿ç§»çŠ¶æ€
            table_exists, status = check_migration_status(session)

            if not table_exists:
                return 1

            if not status['needs_migration']:
                logger.info("")
                logger.info("=" * 80)
                logger.info("âœ“ æ‰€æœ‰è¿ç§»å·²å®Œæˆï¼Œæ— éœ€æ‰§è¡Œ")
                logger.info("=" * 80)
                return 0

            logger.info("")

            # æ‰§è¡Œè¿ç§»æ­¥éª¤
            # 1. æ·»åŠ å­—æ®µ
            if not migrate_add_fields(session, dry_run=dry_run):
                return 1
            logger.info("")

            # 2. åˆ›å»ºç´¢å¼•
            if not migrate_create_index(session, dry_run=dry_run):
                return 1
            logger.info("")

            # 3. åˆå§‹åŒ–æ•°æ®
            if not migrate_initialize_data(session, dry_run=dry_run):
                return 1
            logger.info("")

            # 4. éªŒè¯
            if not dry_run:
                verify_migration(session)

        # è¾“å‡ºæ€»ç»“
        logger.info("")
        logger.info("=" * 80)

        if dry_run:
            logger.info("ğŸ’¡ è¯•è¿è¡Œå®Œæˆ - æ²¡æœ‰å®é™…æ‰§è¡Œè¿ç§»")
            logger.info("")
            logger.info("è¦æ‰§è¡Œå®é™…è¿ç§»ï¼Œè¯·è¿è¡Œ:")
            logger.info("  python scripts/migrate_add_chunk_status.py --force")
            logger.info("")
            logger.info("è¿ç§»å†…å®¹:")
            logger.info("  1. æ·»åŠ  status å­—æ®µ (VARCHAR(50), é»˜è®¤ 'pending')")
            logger.info("  2. æ·»åŠ  vectorization_error å­—æ®µ (TEXT)")
            logger.info("  3. æ·»åŠ  vectorization_retry_count å­—æ®µ (INTEGER, é»˜è®¤ 0)")
            logger.info("  4. åˆ›å»º idx_chunk_status ç´¢å¼•")
            logger.info("  5. åˆå§‹åŒ–ç°æœ‰æ•°æ®çŠ¶æ€")
        else:
            logger.info("âœ… è¿ç§»æˆåŠŸå®Œæˆï¼")
            logger.info("")
            logger.info("åç»­æ­¥éª¤:")
            logger.info("  1. é‡å¯åº”ç”¨æœåŠ¡ä»¥ä½¿ç”¨æ–°å­—æ®µ")
            logger.info("  2. è¿è¡Œ vectorize_chunks_job ä¼šè‡ªåŠ¨é‡è¯•å¤±è´¥çš„åˆ†å—")
            logger.info("  3. æŸ¥è¯¢å¤±è´¥åˆ†å—:")
            logger.info("     SELECT * FROM document_chunks WHERE status = 'failed'")

        logger.info("=" * 80)

        return 0

    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸  æ“ä½œå·²è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        logger.error(f"\n\nâŒ è¿ç§»å¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
