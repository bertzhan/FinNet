# -*- coding: utf-8 -*-
"""
å‘é‡åŒ–ä½œä¸šæµ‹è¯•
æµ‹è¯•å‘é‡åŒ–æœåŠ¡çš„å„ä¸ªç»„ä»¶å’Œå®Œæ•´æµç¨‹
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

from src.processing.ai.embedding.bge_embedder import get_embedder
from src.processing.ai.embedding.vectorizer import get_vectorizer
from src.storage.metadata.postgres_client import get_postgres_client
from src.storage.metadata import crud
from src.storage.metadata.models import DocumentChunk, Document
from src.storage.vector.milvus_client import get_milvus_client
from src.common.constants import MilvusCollection


def test_embedder():
    """æµ‹è¯• Embedder æœåŠ¡"""
    print("=" * 60)
    print("1. æµ‹è¯• BGE Embedder æœåŠ¡")
    print("=" * 60)
    print()
    
    try:
        embedder = get_embedder()
        print(f"âœ… Embedder åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æ¨¡å‹åç§°: {embedder.get_model_name()}")
        print(f"   å‘é‡ç»´åº¦: {embedder.get_model_dim()}")
        print()
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬å‘é‡åŒ–
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        print(f"æµ‹è¯•å•ä¸ªæ–‡æœ¬å‘é‡åŒ–: '{test_text}'")
        vector = embedder.embed_text(test_text)
        print(f"âœ… å‘é‡åŒ–æˆåŠŸï¼Œç»´åº¦: {len(vector)}")
        print(f"   å‰5ä¸ªå€¼: {vector[:5]}")
        print()
        
        # æµ‹è¯•æ‰¹é‡å‘é‡åŒ–
        test_texts = [
            "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
            "è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æœ¬",
            "è¿™æ˜¯ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æœ¬"
        ]
        print(f"æµ‹è¯•æ‰¹é‡å‘é‡åŒ–: {len(test_texts)} ä¸ªæ–‡æœ¬")
        vectors = embedder.embed_batch(test_texts)
        print(f"âœ… æ‰¹é‡å‘é‡åŒ–æˆåŠŸï¼Œè¿”å› {len(vectors)} ä¸ªå‘é‡")
        print(f"   æ¯ä¸ªå‘é‡ç»´åº¦: {len(vectors[0]) if vectors else 0}")
        print()
        
        return True
    except Exception as e:
        print(f"âŒ Embedder æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def check_unvectorized_chunks():
    """æ£€æŸ¥æœªå‘é‡åŒ–çš„åˆ†å—"""
    print("=" * 60)
    print("2. æ£€æŸ¥æœªå‘é‡åŒ–çš„åˆ†å—")
    print("=" * 60)
    print()
    
    pg_client = get_postgres_client()
    
    try:
        with pg_client.get_session() as session:
            # æŸ¥æ‰¾æœªå‘é‡åŒ–çš„åˆ†å—
            query = session.query(DocumentChunk).join(
                Document, DocumentChunk.document_id == Document.id
            ).filter(
                DocumentChunk.vector_id.is_(None)
            )
            
            total_count = query.count()
            chunks = query.limit(10).all()
            
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   æœªå‘é‡åŒ–åˆ†å—æ€»æ•°: {total_count}")
            print(f"   æ˜¾ç¤ºå‰ {len(chunks)} ä¸ªåˆ†å—")
            print()
            
            if not chunks:
                print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æœªå‘é‡åŒ–çš„åˆ†å—")
                print("   å¯èƒ½åŸå› ï¼š")
                print("   - æ‰€æœ‰åˆ†å—éƒ½å·²å‘é‡åŒ–")
                print("   - è¿˜æ²¡æœ‰åˆ†å—æ•°æ®")
                return []
            
            print("å‰10ä¸ªæœªå‘é‡åŒ–åˆ†å—:")
            for i, chunk in enumerate(chunks, 1):
                doc = session.query(Document).filter(
                    Document.id == chunk.document_id
                ).first()
                print(f"   {i}. chunk_id={chunk.id}")
                print(f"      document_id={chunk.document_id}")
                print(f"      stock_code={doc.stock_code if doc else 'N/A'}")
                print(f"      chunk_index={chunk.chunk_index}")
                print(f"      text_preview={chunk.chunk_text[:50]}...")
                print()
            
            return chunks
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def test_vectorizer_single(chunk_ids):
    """æµ‹è¯•å‘é‡åŒ–å•ä¸ªåˆ†å—"""
    print("=" * 60)
    print("3. æµ‹è¯•å‘é‡åŒ–å•ä¸ªåˆ†å—")
    print("=" * 60)
    print()
    
    if not chunk_ids:
        print("âš ï¸  æ²¡æœ‰å¯æµ‹è¯•çš„åˆ†å—ï¼Œè·³è¿‡")
        return False
    
    try:
        vectorizer = get_vectorizer()
        print(f"âœ… Vectorizer åˆå§‹åŒ–æˆåŠŸ")
        print()
        
        # æµ‹è¯•å‘é‡åŒ–å•ä¸ªåˆ†å—
        test_chunk_id = chunk_ids[0].id
        print(f"æµ‹è¯•å‘é‡åŒ–åˆ†å—: {test_chunk_id}")
        
        result = vectorizer.vectorize_chunks([test_chunk_id])
        
        print(f"âœ… å‘é‡åŒ–å®Œæˆ")
        print(f"   æˆåŠŸæ•°é‡: {result.get('vectorized_count', 0)}")
        print(f"   å¤±è´¥æ•°é‡: {result.get('failed_count', 0)}")
        print(f"   å¤±è´¥åˆ†å—: {result.get('failed_chunks', [])}")
        print()
        
        # éªŒè¯æ•°æ®åº“æ›´æ–°
        pg_client = get_postgres_client()
        with pg_client.get_session() as session:
            chunk = session.query(DocumentChunk).filter(
                DocumentChunk.id == test_chunk_id
            ).first()
            
            if chunk and chunk.vector_id:
                print(f"âœ… æ•°æ®åº“æ›´æ–°æˆåŠŸ")
                print(f"   vector_id: {chunk.vector_id}")
                print(f"   embedding_model: {chunk.embedding_model}")
                print(f"   vectorized_at: {chunk.vectorized_at}")
            else:
                print(f"âš ï¸  æ•°æ®åº“æœªæ›´æ–°ï¼ˆå¯èƒ½å‘é‡åŒ–å¤±è´¥ï¼‰")
        print()
        
        return result.get('vectorized_count', 0) > 0
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_vectorizer_batch(chunk_ids):
    """æµ‹è¯•æ‰¹é‡å‘é‡åŒ–"""
    print("=" * 60)
    print("4. æµ‹è¯•æ‰¹é‡å‘é‡åŒ–")
    print("=" * 60)
    print()
    
    if not chunk_ids or len(chunk_ids) < 2:
        print("âš ï¸  åˆ†å—æ•°é‡ä¸è¶³ï¼Œè·³è¿‡æ‰¹é‡æµ‹è¯•")
        return False
    
    try:
        vectorizer = get_vectorizer()
        
        # æµ‹è¯•æ‰¹é‡å‘é‡åŒ–ï¼ˆæœ€å¤š5ä¸ªï¼‰
        test_chunk_ids = [chunk.id for chunk in chunk_ids[:5]]
        print(f"æµ‹è¯•æ‰¹é‡å‘é‡åŒ–: {len(test_chunk_ids)} ä¸ªåˆ†å—")
        
        result = vectorizer.vectorize_chunks(test_chunk_ids)
        
        print(f"âœ… æ‰¹é‡å‘é‡åŒ–å®Œæˆ")
        print(f"   æˆåŠŸæ•°é‡: {result.get('vectorized_count', 0)}")
        print(f"   å¤±è´¥æ•°é‡: {result.get('failed_count', 0)}")
        print(f"   å¤±è´¥åˆ†å—: {result.get('failed_chunks', [])}")
        print()
        
        return result.get('vectorized_count', 0) > 0
    except Exception as e:
        print(f"âŒ æ‰¹é‡å‘é‡åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_milvus_collection():
    """æµ‹è¯• Milvus Collection"""
    print("=" * 60)
    print("5. æµ‹è¯• Milvus Collection")
    print("=" * 60)
    print()
    
    try:
        milvus_client = get_milvus_client()
        collection_name = MilvusCollection.DOCUMENTS
        
        # æ£€æŸ¥ Collection æ˜¯å¦å­˜åœ¨
        collection = milvus_client.get_collection(collection_name)
        
        if collection:
            print(f"âœ… Collection å­˜åœ¨: {collection_name}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = milvus_client.get_collection_stats(collection_name)
            print(f"   å‘é‡æ•°é‡: {stats.get('row_count', 0)}")
            print()
        else:
            print(f"âš ï¸  Collection ä¸å­˜åœ¨: {collection_name}")
            print(f"   Vectorizer ä¼šåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨åˆ›å»º")
            print()
        
        return True
    except Exception as e:
        print(f"âŒ Milvus Collection æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_dagster_ops():
    """æµ‹è¯• Dagster Ops"""
    print("=" * 60)
    print("6. æµ‹è¯• Dagster å‘é‡åŒ– Ops")
    print("=" * 60)
    print()
    
    try:
        from src.processing.compute.dagster.jobs.vectorize_jobs import (
            scan_unvectorized_chunks_op,
            vectorize_chunks_op,
            validate_vectorize_results_op,
        )
        from dagster import build_op_context
        
        # æµ‹è¯• scan_op
        print("æµ‹è¯• scan_unvectorized_chunks_op...")
        context = build_op_context(
            config={
                "batch_size": 10,
                "limit": 20,
            }
        )
        scan_result = scan_unvectorized_chunks_op(context)
        
        if scan_result.get("success"):
            print(f"âœ… æ‰«ææˆåŠŸ")
            print(f"   æ‰¾åˆ°åˆ†å—æ•°: {scan_result.get('total_chunks', 0)}")
            print(f"   æ‰¹æ¬¡æ•°: {scan_result.get('total_batches', 0)}")
        else:
            print(f"âš ï¸  æ‰«æå¤±è´¥: {scan_result.get('error_message')}")
        print()
        
        # å¦‚æœæœ‰åˆ†å—ï¼Œæµ‹è¯•å‘é‡åŒ–
        if scan_result.get("success") and scan_result.get("total_chunks", 0) > 0:
            print("æµ‹è¯• vectorize_chunks_op...")
            context = build_op_context(
                config={
                    "force_revectorize": False,
                }
            )
            vectorize_result = vectorize_chunks_op(context, scan_result)
            
            if vectorize_result.get("success"):
                print(f"âœ… å‘é‡åŒ–æˆåŠŸ")
                print(f"   æˆåŠŸæ•°é‡: {vectorize_result.get('vectorized_count', 0)}")
                print(f"   å¤±è´¥æ•°é‡: {vectorize_result.get('failed_count', 0)}")
            else:
                print(f"âš ï¸  å‘é‡åŒ–å¤±è´¥: {vectorize_result.get('error_message')}")
            print()
            
            # æµ‹è¯•éªŒè¯
            print("æµ‹è¯• validate_vectorize_results_op...")
            context = build_op_context()
            validate_result = validate_vectorize_results_op(context, vectorize_result)
            
            if validate_result.get("success"):
                print(f"âœ… éªŒè¯å®Œæˆ")
                print(f"   éªŒè¯é€šè¿‡: {validate_result.get('validation_passed', False)}")
                print(f"   æˆåŠŸç‡: {validate_result.get('success_rate', 0):.2%}")
            print()
        else:
            print("âš ï¸  æ²¡æœ‰åˆ†å—å¯æµ‹è¯•ï¼Œè·³è¿‡å‘é‡åŒ– Op æµ‹è¯•")
        print()
        
        return True
    except Exception as e:
        print(f"âŒ Dagster Ops æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "=" * 60)
    print("å‘é‡åŒ–ä½œä¸šæµ‹è¯•")
    print("=" * 60)
    print()
    
    results = []
    
    # 1. æµ‹è¯• Embedder
    results.append(("Embedder æœåŠ¡", test_embedder()))
    
    # 2. æ£€æŸ¥æœªå‘é‡åŒ–çš„åˆ†å—
    chunks = check_unvectorized_chunks()
    
    # 3. æµ‹è¯• Milvus Collection
    results.append(("Milvus Collection", test_milvus_collection()))
    
    # 4. æµ‹è¯•å‘é‡åŒ–å•ä¸ªåˆ†å—
    if chunks:
        results.append(("å•ä¸ªåˆ†å—å‘é‡åŒ–", test_vectorizer_single(chunks)))
    
    # 5. æµ‹è¯•æ‰¹é‡å‘é‡åŒ–
    if chunks and len(chunks) >= 2:
        results.append(("æ‰¹é‡å‘é‡åŒ–", test_vectorizer_batch(chunks)))
    
    # 6. æµ‹è¯• Dagster Ops
    results.append(("Dagster Ops", test_dagster_ops()))
    
    # æ±‡æ€»ç»“æœ
    print("=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    print()
    
    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status}: {name}")
    
    print()
    passed = sum(1 for _, result in results if result)
    total = len(results)
    print(f"æ€»è®¡: {passed}/{total} é€šè¿‡")
    print()


if __name__ == "__main__":
    main()
