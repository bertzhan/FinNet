# -*- coding: utf-8 -*-
"""
CNINFO å®šæœŸæŠ¥å‘ŠæŠ“å–ï¼ˆå«å…¨æ–° orgId è·å–ç­–ç•¥ + å¤šè¿›ç¨‹æ”¯æŒï¼‰
- orgId è·å–ï¼š
  1) ä¼˜å…ˆï¼šhisAnnouncement/query + stock=<code>.<EX>ï¼ˆå¦‚ 000001.SZï¼‰ï¼Œä»è¿”å›å…¬å‘Šä¸­çš„ orgId æå–
  2) å…œåº•ï¼šæ£€ç´¢ HTML é¡µï¼Œè§£æé¦–æ¡è¯¦æƒ…é“¾æ¥ querystring é‡Œçš„ orgId
- äº¤æ˜“æ‰€æ˜ å°„ï¼šä¿å­˜ç›®å½• "SZ/SH/BJ"ï¼Œæ¥å£ column "szse/shse/bse"ï¼Œstock åç¼€ ".SZ/.SH/.BJ"
- Q4 è·¨å¹´æ—¶é—´çª— + åˆ†é¡µï¼›åªç”¨ adjunctUrlâ†’static ç›´é“¾ä¸‹è½½ï¼ˆé¿å…è¯¦æƒ…é¡µ 404ï¼‰
- æ–­ç‚¹ç»­æŠ“ / orgId ç¼“å­˜ / åç§°å˜æ›´è®°å½• / æ»šåŠ¨é”™è¯¯æ—¥å¿— / Windows å‹å¥½ CSV
- å¤šè¿›ç¨‹å¹¶å‘ï¼šæ”¯æŒå¤šä¸ª worker è¿›ç¨‹å¹¶è¡Œä¸‹è½½ï¼Œæ˜¾è‘—æå‡æ€§èƒ½
"""

import os
import re
import csv
import sys
import json
import time
import glob
import random
import atexit
import logging
import argparse
import platform
import threading
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, Tuple, Optional, List
from urllib.parse import urlencode, urljoin, urlparse, parse_qs

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from requests.adapters import HTTPAdapter
from requests.exceptions import RequestException
from urllib3.util.retry import Retry

# SQLite-based çŠ¶æ€ç®¡ç†ï¼ˆæ›¿ä»£ JSONï¼‰
try:
    from state_db import SharedStateSQLite
    USE_SQLITE = True
except ImportError:
    logging.warning("âš ï¸  æœªæ‰¾åˆ° state_db.pyï¼Œå°†ä½¿ç”¨ JSON æ¨¡å¼ï¼ˆæ€§èƒ½è¾ƒä½ï¼‰")
    USE_SQLITE = False

# ----------------------- å¸¸é‡ä¸é…ç½® -----------------------
CNINFO_API = "https://www.cninfo.com.cn/new/hisAnnouncement/query"
CNINFO_STATIC = "https://static.cninfo.com.cn/"
CNINFO_SEARCH = "https://www.cninfo.com.cn/new/search"

CATEGORY_ALL = "category_yjdbg_szsh;category_sjdbg_szsh;category_bndbg_szsh;category_ndbg_szsh;"

# Category mapping for each quarter (more reliable than title pattern matching)
CATEGORY_MAP = {
    "Q1": "category_yjdbg_szsh;",      # ä¸€å­£åº¦æŠ¥å‘Š (First quarter report)
    "Q2": "category_bndbg_szsh;",      # åŠå¹´åº¦æŠ¥å‘Š (Semi-annual report)
    "Q3": "category_sjdbg_szsh;",      # ä¸‰å­£åº¦æŠ¥å‘Š (Third quarter report)
    "Q4": "category_ndbg_szsh;",       # å¹´åº¦æŠ¥å‘Š (Annual report)
}

HEADERS_API = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate",  # Removed 'br' (Brotli) - requires brotli package
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Origin": "https://www.cninfo.com.cn",
    "Referer": "https://www.cninfo.com.cn/",
    "X-Requested-With": "XMLHttpRequest",
    "Connection": "keep-alive",
    "sec-ch-ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"Windows"',
    "Sec-Fetch-Dest": "empty",
    "Sec-Fetch-Mode": "cors",
    "Sec-Fetch-Site": "same-origin",
}
HEADERS_HTML = {
    "User-Agent": HEADERS_API["User-Agent"],
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate",  # Removed 'br' (Brotli) - requires brotli package
    "Referer": "https://www.cninfo.com.cn/",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "same-origin",
    "Sec-Fetch-User": "?1",
}

# å¦‚éœ€ä»£ç†ï¼ŒæŒ‰éœ€å¡«å†™ï¼›é»˜è®¤ None
PROXIES = None
# PROXIES = {"http": "http://ip:port", "https": "http://ip:port"}

# æ’é™¤å…³é”®è¯
EXCLUDE_IN_TITLE = ("æ‘˜è¦", "è‹±æ–‡", "è‹±æ–‡ç‰ˆ")

# çŠ¶æ€æ–‡ä»¶è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# checkpoint.json å­˜å‚¨åœ¨è¾“å‡ºç›®å½•ï¼ˆä¸è¾“å‡ºç»‘å®šï¼‰
# orgid_cache å’Œ code_change_cache å­˜å‚¨åœ¨è„šæœ¬ç›®å½•ï¼ˆå…¨å±€å…±äº«ï¼‰
ORGID_CACHE_FILE = os.path.join(SCRIPT_DIR, "orgid_cache.json")
CODE_CHANGE_CACHE_FILE = os.path.join(SCRIPT_DIR, "code_change_cache.json")
NAME_CHANGE_CSV = os.path.join(SCRIPT_DIR, "name_changes.csv")

RETRY_STATUS = (403, 502, 503, 504)
RETRY_TIMES = 3
RETRY_BACKOFF = 1.0
INTER_COMBO_SLEEP_RANGE = (2.0, 3.0)  # ç»„åˆé—´ç¡çœ 
INTER_SAME_STOCK_GAP = 1.0            # åŒä¸€è‚¡ç¥¨é—´éš”

CSV_ENCODING = "gbk" if platform.system().lower().startswith("win") else "utf-8-sig"

# ----------------------- æ—¥å¿— -----------------------
ERROR_LOG_FILE = os.path.join(SCRIPT_DIR, "error.log")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(formatter)
file_handler = logging.FileHandler(ERROR_LOG_FILE, encoding="utf-8")
file_handler.setFormatter(formatter)
logger.addHandler(console_handler)
logger.addHandler(file_handler)

_tail_thread = None
_tail_stop = threading.Event()

def _tail_worker(path: str):
    if not os.path.exists(path):
        with open(path, "w", encoding="utf-8") as _:
            pass
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        f.seek(0, os.SEEK_END)
        print(f"\nğŸ” æ­£åœ¨å®æ—¶ç›‘æ§ {os.path.basename(path)} æ—¥å¿—è¾“å‡º...\n")
        while not _tail_stop.is_set():
            line = f.readline()
            if not line:
                time.sleep(0.5); continue
            print(line.rstrip())

def start_tail():
    global _tail_thread
    _tail_thread = threading.Thread(target=_tail_worker, args=(ERROR_LOG_FILE,), daemon=True)
    _tail_thread.start()

def stop_tail():
    _tail_stop.set()

atexit.register(stop_tail)

# ----------------------- å·¥å…· -----------------------
def make_session(headers) -> requests.Session:
    s = requests.Session()
    s.headers.update(headers)
    retry = Retry(total=RETRY_TIMES, status_forcelist=list(RETRY_STATUS),
                  allowed_methods=frozenset(["GET", "POST"]),
                  backoff_factor=RETRY_BACKOFF, raise_on_status=False)
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter); s.mount("https://", adapter)

    # å…³é”®ï¼šè®¿é—®ä¸»é¡µå»ºç«‹ä¼šè¯ï¼Œè·å– JSESSIONID Cookie
    try:
        s.get("https://www.cninfo.com.cn/", timeout=10, proxies=PROXIES)
        logging.debug("ä¼šè¯å·²å»ºç«‹ï¼ˆè·å– JSESSIONIDï¼‰")
    except Exception as e:
        logging.warning(f"å»ºç«‹ä¼šè¯å¤±è´¥: {e}")

    return s

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def normalize_code(code) -> str:
    s = str(code).strip()
    try:
        s = str(int(float(s)))
    except Exception:
        pass
    return s.zfill(6)

def detect_exchange(code: str) -> Tuple[str, str, str]:
    """
    è¿”å›: (exchange_dir, column_api, stock_suffix)
    - exchange_dir: ä¿å­˜ç›®å½•å "SZ"/"SH"/"BJ"
    - column_api:   æ¥å£ column "szse"/"sse"  (æ³¨æ„ï¼šä¸Šæµ·ç”¨ sse ä¸æ˜¯ shseï¼ŒåŒ—äº¬ä¹Ÿç”¨ szse)
    - stock_suffix: stock=code.<suffix> ç”¨ ".SZ/.SH/.BJ"
    """
    s = str(code)
    # æŒ‰ç…§ä»å…·ä½“åˆ°ä¸€èˆ¬çš„é¡ºåºåŒ¹é…ï¼Œé¿å…å‰ç¼€é‡å 
    if s.startswith("688"):  # ç§‘åˆ›æ¿
        return "SH", "sse", "SH"
    elif s.startswith("6"):  # ä¸Šæµ·ä¸»æ¿
        return "SH", "sse", "SH"
    elif s.startswith(("300", "301")):  # åˆ›ä¸šæ¿
        return "SZ", "szse", "SZ"
    elif s.startswith(("000", "001", "002", "003")):  # æ·±åœ³ä¸»æ¿/ä¸­å°æ¿
        return "SZ", "szse", "SZ"
    elif s.startswith(("8", "43", "83")):  # åŒ—äº¬äº¤æ˜“æ‰€
        return "BJ", "szse", "BJ"
    else:
        # æœªè¯†åˆ«çš„ä»£ç ï¼Œé»˜è®¤æ·±åœ³
        logging.debug(f"æœªè¯†åˆ«çš„è‚¡ç¥¨ä»£ç : {s}ï¼Œé»˜è®¤ä½¿ç”¨æ·±åœ³äº¤æ˜“æ‰€")
        return "SZ", "szse", "SZ"

def normalize_text(s: str) -> str:
    if not s: return ""
    import unicodedata, re as _re
    s = unicodedata.normalize("NFKC", s)
    return _re.sub(r"\s+", "", s).lower()

def build_se_windows(year: int, quarter: str) -> List[str]:
    if quarter in ("Q1", "Q2", "Q3"):
        return [f"{year}-01-01~{year}-12-31"]
    # Q4 è·¨å¹´
    return [f"{year}-10-01~{year}-12-31", f"{year+1}-01-01~{year+1}-06-30"]

def q_pattern(year: int, q: str) -> re.Pattern:
    y = str(year)
    pat = {
        "Q1": rf"{y}å¹´(?:(?:ç¬¬?ä¸€)|ä¸€|1)(?:å­£åº¦æŠ¥å‘Š|å­£æŠ¥)",
        "Q2": rf"{y}å¹´(?:åŠå¹´åº¦|åŠå¹´)æŠ¥å‘Š",
        "Q3": rf"{y}å¹´(?:(?:ç¬¬?ä¸‰)|ä¸‰|3)(?:å­£åº¦æŠ¥å‘Š|å­£æŠ¥)",
        "Q4": rf"{y}å¹´?(?:å¹´åº¦æŠ¥å‘Š|å¹´æŠ¥)",  # Fixed: å¹´? makes it match both "2023å¹´åº¦æŠ¥å‘Š" and "2023å¹´å¹´åº¦æŠ¥å‘Š"
    }[q]
    return re.compile(pat)

def title_ok(title: str, year: int, quarter: str) -> bool:
    t = normalize_text(title)
    if any(k in t for k in map(normalize_text, EXCLUDE_IN_TITLE)):
        return False
    return q_pattern(year, quarter).search(t) is not None

def parse_time_to_ms(t) -> int:
    if isinstance(t, (int, float)):
        return int(t)
    if isinstance(t, str):
        try:
            return int(datetime.strptime(t, "%Y-%m-%d %H:%M").timestamp() * 1000)
        except Exception:
            return 0
    return 0

def ms_to_ddmmyyyy(ms: int) -> str:
    try:
        return datetime.fromtimestamp(ms/1000).strftime("%d-%m-%Y")
    except Exception:
        return "NA"

def pdf_url_from_adj(adj: str) -> str:
    return CNINFO_STATIC + (adj or "").lstrip("/")

def load_json(path, default):
    if not os.path.exists(path): return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def save_json(path, obj):
    """ä¿å­˜JSONæ–‡ä»¶ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼ˆè§£å†³Windowsæ–‡ä»¶é”å®šé—®é¢˜ï¼‰"""
    tmp = path + ".tmp"
    max_retries = 5
    retry_delay = 0.1

    for attempt in range(max_retries):
        try:
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with open(tmp, "w", encoding="utf-8") as f:
                json.dump(obj, f, ensure_ascii=False, indent=2)
            # ç¡®ä¿æ–‡ä»¶å·²å…³é—­
            time.sleep(0.01)

            # åŸå­æ›¿æ¢ï¼ˆWindowsä¸Šå¯èƒ½å¤±è´¥ï¼Œéœ€è¦é‡è¯•ï¼‰
            if os.path.exists(path):
                # Windows: å…ˆåˆ é™¤ç›®æ ‡æ–‡ä»¶ï¼Œå†é‡å‘½å
                if platform.system().lower().startswith("win"):
                    try:
                        os.remove(path)
                        time.sleep(0.02)  # ç­‰å¾…æ–‡ä»¶å¥æŸ„é‡Šæ”¾
                    except (PermissionError, OSError):
                        if attempt < max_retries - 1:
                            time.sleep(retry_delay * (2 ** attempt))
                            continue
                        raise
            os.rename(tmp, path)
            return
        except (PermissionError, OSError) as e:
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (2 ** attempt))
                continue
            # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•é”™è¯¯
            logging.warning(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥ï¼ˆ{path}ï¼‰: {e}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp):
                try:
                    os.remove(tmp)
                except:
                    pass
            raise

def append_name_change(csv_path: str, code: str, name_input: str, name_api: str):
    exists = os.path.exists(csv_path)
    with open(csv_path, "a" if exists else "w", encoding=CSV_ENCODING, newline="", errors="replace") as f:
        w = csv.writer(f)
        if not exists:
            w.writerow(["code", "name_input", "name_from_api"])
        w.writerow([code, name_input, name_api])

def load_code_change_cache() -> Dict[str, List[str]]:
    """
    åŠ è½½è‚¡ç¥¨ä»£ç å˜æ›´ç¼“å­˜
    æ ¼å¼: {orgId: [old_code, new_code, ...]}
    """
    return load_json(CODE_CHANGE_CACHE_FILE, {})

def save_code_change_cache(cache: Dict[str, List[str]]):
    """ä¿å­˜è‚¡ç¥¨ä»£ç å˜æ›´ç¼“å­˜"""
    save_json(CODE_CHANGE_CACHE_FILE, cache)

def detect_code_change(anns: List[dict], requested_code: str, orgId: str) -> Optional[str]:
    """
    æ£€æµ‹è‚¡ç¥¨ä»£ç å˜æ›´ï¼ˆæ”¹è¿›ç‰ˆï¼šé¿å…åˆå¹¶é‡ç»„è¯¯åˆ¤ï¼‰
    å¦‚æœå…¬å‘Šä¸­çš„è‚¡ç¥¨ä»£ç ä¸è¯·æ±‚çš„ä»£ç ä¸ä¸€è‡´ï¼Œè¿”å›å®é™…ä»£ç 

    Args:
        anns: å…¬å‘Šåˆ—è¡¨
        requested_code: è¯·æ±‚çš„è‚¡ç¥¨ä»£ç 
        orgId: ç»„ç»‡ID

    Returns:
        å®é™…è‚¡ç¥¨ä»£ç ï¼Œå¦‚æœæ²¡æœ‰å˜æ›´åˆ™è¿”å› None
    """
    if not anns:
        return None

    requested_code_normalized = normalize_code(requested_code)

    # ç»Ÿè®¡å…¬å‘Šä¸­å‡ºç°çš„æ‰€æœ‰è‚¡ç¥¨ä»£ç å’Œå¯¹åº”çš„å…¬å¸åç§°
    code_info = {}  # {code: {"count": int, "names": set}}
    for ann in anns:
        code = normalize_code(str(ann.get("secCode", "")))
        name = normalize_text(str(ann.get("secName", "")))

        if code and code != "000000":
            if code not in code_info:
                code_info[code] = {"count": 0, "names": set()}
            code_info[code]["count"] += 1
            if name:
                code_info[code]["names"].add(name)

    if not code_info:
        return None

    # æ‰¾å‡ºå‡ºç°æœ€å¤šçš„è‚¡ç¥¨ä»£ç 
    most_common_code = max(code_info.keys(), key=lambda k: code_info[k]["count"])

    # å¦‚æœæœ€å¸¸è§çš„ä»£ç ä¸è¯·æ±‚çš„ä»£ç ä¸åŒ
    if most_common_code != requested_code_normalized:
        # æ£€æŸ¥å…¬å¸åç§°æ˜¯å¦ä¸€è‡´ï¼ˆé¿å…åˆå¹¶é‡ç»„è¯¯åˆ¤ï¼‰
        requested_names = code_info.get(requested_code_normalized, {}).get("names", set())
        most_common_names = code_info[most_common_code]["names"]

        # è®¡ç®—åç§°ç›¸ä¼¼åº¦
        name_overlap = requested_names & most_common_names  # äº¤é›†

        # å¦‚æœæœ‰ç›¸åŒçš„å…¬å¸åç§°ï¼Œæˆ–è€…è¯·æ±‚çš„ä»£ç æ²¡æœ‰å…¬å‘Šï¼ˆçº¯ä»£ç å˜æ›´åœºæ™¯ï¼‰
        if name_overlap or not requested_names:
            logging.warning(
                f"[ä»£ç å˜æ›´æ£€æµ‹] orgId={orgId}: è¯·æ±‚ä»£ç  {requested_code_normalized} -> å®é™…ä»£ç  {most_common_code} "
                f"(å…¬å¸åç§°: {', '.join(list(most_common_names)[:2])})"
            )
            return most_common_code
        else:
            # å…¬å¸åç§°å®Œå…¨ä¸åŒï¼Œå¯èƒ½æ˜¯åˆå¹¶é‡ç»„ï¼Œä¸åˆ¤å®šä¸ºä»£ç å˜æ›´
            logging.info(
                f"[ä»£ç å˜æ›´æ£€æµ‹] orgId={orgId}: æ£€æµ‹åˆ°ä¸åŒè‚¡ç¥¨ä»£ç ï¼Œä½†å…¬å¸åç§°ä¸ä¸€è‡´ï¼Œç–‘ä¼¼åˆå¹¶é‡ç»„ï¼Œä¸ä½œå˜æ›´å¤„ç† "
                f"(è¯·æ±‚: {requested_code_normalized}={requested_names}, æœ€å¤š: {most_common_code}={most_common_names})"
            )

    return None

def get_all_related_codes(code: str, orgId: Optional[str], code_change_cache: Dict[str, List[str]]) -> List[str]:
    """
    è·å–ä¸ç»™å®šä»£ç ç›¸å…³çš„æ‰€æœ‰å†å²ä»£ç 

    Args:
        code: å½“å‰è‚¡ç¥¨ä»£ç 
        orgId: ç»„ç»‡ID
        code_change_cache: ä»£ç å˜æ›´ç¼“å­˜

    Returns:
        ç›¸å…³ä»£ç åˆ—è¡¨ï¼ˆåŒ…æ‹¬å½“å‰ä»£ç ï¼‰
    """
    code_normalized = normalize_code(code)
    related_codes = [code_normalized]

    if orgId and orgId in code_change_cache:
        cached_codes = code_change_cache[orgId]
        for c in cached_codes:
            c_norm = normalize_code(c)
            if c_norm not in related_codes:
                related_codes.append(c_norm)

    return related_codes

def build_existing_pdf_cache(old_pdf_dir: Optional[str]) -> set:
    """
    æ‰«ææ—§PDFç›®å½•ï¼Œæ„å»ºå·²å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜ï¼ˆä¸€æ¬¡æ€§æ‰«æï¼‰

    Args:
        old_pdf_dir: æ—§PDFç›®å½•è·¯å¾„

    Returns:
        set of (code, year, quarter) tuples
    """
    if not old_pdf_dir or not os.path.exists(old_pdf_dir):
        return set()

    cache = set()
    print(f"ğŸ” æ‰«ææ—§PDFç›®å½•ï¼š{old_pdf_dir}")

    # æ‰«ææ‰€æœ‰äº¤æ˜“æ‰€ç›®å½•
    for exchange in ["SZ", "SH", "BJ"]:
        exchange_dir = os.path.join(old_pdf_dir, exchange)
        if not os.path.exists(exchange_dir):
            continue

        # ä½¿ç”¨globé€’å½’æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pattern = os.path.join(exchange_dir, "**", "*.pdf")
        pdf_files = glob.glob(pattern, recursive=True)

        for pdf_path in pdf_files:
            # è§£ææ–‡ä»¶åï¼šcode_year_quarter_date.pdf
            filename = os.path.basename(pdf_path)
            parts = filename.replace(".pdf", "").split("_")

            if len(parts) >= 3:
                code_raw = parts[0]
                year_str = parts[1]
                quarter = parts[2]

                try:
                    year = int(year_str)
                    code = normalize_code(code_raw)  # æ ‡å‡†åŒ–ä»£ç ï¼Œç¡®ä¿åŒ¹é…
                    cache.add((code, year, quarter))
                except ValueError:
                    continue

    print(f"âœ… æ‰¾åˆ° {len(cache)} ä¸ªå·²å­˜åœ¨çš„æŠ¥å‘Š")
    return cache

def check_pdf_exists_in_cache(cache: set, code: str, year: int, quarter: str) -> bool:
    """
    æ£€æŸ¥PDFæ˜¯å¦åœ¨ç¼“å­˜ä¸­ï¼ˆO(1)æŸ¥æ‰¾ï¼Œæ— I/Oï¼‰

    Args:
        cache: å·²å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜é›†åˆ
        code: è‚¡ç¥¨ä»£ç 
        year: å¹´ä»½
        quarter: å­£åº¦

    Returns:
        True å¦‚æœæ–‡ä»¶å­˜åœ¨äºç¼“å­˜ä¸­
    """
    return (code, year, quarter) in cache

def read_tasks_from_csv(path: str, default_year: Optional[int] = None, default_quarter: Optional[str] = None):
    """
    è¯»å– CSV ä»»åŠ¡æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç 
    å°è¯•é¡ºåº: UTF-8-sig -> UTF-8 -> GBK (Windows) -> GB2312
    
    Args:
        path: CSV æ–‡ä»¶è·¯å¾„
        default_year: é»˜è®¤å¹´ä»½ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰yearåˆ—ï¼‰
        default_quarter: é»˜è®¤å­£åº¦ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰quarteråˆ—ï¼Œæ ¼å¼ï¼šQ1/Q2/Q3/Q4ï¼‰
    """
    tasks = []
    encodings = ["utf-8-sig", "utf-8"]
    if platform.system().lower().startswith("win"):
        encodings.extend(["gbk", "gb2312", "gb18030"])

    last_error = None
    for enc in encodings:
        try:
            logging.debug(f"å°è¯•ä½¿ç”¨ç¼–ç  {enc} è¯»å– CSV...")
            with open(path, "r", encoding=enc, newline="") as f:
                reader = csv.DictReader(f)
                # æ£€æŸ¥CSVæ–‡ä»¶åŒ…å«å“ªäº›åˆ—
                fieldnames = reader.fieldnames or []
                has_year = "year" in fieldnames
                has_quarter = "quarter" in fieldnames
                
                if not has_year and default_year is None:
                    logging.error("âŒ CSV æ–‡ä»¶ä¸­æ²¡æœ‰ 'year' åˆ—ï¼Œä¸”æœªæä¾› --year å‚æ•°")
                    raise ValueError("CSV æ–‡ä»¶ä¸­ç¼ºå°‘ 'year' åˆ—ï¼Œè¯·æ·»åŠ è¯¥åˆ—æˆ–ä½¿ç”¨ --year å‚æ•°")
                if not has_quarter and default_quarter is None:
                    logging.error("âŒ CSV æ–‡ä»¶ä¸­æ²¡æœ‰ 'quarter' åˆ—ï¼Œä¸”æœªæä¾› --quarter å‚æ•°")
                    raise ValueError("CSV æ–‡ä»¶ä¸­ç¼ºå°‘ 'quarter' åˆ—ï¼Œè¯·æ·»åŠ è¯¥åˆ—æˆ–ä½¿ç”¨ --quarter å‚æ•°")
                
                for row in reader:
                    try:
                        raw = (row.get("code") or "").strip()
                        code = normalize_code(raw) if raw else ""
                        name = (row.get("name") or "").strip()
                        
                        # ä»CSVè¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼
                        if has_year:
                            year = int(str(row.get("year")).strip())
                        else:
                            year = default_year
                            
                        if has_quarter:
                            quarter = (row.get("quarter") or "").strip().upper()
                        else:
                            quarter = default_quarter.upper() if default_quarter else None
                        
                        if not code or not name:
                            continue
                        if quarter not in ("Q1","Q2","Q3","Q4"):
                            logging.warning(f"è·³è¿‡æ— æ•ˆå­£åº¦ '{quarter}'ï¼ˆè¡Œï¼šcode={code}, name={name}ï¼‰")
                            continue
                        tasks.append((code, name, year, quarter))
                    except Exception as e:
                        logging.debug(f"è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                        continue
            logging.info(f"âœ… æˆåŠŸä½¿ç”¨ç¼–ç  {enc} è¯»å– {len(tasks)} æ¡ä»»åŠ¡")
            return tasks
        except (UnicodeDecodeError, UnicodeError) as e:
            last_error = e
            logging.debug(f"ç¼–ç  {enc} å¤±è´¥: {e}")
            tasks = []  # é‡ç½®ä»»åŠ¡åˆ—è¡¨
            continue
        except Exception as e:
            logging.error(f"è¯»å– CSV å¤±è´¥ ({enc}): {e}")
            raise

    # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥
    logging.error(f"âŒ æ— æ³•è¯»å– CSV æ–‡ä»¶ï¼Œå°è¯•äº†ç¼–ç : {encodings}")
    logging.error(f"æœ€åé”™è¯¯: {last_error}")
    logging.error(f"è¯·ç¡®ä¿ CSV æ–‡ä»¶ä¿å­˜ä¸º UTF-8 æˆ– GBK ç¼–ç ")
    raise ValueError(f"æ— æ³•è§£ç  CSV æ–‡ä»¶: {path}")

# ----------------------- orgId æ„é€ ï¼ˆæŒ‰è§„åˆ™ç”Ÿæˆï¼‰ -----------------------
def build_orgid(code: str) -> str:
    """
    æ ¹æ®è‚¡ç¥¨ä»£ç æ„é€  orgIdï¼ˆä»…é€‚ç”¨äºéƒ¨åˆ†å…¬å¸ï¼‰
    - æ·±åœ³: gssz + 0 + code (å¦‚ gssz0000001, 7ä½)
    - ä¸Šæµ·: gssh + 0 + code (å¦‚ gssh0600519, 7ä½)
    - åŒ—äº¬: gsbj + 0 + code (å¦‚ gsbj0430001, 7ä½)
    æ³¨æ„ï¼šæ­¤æ–¹æ³•ä»…å¯¹éƒ¨åˆ†å…¬å¸æœ‰æ•ˆï¼Œå…¶ä»–å…¬å¸éœ€ä½¿ç”¨ searchkey æŸ¥è¯¢
    """
    exch_dir, _, _ = detect_exchange(code)
    code6 = normalize_code(code)
    code7 = f"0{code6}"  # 7ä½ï¼šå‰é¢è¡¥ä¸€ä¸ª 0

    if exch_dir == "SH":
        return f"gssh{code7}"
    elif exch_dir == "BJ":
        return f"gsbj{code7}"
    else:  # SZ
        return f"gssz{code7}"

def get_orgid_via_search_api(api_session: requests.Session, code: str) -> Optional[Tuple[str, str]]:
    """
    é€šè¿‡æœç´¢APIè·å–orgIdï¼ˆæ¨èæ–¹æ³• - ç®€å•å¯é ï¼‰
    ä½¿ç”¨ /new/information/topSearch/query æ¥å£
    ä¼˜åŠ¿ï¼šä¸éœ€è¦å…¬å¸åï¼Œç›´æ¥ç”¨è‚¡ç¥¨ä»£ç æŸ¥è¯¢

    Args:
        api_session: requestsä¼šè¯
        code: è‚¡ç¥¨ä»£ç 

    Returns:
        (orgId, company_name) æˆ– None
    """
    code6 = normalize_code(code)

    # æ­£ç¡®çš„URLï¼ˆä¸æ˜¯detailOfQueryï¼Œè€Œæ˜¯queryï¼‰
    url = "https://www.cninfo.com.cn/new/information/topSearch/query"

    # ç›´æ¥ç”¨è‚¡ç¥¨ä»£ç æŸ¥è¯¢ï¼ˆä¸éœ€è¦å‰ç¼€ï¼‰
    try:
        r = api_session.post(url, data={"keyWord": code6}, timeout=10, proxies=PROXIES)
        if r.status_code == 200:
            # è¿”å›çš„æ˜¯æ•°ç»„ï¼Œä¸æ˜¯keyBoardList
            js = r.json()
            if isinstance(js, list) and len(js) > 0:
                for item in js:
                    # ç²¾ç¡®åŒ¹é…è‚¡ç¥¨ä»£ç 
                    if item.get("code", "") == code6:
                        orgid = item.get("orgId")
                        company_name = item.get("zwjc", "")
                        if orgid:
                            logging.info(f"[orgId] é€šè¿‡æœç´¢APIè·å–æˆåŠŸï¼š{orgid} ({company_name})")
                            return orgid, company_name
    except Exception as e:
        logging.debug(f"æœç´¢APIæŸ¥è¯¢å¤±è´¥ (code={code6}): {e}")

    return None

def get_orgid_by_searchkey(api_session: requests.Session, code: str, name: str, column_api: str) -> Optional[Tuple[str, str]]:
    """
    é€šè¿‡ searchkey æœç´¢å…¬å¸åï¼Œä»è¿”å›ç»“æœä¸­æå–çœŸå®çš„ orgId å’Œå…¬å¸åç§°
    è¿™æ˜¯å…œåº•æ–¹æ³•ï¼Œå½“æœç´¢APIå¤±è´¥æ—¶ä½¿ç”¨

    Returns:
        (orgId, company_name) æˆ– None
    """
    code6 = normalize_code(code)

    payload = {
        "stock": "",
        "column": column_api,
        "category": "",
        "seDate": f"{datetime.now().year-1}-01-01~{datetime.now().year}-12-31",
        "pageNum": "1",
        "pageSize": "30",
        "tabName": "fulltext",
        "searchkey": name,
        "plate": "",
    }

    try:
        logging.debug(f"ä½¿ç”¨ searchkey='{name}' æŸ¥è¯¢ orgId...")
        r = api_session.post(CNINFO_API, data=payload, timeout=20, proxies=PROXIES)
        if r.status_code == 200:
            data = r.json()
            anns = data.get("announcements") or []

            logging.debug(f"searchkey è¿”å› {len(anns)} æ¡å…¬å‘Š")

            for ann in anns:
                ann_code = str(ann.get("secCode", "")).zfill(6)
                if ann_code == code6:
                    oid = ann.get("orgId")
                    sec_name = ann.get("secName", "")
                    if oid:
                        logging.info(f"[orgId] é€šè¿‡ searchkey è·å–æˆåŠŸï¼š{oid} ({sec_name})")
                        return oid, sec_name

            logging.warning(f"searchkey è¿”å›å…¬å‘Šä¸­æœªæ‰¾åˆ°åŒ¹é…çš„è‚¡ç¥¨ä»£ç  {code6}")
            if anns and len(anns) > 0:
                logging.debug(f"å‰5æ¡å…¬å‘Šçš„è‚¡ç¥¨ä»£ç : {[ann.get('secCode') for ann in anns[:5]]}")
        else:
            logging.warning(f"searchkey è¯·æ±‚å¤±è´¥: HTTP {r.status_code}")
    except Exception as e:
        logging.warning(f"searchkey æŸ¥è¯¢å¼‚å¸¸ï¼š{e}")

    return None

def get_orgid_via_html(code: str, name: Optional[str], html_session: Optional[requests.Session] = None) -> Optional[Tuple[str, str]]:
    """
    æ”¹è¿›ç‰ˆï¼šè®¿é—®æ£€ç´¢é¡µå¹¶è§£æé¦–æ¡è¯¦æƒ…é“¾æ¥ä¸­çš„ orgId
    æ”¯æŒå¤šç§æœç´¢å…³é”®è¯ï¼Œè¿”å› (orgId, å…¬å¸åç§°)

    Args:
        code: è‚¡ç¥¨ä»£ç 
        name: å…¬å¸åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—ï¼‰
        html_session: å¯å¤ç”¨çš„ä¼šè¯ï¼ˆå¯é€‰ï¼‰

    Returns:
        (orgId, company_name) æˆ– None
    """
    code6 = normalize_code(code)
    exch_dir, column_api, stock_suffix = detect_exchange(code6)

    # åˆ›å»ºæˆ–å¤ç”¨ä¼šè¯
    if html_session is None:
        html_session = make_session(HEADERS_HTML)

    # å°è¯•å¤šç§æœç´¢å…³é”®è¯
    search_keywords = ["å¹´åº¦æŠ¥å‘Š", "å­£åº¦æŠ¥å‘Š", ""]  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸é™å®šæŠ¥å‘Šç±»å‹

    for keyword in search_keywords:
        params = {
            "stock": code6,
            "searchkey": keyword,
            "category": column_api,
            "pageNum": 1,
        }
        url = f"{CNINFO_SEARCH}?{urlencode(params, safe='')}"

        logging.info(f"[HTMLå…œåº•] å°è¯•æœç´¢å…³é”®è¯'{keyword}'ï¼š{code6}")

        try:
            r = html_session.get(url, timeout=20, proxies=PROXIES)
            if r.status_code != 200:
                logging.debug(f"HTML æ£€ç´¢å¤±è´¥ï¼šHTTP {r.status_code}")
                continue

            soup = BeautifulSoup(r.text, "html.parser")

            # æ–¹æ³•1ï¼šè§£æåˆ—è¡¨é¡µçš„è¯¦æƒ…é“¾æ¥
            main = soup.find("div", class_="list-main")
            if main:
                items = main.find_all("div", class_="list-item")
                logging.debug(f"æ‰¾åˆ° {len(items)} æ¡å…¬å‘Š")

                for item in items[:5]:  # æ£€æŸ¥å‰5æ¡
                    # æå–å…¬å¸åç§°
                    company_span = item.find("span", class_="company-name")
                    company_name = company_span.get_text(strip=True) if company_span else ""

                    # æå–è¯¦æƒ…é“¾æ¥
                    a = item.select_one("span.ahover.ell a")
                    if not a or not a.get("href"):
                        continue

                    detail_url = urljoin("https://www.cninfo.com.cn", a["href"])
                    qs = parse_qs(urlparse(detail_url).query)
                    oid = (qs.get("orgId") or [""])[0]

                    # éªŒè¯è‚¡ç¥¨ä»£ç æ˜¯å¦åŒ¹é…
                    stock_code = (qs.get("stockCode") or [""])[0]
                    if stock_code and normalize_code(stock_code) == code6:
                        if oid:
                            logging.info(f"[HTMLå…œåº•] æˆåŠŸè§£æ orgIdï¼š{oid} ({company_name})")
                            return oid, company_name

            # æ–¹æ³•2ï¼šä»é¡µé¢è„šæœ¬ä¸­æå– orgIdï¼ˆå¤‡ç”¨ï¼‰
            scripts = soup.find_all("script")
            for script in scripts:
                script_text = script.string or ""
                # æŸ¥æ‰¾ç±»ä¼¼ "orgId":"9900012345" çš„æ¨¡å¼
                import re
                matches = re.findall(r'"orgId"\s*:\s*"([^"]+)"', script_text)
                if matches:
                    oid = matches[0]
                    logging.info(f"[HTMLå…œåº•] ä»è„šæœ¬ä¸­è§£æ orgIdï¼š{oid}")
                    return oid, name or ""

        except Exception as e:
            logging.debug(f"HTML è§£æå¼‚å¸¸ (keyword={keyword}): {e}")
            continue

    logging.warning(f"[HTMLå…œåº•] æ‰€æœ‰æœç´¢å…³é”®è¯å‡å¤±è´¥ï¼š{code6}")
    return None

def get_orgid(code: str, name: Optional[str]) -> Optional[str]:
    """
    å¯¹å¤–ç»Ÿä¸€å…¥å£ï¼šHTML scraping fallback
    æ³¨æ„ï¼šæ­¤å‡½æ•°å·²ä¸å†ä½¿ç”¨ï¼Œä¿ç•™ä»…ä¾›å‘åå…¼å®¹
    """
    code6 = normalize_code(code)
    return get_orgid_via_html(code6, name)

# ----------------------- å…¬å‘ŠæŠ“å–ä¸ä¸‹è½½ -----------------------
def fetch_anns_by_category(api_session: requests.Session, code: str, orgId: Optional[str],
                           column_api: str, year: int, quarter: str, page_size=100) -> List[dict]:
    """
    ä½¿ç”¨ç±»åˆ«è¿‡æ»¤çš„å…¬å‘ŠæŠ“å–ï¼ˆæ¨èæ–¹æ³•ï¼‰
    - ç›´æ¥ä½¿ç”¨å¯¹åº”å­£åº¦çš„ç±»åˆ«ï¼ŒæœåŠ¡å™¨ç«¯è¿‡æ»¤ï¼Œæ›´å¯é 
    - é¿å…å¤æ‚çš„æ ‡é¢˜æ­£åˆ™åŒ¹é…å’Œæ ‡é¢˜æ ¼å¼å˜åŒ–é—®é¢˜
    - ä¼˜å…ˆä½¿ç”¨ stock="<code>,<orgId>"ï¼›è‹¥æ—  orgIdï¼Œåˆ™é€€åŒ–ä¸º stock="<code>.<EX>"
    """
    _, _, stock_suffix = detect_exchange(code)
    stock_field = f"{code},{orgId}" if orgId else f"{code}.{stock_suffix}"

    # ä½¿ç”¨å­£åº¦å¯¹åº”çš„å…·ä½“ç±»åˆ«ï¼Œè€Œä¸æ˜¯ CATEGORY_ALL
    category = CATEGORY_MAP.get(quarter, CATEGORY_ALL)

    all_list: List[dict] = []
    for seDate in build_se_windows(year, quarter):
        page = 1
        while True:
            payload = {
                "tabName": "fulltext",
                "column": column_api,
                "stock": stock_field,
                "category": category,  # ä½¿ç”¨å…·ä½“ç±»åˆ«
                "seDate": seDate,
                "pageNum": str(page),
                "pageSize": str(page_size),
                "searchkey": "",
                "plate": "",
                "isHLtitle": "true",
            }
            logging.debug(f"[APIè¯·æ±‚] stock={stock_field}, category={category}, seDate={seDate}, page={page}")
            data = None
            try:
                data = api_session.post(CNINFO_API, data=payload, timeout=20, proxies=PROXIES)
                if data.status_code >= 400:
                    logging.warning(f"hisAnnouncement HTTP {data.status_code} ({code} é¡µ {page})")
                    break
                response_text = data.text.strip()
                if not response_text.startswith("{"):
                    logging.warning(f"[API] è¿”å›éJSONå†…å®¹: {response_text[:200]}")
                    break
                data = data.json()
            except RequestException as e:
                logging.warning(f"hisAnnouncement è¯·æ±‚å¼‚å¸¸ï¼ˆ{code} é¡µ {page}ï¼‰ï¼š{e}")
                break
            except Exception as e:
                logging.warning(f"[API] JSONè§£æå¤±è´¥: {e}, å“åº”: {data.text[:200] if data else 'None'}")
                break

            anns = (data or {}).get("announcements") or []
            logging.debug(f"[APIå“åº”] è¿”å› {len(anns)} æ¡å…¬å‘Š (seDate={seDate})")
            if not anns: break
            all_list.extend(anns)
            if len(anns) < page_size: break
            page += 1
    return all_list

def fetch_anns(api_session: requests.Session, code: str, orgId: Optional[str],
               column_api: str, year: int, quarter: str, page_size=100) -> List[dict]:
    """
    å¤šæ—¶é—´çª—ã€åˆ†é¡µæŠ“å–å¹¶åˆå¹¶
    ä¼˜å…ˆä½¿ç”¨ stock="<code>,<orgId>"ï¼›è‹¥æ—  orgIdï¼Œåˆ™é€€åŒ–ä¸º stock="<code>.<EX>"

    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¿ç•™å‘åå…¼å®¹ï¼Œä½†æ¨èä½¿ç”¨ fetch_anns_by_category() è·å¾—æ›´å¯é çš„ç»“æœ
    """
    _, _, stock_suffix = detect_exchange(code)
    stock_field = f"{code},{orgId}" if orgId else f"{code}.{stock_suffix}"

    all_list: List[dict] = []
    for seDate in build_se_windows(year, quarter):
        page = 1
        while True:
            payload = {
                "tabName": "fulltext",
                "column": column_api,
                "stock": stock_field,
                "category": CATEGORY_ALL,
                "seDate": seDate,
                "pageNum": str(page),  # è½¬ä¸ºå­—ç¬¦ä¸²
                "pageSize": str(page_size),  # è½¬ä¸ºå­—ç¬¦ä¸²
                "searchkey": "",
                "plate": "",
                "isHLtitle": "true",
            }
            logging.debug(f"[APIè¯·æ±‚ CATEGORY_ALL] stock={stock_field}, seDate={seDate}, page={page}")
            data = None
            try:
                # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ data= è€Œä¸æ˜¯ json=ï¼ˆform-data æ ¼å¼ï¼‰
                data = api_session.post(CNINFO_API, data=payload, timeout=20, proxies=PROXIES)
                if data.status_code >= 400:
                    logging.warning(f"hisAnnouncement HTTP {data.status_code} ({code} é¡µ {page})")
                    break
                response_text = data.text.strip()
                if not response_text.startswith("{"):
                    logging.warning(f"[API] è¿”å›éJSONå†…å®¹: {response_text[:200]}")
                    break
                data = data.json()
            except RequestException as e:
                logging.warning(f"hisAnnouncement è¯·æ±‚å¼‚å¸¸ï¼ˆ{code} é¡µ {page}ï¼‰ï¼š{e}")
                break
            except Exception as e:
                logging.warning(f"[API] JSONè§£æå¤±è´¥: {e}, å“åº”: {data.text[:200] if data else 'None'}")
                break

            anns = (data or {}).get("announcements") or []
            logging.debug(f"[APIå“åº”] è¿”å› {len(anns)} æ¡å…¬å‘Š (seDate={seDate})")
            if not anns: break
            all_list.extend(anns)
            if len(anns) < page_size: break
            page += 1
    return all_list

def pick_latest(anns: List[dict], code: str, year: int, quarter: str, 
                related_codes: Optional[List[str]] = None) -> Optional[dict]:
    """
    ä»å…¬å‘Šåˆ—è¡¨ä¸­é€‰æ‹©æœ€æ–°çš„åŒ¹é…æŠ¥å‘Š
    
    Args:
        anns: å…¬å‘Šåˆ—è¡¨
        code: ä¸»è¦è‚¡ç¥¨ä»£ç 
        year: å¹´ä»½
        quarter: å­£åº¦
        related_codes: ç›¸å…³è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆç”¨äºå¤„ç†ä»£ç å˜æ›´ï¼‰
        
    Returns:
        æœ€æ–°çš„åŒ¹é…å…¬å‘Šï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
    """
    # å¦‚æœæ²¡æœ‰æä¾›ç›¸å…³ä»£ç ï¼Œä½¿ç”¨ä¸»ä»£ç 
    if related_codes is None:
        related_codes = [normalize_code(code)]
    else:
        related_codes = [normalize_code(c) for c in related_codes]
    
    cands = []
    for a in anns:
        ann_code = normalize_code(str(a.get("secCode", "")))
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•ç›¸å…³ä»£ç 
        if ann_code not in related_codes:
            continue
            
        title = a.get("announcementTitle","")
        if not title_ok(title, year, quarter):
            continue
        adj = a.get("adjunctUrl","")
        if not adj or not adj.lower().endswith(".pdf"):
            continue
        ts = parse_time_to_ms(a.get("announcementTime"))
        cands.append((ts, a, ann_code))  # ä¿å­˜å®é™…ä»£ç 
    
    if not cands:
        return None
    cands.sort(key=lambda x: x[0], reverse=True)
    
    # å¦‚æœä½¿ç”¨çš„æ˜¯éä¸»ä»£ç ï¼Œè®°å½•æ—¥å¿—
    best_ann = cands[0][1]
    actual_code = cands[0][2]
    if actual_code != normalize_code(code):
        logging.info(f"[ä»£ç å˜æ›´] ä½¿ç”¨å†å²ä»£ç  {actual_code} æ‰¾åˆ°æŠ¥å‘Šï¼ˆè¯·æ±‚ä»£ç : {normalize_code(code)}ï¼‰")
    
    return best_ann

def download_pdf_resilient(session: requests.Session, url: str, path: str,
                           referer: Optional[str] = None,
                           refresh_fn=None, max_retries=3):
    cur_url, cur_ref = url, referer
    last_err = None
    for attempt in range(1, max_retries+1):
        try:
            headers = {"Referer": cur_ref} if cur_ref else {}
            r = session.get(cur_url, timeout=20, stream=True, proxies=PROXIES, headers=headers)
            if r.status_code == 404 and refresh_fn:
                new_url, new_ref = refresh_fn()
                if new_url and new_url != cur_url:
                    logging.warning(f"404 åˆ·æ–°é“¾æ¥ï¼š\nold={cur_url}\nnew={new_url}")
                    cur_url, cur_ref = new_url, (new_ref or cur_ref)
                    continue
            if r.status_code in RETRY_STATUS:
                time.sleep(RETRY_BACKOFF); continue
            if r.status_code >= 400:
                last_err = f"HTTP {r.status_code}"
                time.sleep(RETRY_BACKOFF); continue

            head = r.raw.read(5); r.raw.decode_content = True
            if not head.startswith(b"%PDF-"):
                return False, "éPDFå†…å®¹"
            ensure_dir(os.path.dirname(path))
            with open(path, "wb") as f:
                f.write(head or b"")
                for chunk in r.iter_content(8192):
                    if chunk: f.write(chunk)
            return True, "ok"
        except RequestException as e:
            last_err = str(e)
            time.sleep(RETRY_BACKOFF)
    return False, last_err or "HTTP 4xx/5xx"

# ----------------------- å¤šè¿›ç¨‹æ”¯æŒ -----------------------
class SharedState:
    """çº¿ç¨‹å®‰å…¨çš„å…±äº«çŠ¶æ€ç®¡ç†å™¨"""
    def __init__(self, checkpoint_file: str, orgid_cache_file: str, code_change_cache_file: str, shared_lock=None):
        self.checkpoint_file = checkpoint_file
        self.orgid_cache_file = orgid_cache_file
        self.code_change_cache_file = code_change_cache_file
        # ä½¿ç”¨ä¼ å…¥çš„å…±äº«é”ï¼ˆå¤šè¿›ç¨‹ï¼‰æˆ–åˆ›å»ºæ–°é”ï¼ˆå•è¿›ç¨‹ï¼‰
        self.lock = shared_lock if shared_lock is not None else multiprocessing.Lock()

    def load_checkpoint(self) -> Dict[str, bool]:
        with self.lock:
            return load_json(self.checkpoint_file, {})

    def save_checkpoint(self, key: str):
        with self.lock:
            data = load_json(self.checkpoint_file, {})
            data[key] = True
            save_json(self.checkpoint_file, data)

    def load_orgid_cache(self) -> Dict[str, str]:
        with self.lock:
            return load_json(self.orgid_cache_file, {})

    def save_orgid(self, code: str, orgid: str):
        with self.lock:
            data = load_json(self.orgid_cache_file, {})
            data[code] = orgid
            save_json(self.orgid_cache_file, data)

    def get_orgid(self, code: str) -> Optional[str]:
        with self.lock:
            data = load_json(self.orgid_cache_file, {})
            return data.get(code)
    
    def load_code_change_cache(self) -> Dict[str, List[str]]:
        with self.lock:
            return load_json(self.code_change_cache_file, {})
    
    def save_code_change(self, orgid: str, codes: List[str]):
        """ä¿å­˜ä»£ç å˜æ›´è®°å½•"""
        with self.lock:
            data = load_json(self.code_change_cache_file, {})
            # åˆå¹¶æ–°æ—§ä»£ç åˆ—è¡¨
            existing = data.get(orgid, [])
            for code in codes:
                code_norm = normalize_code(code)
                if code_norm not in existing:
                    existing.append(code_norm)
            data[orgid] = existing
            save_json(self.code_change_cache_file, data)

# ----------------------- MinIOä¸Šä¼ è¾…åŠ©å‡½æ•° -----------------------
def upload_to_minio_in_process(local_path: str, code: str, year: int, quarter: str, out_root: str,
                               publish_date: Optional[str] = None):
    """
    åœ¨å·¥ä½œè¿›ç¨‹ä¸­ä¸Šä¼ æ–‡ä»¶åˆ°MinIO
    
    Args:
        local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        code: è‚¡ç¥¨ä»£ç 
        year: å¹´ä»½
        quarter: å­£åº¦ï¼ˆå¦‚ "Q1"ï¼‰
        out_root: è¾“å‡ºæ ¹ç›®å½•
        publish_date: å‘å¸ƒæ—¥æœŸï¼ˆISOæ ¼å¼ï¼Œå¯é€‰ï¼‰
    """
    try:
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        use_minio = os.getenv("USE_MINIO", "false").lower() == "true"
        if not use_minio:
            return  # MinIOæœªå¯ç”¨ï¼Œè·³è¿‡
        
        minio_upload_on_download = os.getenv("MINIO_UPLOAD_ON_DOWNLOAD", "true").lower() == "true"
        if not minio_upload_on_download:
            return  # é…ç½®ä¸ºä¸åœ¨ä¸‹è½½æ—¶ä¸Šä¼ 
        
        # è¯»å–MinIOé…ç½®
        minio_endpoint = os.getenv("MINIO_ENDPOINT")
        minio_access_key = os.getenv("MINIO_ACCESS_KEY")
        minio_secret_key = os.getenv("MINIO_SECRET_KEY")
        minio_bucket = os.getenv("MINIO_BUCKET", "company-datalake")
        
        if not all([minio_endpoint, minio_access_key, minio_secret_key]):
            logging.warning(f"[{code}] MinIOé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡ä¸Šä¼ ")
            return
        
        # åˆ›å»ºMinIOå®¢æˆ·ç«¯ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åˆ›å»ºï¼‰
        try:
            from minio import Minio
            from minio.error import S3Error
        except ImportError:
            logging.warning(f"[{code}] minioåº“æœªå®‰è£…ï¼Œè·³è¿‡ä¸Šä¼ ")
            return
        
        endpoint = minio_endpoint.replace("http://", "").replace("https://", "")
        secure = minio_endpoint.startswith("https://")
        
        client = Minio(
            endpoint,
            access_key=minio_access_key,
            secret_key=minio_secret_key,
            secure=secure
        )
        
        # ç¡®ä¿æ¡¶å­˜åœ¨
        try:
            if not client.bucket_exists(minio_bucket):
                client.make_bucket(minio_bucket)
        except S3Error as e:
            logging.warning(f"[{code}] æ£€æŸ¥/åˆ›å»ºæ¡¶å¤±è´¥: {e}")
            return
        
        # ç”Ÿæˆå¯¹è±¡åç§°ï¼ˆç¬¦åˆplan.mdè§„èŒƒï¼‰
        # æ ¼å¼: bronze/zh_stock/quarterly_reports/{year}/{quarter}/{stock_code}/{filename}
        filename = os.path.basename(local_path)
        
        # ç¡®å®šæ–‡æ¡£ç±»å‹
        quarter_num = int(quarter[1]) if quarter.startswith("Q") else 4
        if quarter_num == 4:
            doc_type = "annual_reports"
        elif quarter_num == 2:
            doc_type = "interim_reports"
        else:
            doc_type = "quarterly_reports"
        
        # æ„å»ºå¯¹è±¡åç§°ï¼ˆå»æ‰key=valueæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨å€¼ï¼‰
        object_name = f"bronze/zh_stock/{doc_type}/{year}/{quarter}/{code}/{filename}"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        try:
            client.stat_object(minio_bucket, object_name)
            logging.info(f"[{code}] æ–‡ä»¶å·²åœ¨MinIOä¸­å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {object_name}")
            return
        except S3Error:
            pass  # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­ä¸Šä¼ 
        
        # ä¸Šä¼ æ–‡ä»¶ï¼ˆå¸¦å…ƒæ•°æ®ï¼‰
        try:
            # å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                "stock_code": code,
                "year": str(year),
                "quarter": quarter,
            }
            if publish_date:
                metadata["publish_date"] = publish_date
            
            client.fput_object(
                minio_bucket,
                object_name,
                local_path,
                content_type='application/pdf',
                metadata=metadata
            )
            logging.info(f"[{code}] âœ… å·²ä¸Šä¼ åˆ°MinIO: {object_name} (å‘å¸ƒæ—¥æœŸ: {publish_date or 'N/A'})")
        except S3Error as e:
            # ä¸Šä¼ å¤±è´¥ï¼Œè®°å½•åˆ°å¤±è´¥æ–‡ä»¶
            error_msg = str(e)
            logging.error(f"[{code}] âŒ MinIOä¸Šä¼ å¤±è´¥: {error_msg}")
            _record_minio_upload_failure(code, year, quarter, local_path, error_msg, out_root)
        except Exception as e:
            error_msg = str(e)
            logging.error(f"[{code}] âŒ MinIOä¸Šä¼ å¼‚å¸¸: {error_msg}")
            _record_minio_upload_failure(code, year, quarter, local_path, error_msg, out_root)
    
    except Exception as e:
        # ä»»ä½•å¼‚å¸¸éƒ½ä¸åº”è¯¥é˜»å¡ä¸‹è½½æµç¨‹
        logging.warning(f"[{code}] MinIOä¸Šä¼ å¤„ç†å¼‚å¸¸ï¼ˆä¸å½±å“ä¸‹è½½ï¼‰: {e}")


def _record_minio_upload_failure(code: str, year: int, quarter: str, file_path: str, 
                                 error_msg: str, out_root: str):
    """
    è®°å½•MinIOä¸Šä¼ å¤±è´¥
    
    Args:
        code: è‚¡ç¥¨ä»£ç 
        year: å¹´ä»½
        quarter: å­£åº¦
        file_path: æ–‡ä»¶è·¯å¾„
        error_msg: é”™è¯¯ä¿¡æ¯
        out_root: è¾“å‡ºæ ¹ç›®å½•
    """
    try:
        fail_csv = os.path.join(out_root, "minio_upload_failed.csv")
        file_exists = os.path.exists(fail_csv)
        
        with open(fail_csv, 'a', encoding=CSV_ENCODING, newline='', errors='replace') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(['code', 'year', 'quarter', 'file_path', 'error_message', 'timestamp'])
            writer.writerow([code, year, quarter, file_path, error_msg, datetime.now().isoformat()])
    except Exception as e:
        logging.warning(f"è®°å½•MinIOä¸Šä¼ å¤±è´¥ä¿¡æ¯æ—¶å‡ºé”™: {e}")


def process_single_task(task_data: Tuple) -> Tuple[bool, Optional[Tuple]]:
    """
    å¤„ç†å•ä¸ªä¸‹è½½ä»»åŠ¡ï¼ˆåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼‰

    Args:
        task_data: (code, name, year, quarter, out_root, orgid_cache_file, code_change_cache_file, shared_lock, existing_pdf_cache)

    Returns:
        (success, failure_record or None)
    """
    code, name, year, quarter, out_root, orgid_cache_file, code_change_cache_file, shared_lock, existing_pdf_cache = task_data

    # æ¯ä¸ªè¿›ç¨‹åˆ›å»ºè‡ªå·±çš„ session
    api_session = make_session(HEADERS_API)
    html_session = make_session(HEADERS_HTML)

    # checkpoint å­˜å‚¨åœ¨è¾“å‡ºç›®å½•
    checkpoint_file = os.path.join(out_root, "checkpoint.json")

    # åˆ›å»ºå…±äº«çŠ¶æ€ç®¡ç†å™¨ï¼ˆSQLite æˆ– JSON+é”ï¼‰
    if USE_SQLITE:
        shared = SharedStateSQLite(checkpoint_file, orgid_cache_file, code_change_cache_file)
    else:
        shared = SharedState(checkpoint_file, orgid_cache_file, code_change_cache_file, shared_lock)

    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    key = f"{code}-{year}-{quarter}"
    checkpoint = shared.load_checkpoint()
    if checkpoint.get(key):
        return True, None

    # æ£€æŸ¥PDFæ˜¯å¦å·²å­˜åœ¨äºæ—§ç›®å½•ï¼ˆä½¿ç”¨ç¼“å­˜ï¼ŒO(1)æŸ¥æ‰¾ï¼‰
    if existing_pdf_cache and check_pdf_exists_in_cache(existing_pdf_cache, code, year, quarter):
        logging.info(f"[{code}] PDFå·²å­˜åœ¨äºæ—§ç›®å½•ï¼Œè·³è¿‡ä¸‹è½½")
        shared.save_checkpoint(key)  # æ ‡è®°ä¸ºå·²å®Œæˆï¼Œé¿å…é‡å¤æ£€æŸ¥
        return True, None

    exch_dir, column_api, _ = detect_exchange(code)

    # ç”¨äºä¿å­˜ä»APIè·å–çš„çœŸå®å…¬å¸åï¼ˆç”¨äºå¤±è´¥è®°å½•ï¼‰
    real_company_name = name

    try:
        # orgId è·å–ç­–ç•¥
        orgId = shared.get_orgid(code)
        if not orgId:
            orgId = build_orgid(code)
            shared.save_orgid(code, orgId)
            logging.info(f"[{code}] orgId æ„é€ æ–¹æ³•ï¼š{orgId}")

        # åŠ è½½ä»£ç å˜æ›´ç¼“å­˜
        code_change_cache = shared.load_code_change_cache()
        related_codes = get_all_related_codes(code, orgId, code_change_cache)

        # æŠ“å…¬å‘Š - ä½¿ç”¨ç±»åˆ«è¿‡æ»¤ï¼ˆå§‹ç»ˆå…ˆå°è¯•å¸¦orgIdæŸ¥è¯¢ï¼‰
        anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

        # å…œåº•1ï¼šå¦‚æœç±»åˆ«è¿‡æ»¤æ— ç»“æœï¼Œå°è¯•ä½¿ç”¨ CATEGORY_ALLï¼ˆå¯èƒ½æ˜¯ç±»åˆ«åˆ†ç±»é—®é¢˜ï¼‰
        if not anns:
            logging.warning(f"[{code}] ç±»åˆ«è¿‡æ»¤æŸ¥è¯¢æ— ç»“æœï¼Œå°è¯•ä½¿ç”¨ CATEGORY_ALL")
            anns = fetch_anns(api_session, code, orgId, column_api, year, quarter)  # fetch_anns uses CATEGORY_ALL
            if anns:
                logging.info(f"[{code}] CATEGORY_ALLæŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(anns)} æ¡å…¬å‘Š")

        # å…œåº•2ï¼šå¦‚æœå¸¦orgIdæŸ¥è¯¢æ— ç»“æœï¼Œå°è¯•ä¸å¸¦orgIdæŸ¥è¯¢ï¼ˆå¤„ç†åˆå¹¶/é‡ç»„åœºæ™¯ï¼ŒorgIdå˜æ›´ï¼‰
        if not anns and orgId:
            logging.warning(f"[{code}] å¸¦orgIdæŸ¥è¯¢æ— ç»“æœï¼Œå°è¯•ä¸å¸¦orgIdæŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯å…¬å¸åå˜æ›´ï¼‰")
            anns = fetch_anns(api_session, code, None, column_api, year, quarter)  # Use CATEGORY_ALL without orgId
            if anns:
                logging.info(f"[{code}] ä¸å¸¦orgIdæŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(anns)} æ¡å…¬å‘Š")
        
        # æ£€æµ‹ä»£ç å˜æ›´
        if anns:
            actual_code = detect_code_change(anns, code, orgId)
            if actual_code and actual_code not in related_codes:
                logging.info(f"[{code}] æ£€æµ‹åˆ°ä»£ç å˜æ›´ï¼š{code} -> {actual_code}")
                related_codes.append(actual_code)
                # æ›´æ–°ç¼“å­˜
                shared.save_code_change(orgId, related_codes)
        
        if not anns:
            # å°è¯•é€šè¿‡æœç´¢APIè·å–çœŸå® orgIdï¼ˆæ–°æ–¹æ³•ï¼Œä¸éœ€è¦å…¬å¸åï¼‰
            logging.warning(f"[{code}] æ„é€ çš„ orgId å¯èƒ½æ— æ•ˆï¼Œå°è¯•æœç´¢APIæ–¹æ³•...")
            result = get_orgid_via_search_api(api_session, code)
            if result:
                real_orgid, company_name = result
                real_company_name = company_name  # ä¿å­˜çœŸå®å…¬å¸å
                if real_orgid != orgId:
                    logging.info(f"[{code}] orgId æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid}")
                    orgId = real_orgid
                    shared.save_orgid(code, orgId)
                    anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

            # å¦‚æœæœç´¢APIä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨searchkeyæ–¹æ³•ï¼ˆéœ€è¦å…¬å¸åï¼‰
            if not anns:
                logging.warning(f"[{code}] æœç´¢APIå¤±è´¥ï¼Œå°è¯• searchkey æ–¹æ³•...")
                searchkey_result = get_orgid_by_searchkey(api_session, code, name, column_api)
                if searchkey_result:
                    real_orgid, _ = searchkey_result
                    if real_orgid and real_orgid != orgId:
                        logging.info(f"[{code}] orgId æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid}")
                        orgId = real_orgid
                        shared.save_orgid(code, orgId)
                        anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

            # æ–¹æ³•4ï¼šæœ€åçš„å…œåº• - HTML é¡µé¢è§£æï¼ˆä¸ä¾èµ–å…¬å¸åï¼Œæœ€å¯é ï¼‰
            if not anns:
                logging.warning(f"[{code}] searchkey æ–¹æ³•ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨ HTML å…œåº•æ–¹æ³•...")
                html_result = get_orgid_via_html(code, name, html_session)
                if html_result:
                    real_orgid, company_name = html_result
                    if real_orgid != orgId:
                        logging.info(f"[{code}] [HTMLå…œåº•] orgId æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid} ({company_name})")
                        orgId = real_orgid
                        shared.save_orgid(code, orgId)
                        # ç”¨æ–° orgId é‡æ–°æŠ“å–
                        anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

        if not anns:
            logging.error(f"[{code}] æœªè·å¾—å…¬å‘Šåˆ—è¡¨ï¼š{real_company_name}ï¼ˆ{code}ï¼‰{year}-{quarter}")
            return False, (code, real_company_name, year, quarter, "no announcements")

        # å–æœ€æ–°ç‰ˆï¼ˆä½¿ç”¨ç›¸å…³ä»£ç åˆ—è¡¨ï¼‰
        best = pick_latest(anns, code, year, quarter, related_codes=related_codes)
        if not best:
            logging.error(f"[{code}] å…¬å‘Šè¿‡æ»¤åä¸ºç©ºï¼š{real_company_name}ï¼ˆ{code}ï¼‰{year}-{quarter}")
            return False, (code, real_company_name, year, quarter, "not found after filter")

        # ä¸‹è½½
        adj = best.get("adjunctUrl", "")
        pdf_url = pdf_url_from_adj(adj)
        ts = parse_time_to_ms(best.get("announcementTime"))
        pub_date = ms_to_ddmmyyyy(ts)  # å‘å¸ƒæ—¥æœŸï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰
        pub_date_iso = datetime.fromtimestamp(ts/1000).isoformat() if ts > 0 else None  # ISOæ ¼å¼æ—¥æœŸ

        out_dir = os.path.join(out_root, exch_dir, code, str(year))
        ensure_dir(out_dir)
        # æ–‡ä»¶åä¸åŒ…å«å‘å¸ƒæ—¥æœŸï¼ŒåªåŒ…å«ï¼šè‚¡ç¥¨ä»£ç _å¹´ä»½_å­£åº¦.pdf
        fname = f"{code}_{year}_{quarter}.pdf"
        out_path = os.path.join(out_dir, fname)

        # å®šä¹‰åˆ·æ–°å‡½æ•°ï¼ˆå…ˆå°è¯•å¸¦orgIdï¼Œå¤±è´¥åˆ™ä¸å¸¦orgIdï¼‰
        def refresh_fn():
            anns2 = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)
            if not anns2 and orgId:
                anns2 = fetch_anns_by_category(api_session, code, None, column_api, year, quarter)
            b2 = pick_latest(anns2, code, year, quarter, related_codes=related_codes)
            if not b2: return None, None
            return pdf_url_from_adj(b2.get("adjunctUrl", "")), None

        ok, msg = download_pdf_resilient(html_session, pdf_url, out_path, referer=None,
                                        refresh_fn=refresh_fn, max_retries=3)
        if ok:
            logging.info(f"[{code}] ä¿å­˜æˆåŠŸï¼š{out_path}")
            shared.save_checkpoint(key)
            
            # ç«‹å³ä¸Šä¼ åˆ°MinIOï¼ˆå¦‚æœå¯ç”¨ï¼‰
            # quarter åº”è¯¥æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆ"Q1", "Q2"ç­‰ï¼‰ï¼Œå¦‚æœä¸æ˜¯åˆ™è½¬æ¢
            quarter_str = quarter if isinstance(quarter, str) else f"Q{quarter}"
            upload_to_minio_in_process(out_path, code, year, quarter_str, out_root, pub_date_iso)
            
            return True, None
        else:
            logging.error(f"[{code}] ä¸‹è½½å¤±è´¥ï¼š{real_company_name}ï¼ˆ{code}ï¼‰{year}-{quarter} - {msg}")
            return False, (code, real_company_name, year, quarter, f"download failed: {msg}")

    except Exception as e:
        logging.error(f"[{code}] å¤„ç†å¼‚å¸¸ï¼š{real_company_name}ï¼ˆ{code}ï¼‰{year}-{quarter} - {e}")
        return False, (code, real_company_name, year, quarter, f"exception: {str(e)}")
    finally:
        # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¿‡åº¦è¯·æ±‚
        time.sleep(random.uniform(0.5, 1.5))

def run_multiprocessing(input_csv: str, out_root: str, fail_csv: str,
                       workers: int = 4, debug=False, old_pdf_dir: Optional[str] = None,
                       default_year: Optional[int] = None, default_quarter: Optional[str] = None):
    """
    å¤šè¿›ç¨‹å¹¶è¡Œä¸‹è½½æ¨¡å¼

    Args:
        input_csv: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
        out_root: è¾“å‡ºæ ¹ç›®å½•
        fail_csv: å¤±è´¥è®°å½•CSVè·¯å¾„
        workers: workerè¿›ç¨‹æ•° (å»ºè®® 4-8)
        debug: è°ƒè¯•æ¨¡å¼
        old_pdf_dir: æ—§PDFç›®å½•è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼Œä¼šå…ˆæ£€æŸ¥è¯¥ç›®å½•é¿å…é‡å¤ä¸‹è½½ï¼‰
        default_year: é»˜è®¤å¹´ä»½ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰yearåˆ—ï¼‰
        default_quarter: é»˜è®¤å­£åº¦ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰quarteråˆ—ï¼‰
    """
    if debug:
        logger.setLevel(logging.DEBUG)
        logging.info("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    tasks = read_tasks_from_csv(input_csv, default_year, default_quarter)
    total = len(tasks)
    print(f"å…±è¯»å–ä»»åŠ¡ï¼š{total} æ¡")
    print(f"ä½¿ç”¨ {workers} ä¸ªå¹¶è¡Œè¿›ç¨‹å¤„ç†")

    if USE_SQLITE:
        print("âœ… ä½¿ç”¨ SQLite çŠ¶æ€ç®¡ç†ï¼ˆé«˜æ€§èƒ½ã€æ— æ–‡ä»¶é”å†²çªï¼‰")
    else:
        print("âš ï¸  ä½¿ç”¨ JSON çŠ¶æ€ç®¡ç†ï¼ˆå»ºè®®å‡çº§åˆ° SQLiteï¼‰")

    # æ„å»ºå·²å­˜åœ¨PDFçš„ç¼“å­˜ï¼ˆä¸€æ¬¡æ€§æ‰«æï¼Œæ‰€æœ‰è¿›ç¨‹å…±äº«ï¼‰
    existing_pdf_cache = build_existing_pdf_cache(old_pdf_dir)

    # åˆ›å»ºè·¨è¿›ç¨‹å…±äº«çš„é”
    manager = multiprocessing.Manager()
    shared_lock = manager.Lock()

    # å‡†å¤‡ä»»åŠ¡æ•°æ®ï¼ˆåŒ…å«å…±äº«é”å’ŒPDFç¼“å­˜ï¼‰
    task_data_list = [
        (code, name, year, quarter, out_root, ORGID_CACHE_FILE, CODE_CHANGE_CACHE_FILE, shared_lock, existing_pdf_cache)
        for code, name, year, quarter in tasks
    ]

    failures = []
    completed = 0

    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {
            executor.submit(process_single_task, task_data): task_data
            for task_data in task_data_list
        }

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=total, desc="æŠ“å–è¿›åº¦", unit="ä»»åŠ¡") as pbar:
            for future in as_completed(future_to_task):
                task_data = future_to_task[future]
                code, name, year, quarter = task_data[:4]

                try:
                    success, failure_record = future.result()
                    if success:
                        completed += 1
                    elif failure_record:
                        failures.append(failure_record)
                except Exception as e:
                    logging.error(f"ä»»åŠ¡å¤„ç†å¼‚å¸¸ï¼š{name}ï¼ˆ{code}ï¼‰{year}-{quarter} - {e}")
                    failures.append((code, name, year, quarter, f"exception: {str(e)}"))

                pbar.update(1)

    # å†™å…¥å¤±è´¥è®°å½•
    if failures:
        with open(fail_csv, "w", encoding=CSV_ENCODING, newline="", errors="replace") as f:
            w = csv.writer(f)
            w.writerow(["code", "name", "year", "quarter", "reason"])
            w.writerows(failures)
        print(f"âŒ å†™å…¥å¤±è´¥è®°å½•ï¼š{fail_csv}ï¼ˆç¼–ç ï¼š{CSV_ENCODING}ï¼‰")
        print(f"âœ… æˆåŠŸï¼š{completed}/{total} ({completed*100//total}%)")
    else:
        print("âœ… å…¨éƒ¨æˆåŠŸï¼Œæ— å¤±è´¥è®°å½•ã€‚")

# ----------------------- ä¸»æµç¨‹ -----------------------
def run(input_csv: str, out_root: str, fail_csv: str, watch_log=False, debug=False, old_pdf_dir: Optional[str] = None,
        default_year: Optional[int] = None, default_quarter: Optional[str] = None):
    if debug:
        logger.setLevel(logging.DEBUG)
        logging.info("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    if watch_log:
        start_tail()

    api_session = make_session(HEADERS_API)
    html_session = make_session(HEADERS_HTML)

    # checkpoint å­˜å‚¨åœ¨è¾“å‡ºç›®å½•
    checkpoint_file = os.path.join(out_root, "checkpoint.json")

    # ä½¿ç”¨ SQLite æˆ– JSON
    if USE_SQLITE:
        state = SharedStateSQLite(checkpoint_file, ORGID_CACHE_FILE, CODE_CHANGE_CACHE_FILE)
        checkpoint = state.load_checkpoint()
        orgid_cache = state.load_orgid_cache()
        code_change_cache = state.load_code_change_cache()
        print("âœ… ä½¿ç”¨ SQLite çŠ¶æ€ç®¡ç†")
    else:
        state = None
        checkpoint = load_json(checkpoint_file, {})
        orgid_cache = load_json(ORGID_CACHE_FILE, {})
        code_change_cache = load_json(CODE_CHANGE_CACHE_FILE, {})
        print("âš ï¸  ä½¿ç”¨ JSON çŠ¶æ€ç®¡ç†")

    tasks = read_tasks_from_csv(input_csv, default_year, default_quarter)
    total = len(tasks)
    print(f"å…±è¯»å–ä»»åŠ¡ï¼š{total} æ¡")

    # æ„å»ºå·²å­˜åœ¨PDFçš„ç¼“å­˜ï¼ˆä¸€æ¬¡æ€§æ‰«æï¼‰
    existing_pdf_cache = build_existing_pdf_cache(old_pdf_dir)

    failures = []
    last_code = None

    for code, name, year, quarter in tqdm(tasks, desc="æŠ“å–è¿›åº¦", unit="ä»»åŠ¡"):
        key = f"{code}-{year}-{quarter}"
        if checkpoint.get(key):
            continue

        # æ£€æŸ¥PDFæ˜¯å¦å·²å­˜åœ¨äºæ—§ç›®å½•ï¼ˆä½¿ç”¨ç¼“å­˜ï¼ŒO(1)æŸ¥æ‰¾ï¼‰
        if existing_pdf_cache and check_pdf_exists_in_cache(existing_pdf_cache, code, year, quarter):
            logging.info(f"[{code}] PDFå·²å­˜åœ¨äºæ—§ç›®å½•ï¼Œè·³è¿‡ä¸‹è½½")
            if USE_SQLITE and state:
                state.save_checkpoint(key)
            else:
                checkpoint[key] = True
                save_json(checkpoint_file, checkpoint)
            continue

        if last_code == code:
            time.sleep(INTER_SAME_STOCK_GAP)
        last_code = code

        exch_dir, column_api, stock_suffix = detect_exchange(code)
        logging.info(f"æ­£åœ¨æŠ“å–ï¼š{name}ï¼ˆ{code}ï¼‰ {year}-{quarter} [{column_api}]")

        # ç”¨äºä¿å­˜ä»APIè·å–çš„çœŸå®å…¬å¸åï¼ˆç”¨äºå¤±è´¥è®°å½•ï¼‰
        real_company_name = name

        # orgId è·å–ç­–ç•¥ï¼šå…ˆä»ç¼“å­˜ï¼Œå†å°è¯•æ„é€ ï¼Œæœ€åç”¨ searchkey æŸ¥è¯¢
        orgId = orgid_cache.get(code)
        if not orgId:
            # æ–¹æ³•1ï¼šå°è¯•ç®€å•æ„é€ ï¼ˆå¿«é€Ÿï¼Œä½†ä»…é€‚ç”¨éƒ¨åˆ†å…¬å¸ï¼‰
            orgId = build_orgid(code)
            if USE_SQLITE and state:
                state.save_orgid(code, orgId)
            else:
                orgid_cache[code] = orgId
                save_json(ORGID_CACHE_FILE, orgid_cache)
            logging.info(f"[orgId] æ„é€ æ–¹æ³•ï¼š{orgId}")

        # è·å–ç›¸å…³ä»£ç åˆ—è¡¨ï¼ˆå¤„ç†ä»£ç å˜æ›´ï¼‰
        related_codes = get_all_related_codes(code, orgId, code_change_cache)

        # æŠ“å…¬å‘Šï¼ˆå¤šçª— + åˆ†é¡µï¼‰- ä½¿ç”¨ç±»åˆ«è¿‡æ»¤ï¼ˆå§‹ç»ˆå…ˆå°è¯•å¸¦orgIdæŸ¥è¯¢ï¼‰
        anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

        # DEBUG: æ‰“å°æ‰€æœ‰è¿”å›çš„å…¬å‘Šæ ‡é¢˜
        if anns:
            logging.info(f"[DEBUG] è¿”å› {len(anns)} æ¡å…¬å‘Š:")
            for ann in anns[:10]:  # åªæ˜¾ç¤ºå‰10æ¡
                logging.info(f"  - {ann.get('secName')} {ann.get('announcementTitle')}")

        # å…œåº•1ï¼šå¦‚æœç±»åˆ«è¿‡æ»¤æ— ç»“æœï¼Œå°è¯•ä½¿ç”¨ CATEGORY_ALLï¼ˆå¯èƒ½æ˜¯ç±»åˆ«åˆ†ç±»é—®é¢˜ï¼‰
        if not anns:
            logging.warning(f"[{code}] ç±»åˆ«è¿‡æ»¤æŸ¥è¯¢æ— ç»“æœï¼Œå°è¯•ä½¿ç”¨ CATEGORY_ALL")
            anns = fetch_anns(api_session, code, orgId, column_api, year, quarter)  # fetch_anns uses CATEGORY_ALL
            if anns:
                logging.info(f"[{code}] CATEGORY_ALLæŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(anns)} æ¡å…¬å‘Š")

        # å…œåº•2ï¼šå¦‚æœå¸¦orgIdæŸ¥è¯¢æ— ç»“æœï¼Œå°è¯•ä¸å¸¦orgIdæŸ¥è¯¢ï¼ˆå¤„ç†åˆå¹¶/é‡ç»„åœºæ™¯ï¼ŒorgIdå˜æ›´ï¼‰
        if not anns and orgId:
            logging.warning(f"[{code}] å¸¦orgIdæŸ¥è¯¢æ— ç»“æœï¼Œå°è¯•ä¸å¸¦orgIdæŸ¥è¯¢ï¼ˆå¯èƒ½æ˜¯å…¬å¸åå˜æ›´ï¼‰")
            anns = fetch_anns(api_session, code, None, column_api, year, quarter)  # Use CATEGORY_ALL without orgId
            if anns:
                logging.info(f"[{code}] ä¸å¸¦orgIdæŸ¥è¯¢æˆåŠŸï¼Œè¿”å› {len(anns)} æ¡å…¬å‘Š")

        # æ£€æµ‹ä»£ç å˜æ›´
        if anns:
            actual_code = detect_code_change(anns, code, orgId)
            if actual_code and actual_code not in related_codes:
                logging.info(f"[ä»£ç å˜æ›´æ£€æµ‹] {code} -> {actual_code}")
                related_codes.append(actual_code)
                # æ›´æ–°ç¼“å­˜
                if orgId:
                    if USE_SQLITE and state:
                        state.save_code_change(orgId, related_codes)
                    else:
                        if orgId not in code_change_cache:
                            code_change_cache[orgId] = []
                        for c in related_codes:
                            if c not in code_change_cache[orgId]:
                                code_change_cache[orgId].append(c)
                        save_json(CODE_CHANGE_CACHE_FILE, code_change_cache)
        
        if not anns:
            # æ–¹æ³•2ï¼šå¦‚æœæ„é€ çš„ orgId æ— æ•ˆï¼Œå°è¯•é€šè¿‡æœç´¢APIè·å–çœŸå® orgIdï¼ˆä¸éœ€è¦å…¬å¸åï¼‰
            logging.warning(f"æ„é€ çš„ orgId å¯èƒ½æ— æ•ˆï¼Œå°è¯•æœç´¢APIæ–¹æ³•...")
            result = get_orgid_via_search_api(api_session, code)
            if result:
                real_orgid, api_company_name = result
                real_company_name = api_company_name  # ä¿å­˜çœŸå®å…¬å¸å
                if real_orgid != orgId:
                    logging.info(f"[orgId] æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid}")
                    orgId = real_orgid
                    if USE_SQLITE and state:
                        state.save_orgid(code, orgId)
                    else:
                        orgid_cache[code] = orgId
                        save_json(ORGID_CACHE_FILE, orgid_cache)
                    # ç”¨æ–° orgId é‡æ–°æŠ“å–
                    anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

            # å¦‚æœæœç´¢APIä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨searchkeyæ–¹æ³•ï¼ˆéœ€è¦å…¬å¸åï¼‰
            if not anns:
                logging.warning(f"æœç´¢APIå¤±è´¥ï¼Œå°è¯• searchkey æ–¹æ³•...")
                searchkey_result = get_orgid_by_searchkey(api_session, code, name, column_api)
                if searchkey_result:
                    real_orgid, _ = searchkey_result
                    if real_orgid and real_orgid != orgId:
                        logging.info(f"[orgId] æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid}")
                        orgId = real_orgid
                        if USE_SQLITE and state:
                            state.save_orgid(code, orgId)
                        else:
                            orgid_cache[code] = orgId
                            save_json(ORGID_CACHE_FILE, orgid_cache)
                        # ç”¨æ–° orgId é‡æ–°æŠ“å–
                        anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

            # æ–¹æ³•4ï¼šæœ€åçš„å…œåº• - HTML é¡µé¢è§£æï¼ˆä¸ä¾èµ–å…¬å¸åï¼Œæœ€å¯é ï¼‰
            if not anns:
                logging.warning(f"searchkey æ–¹æ³•ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨ HTML å…œåº•æ–¹æ³•...")
                html_result = get_orgid_via_html(code, name, html_session)
                if html_result:
                    real_orgid, company_name = html_result
                    if real_orgid != orgId:
                        logging.info(f"[HTMLå…œåº•] orgId æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid} ({company_name})")
                        orgId = real_orgid
                        if USE_SQLITE and state:
                            state.save_orgid(code, orgId)
                        else:
                            orgid_cache[code] = orgId
                            save_json(ORGID_CACHE_FILE, orgid_cache)
                        # ç”¨æ–° orgId é‡æ–°æŠ“å–
                        anns = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)

        if not anns:
            logging.error(f"æœªè·å¾—å…¬å‘Šåˆ—è¡¨ï¼š{real_company_name}ï¼ˆ{code}ï¼‰{year}-{quarter}")
            failures.append((code, real_company_name, year, quarter, "no announcements"))
            time.sleep(random.uniform(*INTER_COMBO_SLEEP_RANGE))
            continue

        # å–æœ€æ–°ç‰ˆï¼ˆä½¿ç”¨ç›¸å…³ä»£ç åˆ—è¡¨å¤„ç†ä»£ç å˜æ›´ï¼‰
        best = pick_latest(anns, code, year, quarter, related_codes=related_codes)
        if not best:
            logging.error(f"å…¬å‘Šè¿‡æ»¤åä¸ºç©ºï¼š{real_company_name}ï¼ˆ{code}ï¼‰{year}-{quarter}")
            failures.append((code, real_company_name, year, quarter, "not found after filter"))
            time.sleep(random.uniform(*INTER_COMBO_SLEEP_RANGE))
            continue

        # ä¸‹è½½
        adj = best.get("adjunctUrl","")
        pdf_url = pdf_url_from_adj(adj)
        ts = parse_time_to_ms(best.get("announcementTime"))
        pub_date = ms_to_ddmmyyyy(ts)  # å‘å¸ƒæ—¥æœŸï¼ˆç”¨äºæ—¥å¿—ï¼‰
        pub_date_iso = datetime.fromtimestamp(ts/1000).isoformat() if ts > 0 else None  # ISOæ ¼å¼æ—¥æœŸï¼ˆç”¨äºå…ƒæ•°æ®ï¼‰

        out_dir = os.path.join(out_root, exch_dir, code, str(year))
        ensure_dir(out_dir)
        # æ–‡ä»¶åä¸åŒ…å«å‘å¸ƒæ—¥æœŸï¼ŒåªåŒ…å«ï¼šè‚¡ç¥¨ä»£ç _å¹´ä»½_å­£åº¦.pdf
        fname = f"{code}_{year}_{quarter}.pdf"
        out_path = os.path.join(out_dir, fname)

        # å®šä¹‰åˆ·æ–°å‡½æ•°ï¼ˆ404 æ—¶é‡æ–°æ‹‰åˆ—è¡¨ï¼Œå…ˆå°è¯•å¸¦orgIdï¼Œå¤±è´¥åˆ™ä¸å¸¦orgIdï¼‰
        def refresh_fn():
            anns2 = fetch_anns_by_category(api_session, code, orgId, column_api, year, quarter)
            if not anns2 and orgId:
                anns2 = fetch_anns_by_category(api_session, code, None, column_api, year, quarter)
            b2 = pick_latest(anns2, code, year, quarter, related_codes=related_codes)
            if not b2: return None, None
            return pdf_url_from_adj(b2.get("adjunctUrl","")), None

        logging.debug(f"Downloading: url={pdf_url} -> {out_path}")
        ok, msg = download_pdf_resilient(html_session, pdf_url, out_path, referer=None, refresh_fn=refresh_fn, max_retries=3)
        if ok:
            logging.info(f"ä¿å­˜æˆåŠŸï¼š{out_path}")
            if USE_SQLITE and state:
                state.save_checkpoint(key)
            else:
                checkpoint[key] = True
                save_json(checkpoint_file, checkpoint)
        else:
            logging.error(f"ä¸‹è½½å¤±è´¥ï¼š{real_company_name}ï¼ˆ{code}ï¼‰{year}-{quarter} - {msg}")
            failures.append((code, real_company_name, year, quarter, f"download failed: {msg}"))

        time.sleep(random.uniform(*INTER_COMBO_SLEEP_RANGE))

    # å¤±è´¥è®°å½•
    if failures:
        with open(fail_csv, "w", encoding=CSV_ENCODING, newline="", errors="replace") as f:
            w = csv.writer(f)
            w.writerow(["code", "name", "year", "quarter", "reason"])
            w.writerows(failures)
        print(f"âŒ å†™å…¥å¤±è´¥è®°å½•ï¼š{fail_csv}ï¼ˆç¼–ç ï¼š{CSV_ENCODING}ï¼‰")
    else:
        print("âœ… å…¨éƒ¨æˆåŠŸï¼Œæ— å¤±è´¥è®°å½•ã€‚")

# ----------------------- CLI -----------------------
if __name__ == "__main__":
    p = argparse.ArgumentParser(description="CNINFO å®šæœŸæŠ¥å‘ŠæŠ“å–ï¼ˆå«æ–° orgId è·å–ç­–ç•¥ + å¤šè¿›ç¨‹æ”¯æŒï¼‰")
    p.add_argument("--input", required=True, help="è¾“å…¥ CSVï¼ˆcode,name æˆ– code,name,year,quarterï¼‰")
    p.add_argument("--out", required=True, help="è¾“å‡ºæ ¹ç›®å½•")
    p.add_argument("--fail", required=True, help="å¤±è´¥è®°å½• CSV")
    p.add_argument("--year", type=int, default=None, help="é»˜è®¤å¹´ä»½ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰yearåˆ—ï¼‰")
    p.add_argument("--quarter", type=str, default=None, choices=["Q1", "Q2", "Q3", "Q4"], help="é»˜è®¤å­£åº¦ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰quarteråˆ—ï¼‰")
    p.add_argument("--workers", type=int, default=0, help="å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ0=é¡ºåºæ¨¡å¼ï¼Œæ¨è 4-8ï¼‰")
    p.add_argument("--old-pdf-dir", type=str, default=None, help="æ—§PDFç›®å½•è·¯å¾„ï¼ˆå¦‚å­˜åœ¨ï¼Œä¼šå…ˆæ£€æŸ¥é¿å…é‡å¤ä¸‹è½½ï¼‰")
    p.add_argument("--watch-log", action="store_true", help="å®æ—¶æ»šåŠ¨æ˜¾ç¤º error.logï¼ˆä»…é¡ºåºæ¨¡å¼ï¼‰")
    p.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼ï¼ˆè¾“å‡ºæ›´å¤šæ—¥å¿—ï¼‰")
    args = p.parse_args()

    # å¤šè¿›ç¨‹æ¨¡å¼ or é¡ºåºæ¨¡å¼
    if args.workers > 0:
        if args.watch_log:
            print("âš ï¸  å¤šè¿›ç¨‹æ¨¡å¼ä¸‹ä¸æ”¯æŒ --watch-logï¼Œå·²å¿½ç•¥")
        run_multiprocessing(args.input, args.out, args.fail, workers=args.workers, debug=args.debug, 
                          old_pdf_dir=args.old_pdf_dir, default_year=args.year, default_quarter=args.quarter)
    else:
        run(args.input, args.out, args.fail, watch_log=args.watch_log, debug=args.debug, 
            old_pdf_dir=args.old_pdf_dir, default_year=args.year, default_quarter=args.quarter)
