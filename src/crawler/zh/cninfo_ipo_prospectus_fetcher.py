# -*- coding: utf-8 -*-
"""
CNINFO IPOæ‹›è‚¡è¯´æ˜ä¹¦æŠ“å–å·¥å…·ï¼ˆåŸºäºåŸç‰ˆå®šæœŸæŠ¥å‘ŠæŠ“å–è„šæœ¬æ”¹é€ ï¼‰
- ç›®æ ‡ï¼šæŠ“å–IPOæ‹›è‚¡è¯´æ˜ä¹¦ï¼ˆé¦–æ¬¡å…¬å¼€å‘è¡Œè‚¡ç¥¨æ‹›è‚¡è¯´æ˜ä¹¦ï¼‰
- orgId è·å–ï¼š
  1) ä¼˜å…ˆï¼šæœç´¢APIç›´æ¥è·å–
  2) å…œåº•ï¼šHTMLé¡µé¢è§£æ
- äº¤æ˜“æ‰€æ˜ å°„ï¼šä¿å­˜ç›®å½• "SZ/SH/BJ"ï¼Œæ¥å£ column "szse/sse/bse"
- æ–­ç‚¹ç»­æŠ“ / orgId ç¼“å­˜ / æ»šåŠ¨é”™è¯¯æ—¥å¿— / Windows å‹å¥½ CSV
- å¤šè¿›ç¨‹å¹¶å‘ï¼šæ”¯æŒå¤šä¸ª worker è¿›ç¨‹å¹¶è¡Œä¸‹è½½
"""

import os
import re
import csv
import sys
import json
import time
import random
import atexit
import logging
import argparse
import platform
import threading
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
from typing import Dict, Tuple, Optional, List
from urllib.parse import urlencode, urljoin, urlparse, parse_qs

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from requests.adapters import HTTPAdapter
from requests.exceptions import RequestException
from urllib3.util.retry import Retry

# ----------------------- å¸¸é‡ä¸é…ç½® -----------------------
CNINFO_API = "https://www.cninfo.com.cn/new/hisAnnouncement/query"
CNINFO_STATIC = "https://static.cninfo.com.cn/"
CNINFO_SEARCH = "https://www.cninfo.com.cn/new/search"

# IPOæ‹›è‚¡è¯´æ˜ä¹¦ç±»åˆ«ï¼ˆé¦–æ¬¡å…¬å¼€å‘è¡Œå…¬å‘Šï¼‰
CATEGORY_IPO = "category_scgkfxgg_szsh;"

HEADERS_API = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/json, text/javascript, */*; q=0.01",
    "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8",
    "Referer": "https://www.cninfo.com.cn/",
    "X-Requested-With": "XMLHttpRequest",
}
HEADERS_HTML = {
    "User-Agent": HEADERS_API["User-Agent"],
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Referer": "https://www.cninfo.com.cn/",
}

# å¦‚éœ€ä»£ç†ï¼ŒæŒ‰éœ€å¡«å†™ï¼›é»˜è®¤ None
PROXIES = None
# PROXIES = {"http": "http://ip:port", "https": "http://ip:port"}

# æ‹›è‚¡è¯´æ˜ä¹¦å…³é”®è¯ï¼ˆç”¨äºæ ‡é¢˜åŒ¹é…ï¼‰
IPO_KEYWORDS = ("æ‹›è‚¡è¯´æ˜ä¹¦", "æ‹›è‚¡ä¹¦")
EXCLUDE_IN_TITLE = ("æ‘˜è¦", "è‹±æ–‡", "è‹±æ–‡ç‰ˆ", "æ›´æ­£", "è¡¥å……", "ä¿®è®¢", "æ³¨å†Œç¨¿",
                    "æç¤ºæ€§å…¬å‘Š", "æ„å‘ä¹¦", "é™„å½•", "ä¸Šå¸‚å…¬å‘Šä¹¦")

# çŠ¶æ€æ–‡ä»¶è·¯å¾„ï¼ˆä¿å­˜åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ï¼‰
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CHECKPOINT_FILE = os.path.join(SCRIPT_DIR, "checkpoint_ipo.json")
ORGID_CACHE_FILE = os.path.join(SCRIPT_DIR, "orgid_cache_ipo.json")

RETRY_STATUS = (403, 502, 503, 504)
RETRY_TIMES = 3
RETRY_BACKOFF = 1.0
INTER_COMBO_SLEEP_RANGE = (2.0, 3.0)  # ç»„åˆé—´ç¡çœ 
INTER_SAME_STOCK_GAP = 1.0            # åŒä¸€è‚¡ç¥¨é—´éš”

CSV_ENCODING = "gbk" if platform.system().lower().startswith("win") else "utf-8-sig"

# ----------------------- æ—¥å¿— -----------------------
ERROR_LOG_FILE = os.path.join(SCRIPT_DIR, "error_ipo.log")

logger = logging.getLogger()
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(formatter)
file_handler = logging.FileHandler(ERROR_LOG_FILE, encoding="utf-8")
file_handler.setFormatter(formatter)
logger.addHandler(console_handler)
logger.addHandler(file_handler)

_tail_thread = None
_tail_stop = threading.Event()

def _tail_worker(path: str):
    if not os.path.exists(path):
        with open(path, "w", encoding="utf-8") as _:
            pass
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        f.seek(0, os.SEEK_END)
        print(f"\nğŸ” æ­£åœ¨å®æ—¶ç›‘æ§ {os.path.basename(path)} æ—¥å¿—è¾“å‡º...\n")
        while not _tail_stop.is_set():
            line = f.readline()
            if not line:
                time.sleep(0.5); continue
            print(line.rstrip())

def start_tail():
    global _tail_thread
    _tail_thread = threading.Thread(target=_tail_worker, args=(ERROR_LOG_FILE,), daemon=True)
    _tail_thread.start()

def stop_tail():
    _tail_stop.set()

atexit.register(stop_tail)

# ----------------------- å·¥å…· -----------------------
def make_session(headers) -> requests.Session:
    s = requests.Session()
    s.headers.update(headers)
    retry = Retry(total=RETRY_TIMES, status_forcelist=list(RETRY_STATUS),
                  allowed_methods=frozenset(["GET", "POST"]),
                  backoff_factor=RETRY_BACKOFF, raise_on_status=False)
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter); s.mount("https://", adapter)

    # å…³é”®ï¼šè®¿é—®ä¸»é¡µå»ºç«‹ä¼šè¯ï¼Œè·å– JSESSIONID Cookie
    try:
        s.get("https://www.cninfo.com.cn/", timeout=10, proxies=PROXIES)
        logging.debug("ä¼šè¯å·²å»ºç«‹ï¼ˆè·å– JSESSIONIDï¼‰")
    except Exception as e:
        logging.warning(f"å»ºç«‹ä¼šè¯å¤±è´¥: {e}")

    return s

def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def normalize_code(code) -> str:
    s = str(code).strip()
    try:
        s = str(int(float(s)))
    except Exception:
        pass
    return s.zfill(6)

def detect_exchange(code: str) -> Tuple[str, str, str]:
    """
    è¿”å›: (exchange_dir, column_api, stock_suffix)
    - exchange_dir: ä¿å­˜ç›®å½•å "SZ"/"SH"/"BJ"
    - column_api:   æ¥å£ column "szse"/"sse"  (æ³¨æ„ï¼šä¸Šæµ·ç”¨ sse ä¸æ˜¯ shseï¼ŒåŒ—äº¬ä¹Ÿç”¨ szse)
    - stock_suffix: stock=code.<suffix> ç”¨ ".SZ/.SH/.BJ"
    """
    s = str(code)
    if s.startswith(("6", "68")):
        return "SH", "sse", "SH"  # ä¸Šæµ·ç”¨ sse
    if s.startswith(("8", "43", "83")):
        return "BJ", "szse", "BJ"  # åŒ—äº¬ä¹Ÿç”¨ szse
    return "SZ", "szse", "SZ"

def normalize_text(s: str) -> str:
    if not s: return ""
    import unicodedata, re as _re
    s = unicodedata.normalize("NFKC", s)
    return _re.sub(r"\s+", "", s).lower()

def build_se_window_for_ipo(lookback_years: int = 20) -> str:
    """
    ä¸ºIPOæ‹›è‚¡è¯´æ˜ä¹¦æ„å»ºæ—¶é—´çª—å£
    ç”±äºIPOæ—¶é—´ä¸ç¡®å®šï¼Œä½¿ç”¨è¾ƒå¤§çš„æ—¶é—´èŒƒå›´ï¼ˆé»˜è®¤æœ€è¿‘20å¹´ï¼‰
    """
    end_year = datetime.now().year
    start_year = end_year - lookback_years
    return f"{start_year}-01-01~{end_year}-12-31"

def title_ok_for_ipo(title: str) -> bool:
    """
    æ£€æŸ¥æ ‡é¢˜æ˜¯å¦ä¸ºæ‹›è‚¡è¯´æ˜ä¹¦
    - å¿…é¡»åŒ…å«"æ‹›è‚¡è¯´æ˜ä¹¦"æˆ–"æ‹›è‚¡ä¹¦"
    - æ’é™¤æ‘˜è¦ã€è‹±æ–‡ç‰ˆã€æ›´æ­£ç­‰
    """
    t = normalize_text(title)

    # æ’é™¤ä¸éœ€è¦çš„å…³é”®è¯
    if any(k in t for k in map(normalize_text, EXCLUDE_IN_TITLE)):
        return False

    # å¿…é¡»åŒ…å«æ‹›è‚¡è¯´æ˜ä¹¦å…³é”®è¯
    return any(k in t for k in map(normalize_text, IPO_KEYWORDS))

def parse_time_to_ms(t) -> int:
    if isinstance(t, (int, float)):
        return int(t)
    if isinstance(t, str):
        try:
            return int(datetime.strptime(t, "%Y-%m-%d %H:%M").timestamp() * 1000)
        except Exception:
            return 0
    return 0

def ms_to_ddmmyyyy(ms: int) -> str:
    try:
        return datetime.fromtimestamp(ms/1000).strftime("%d-%m-%Y")
    except Exception:
        return "NA"

def ms_to_year(ms: int) -> str:
    """ä»æ¯«ç§’æ—¶é—´æˆ³ä¸­æå–å¹´ä»½"""
    try:
        return datetime.fromtimestamp(ms/1000).strftime("%Y")
    except Exception:
        return str(datetime.now().year)

def pdf_url_from_adj(adj: str) -> str:
    return CNINFO_STATIC + (adj or "").lstrip("/")

def load_json(path, default):
    if not os.path.exists(path): return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def save_json(path, obj):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def read_tasks_from_csv(path: str):
    """
    è¯»å– CSV ä»»åŠ¡æ–‡ä»¶ï¼ˆIPOç‰ˆæœ¬ï¼šåªéœ€è¦ code å’Œ nameï¼‰
    CSVæ ¼å¼ï¼šcode,name
    """
    tasks = []
    encodings = ["utf-8-sig", "utf-8"]
    if platform.system().lower().startswith("win"):
        encodings.extend(["gbk", "gb2312", "gb18030"])

    last_error = None
    for enc in encodings:
        try:
            logging.debug(f"å°è¯•ä½¿ç”¨ç¼–ç  {enc} è¯»å– CSV...")
            with open(path, "r", encoding=enc, newline="") as f:
                for row in csv.DictReader(f):
                    try:
                        raw = (row.get("code") or "").strip()
                        code = normalize_code(raw) if raw else ""
                        name = (row.get("name") or "").strip()
                        if not code or not name:
                            continue
                        tasks.append((code, name))
                    except Exception as e:
                        logging.debug(f"è·³è¿‡æ— æ•ˆè¡Œ: {e}")
                        continue
            logging.info(f"âœ… æˆåŠŸä½¿ç”¨ç¼–ç  {enc} è¯»å– {len(tasks)} æ¡ä»»åŠ¡")
            return tasks
        except (UnicodeDecodeError, UnicodeError) as e:
            last_error = e
            logging.debug(f"ç¼–ç  {enc} å¤±è´¥: {e}")
            tasks = []
            continue
        except Exception as e:
            logging.error(f"è¯»å– CSV å¤±è´¥ ({enc}): {e}")
            raise

    # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥
    logging.error(f"âŒ æ— æ³•è¯»å– CSV æ–‡ä»¶ï¼Œå°è¯•äº†ç¼–ç : {encodings}")
    logging.error(f"æœ€åé”™è¯¯: {last_error}")
    logging.error(f"è¯·ç¡®ä¿ CSV æ–‡ä»¶ä¿å­˜ä¸º UTF-8 æˆ– GBK ç¼–ç ")
    raise ValueError(f"æ— æ³•è§£ç  CSV æ–‡ä»¶: {path}")

# ----------------------- orgId è·å– -----------------------
def build_orgid(code: str) -> str:
    """
    æ ¹æ®è‚¡ç¥¨ä»£ç æ„é€  orgId
    """
    exch_dir, _, _ = detect_exchange(code)
    code6 = normalize_code(code)
    code7 = f"0{code6}"  # 7ä½ï¼šå‰é¢è¡¥ä¸€ä¸ª 0

    if exch_dir == "SH":
        return f"gssh{code7}"
    elif exch_dir == "BJ":
        return f"gsbj{code7}"
    else:  # SZ
        return f"gssz{code7}"

def get_orgid_via_search_api(api_session: requests.Session, code: str) -> Optional[Tuple[str, str]]:
    """
    é€šè¿‡æœç´¢APIè·å–orgIdï¼ˆæ¨èæ–¹æ³•ï¼‰
    """
    code6 = normalize_code(code)
    url = "https://www.cninfo.com.cn/new/information/topSearch/query"

    try:
        r = api_session.post(url, data={"keyWord": code6}, timeout=10, proxies=PROXIES)
        if r.status_code == 200:
            js = r.json()
            if isinstance(js, list) and len(js) > 0:
                for item in js:
                    if item.get("code", "") == code6:
                        orgid = item.get("orgId")
                        company_name = item.get("zwjc", "")
                        if orgid:
                            logging.info(f"[orgId] é€šè¿‡æœç´¢APIè·å–æˆåŠŸï¼š{orgid} ({company_name})")
                            return orgid, company_name
    except Exception as e:
        logging.debug(f"æœç´¢APIæŸ¥è¯¢å¤±è´¥ (code={code6}): {e}")

    return None

def get_orgid_via_html(code: str, name: Optional[str], html_session: Optional[requests.Session] = None) -> Optional[Tuple[str, str]]:
    """
    é€šè¿‡HTMLé¡µé¢è§£æè·å–orgIdï¼ˆå…œåº•æ–¹æ³•ï¼‰
    """
    code6 = normalize_code(code)
    exch_dir, column_api, stock_suffix = detect_exchange(code6)

    if html_session is None:
        html_session = make_session(HEADERS_HTML)

    search_keywords = ["æ‹›è‚¡è¯´æ˜ä¹¦", "é¦–æ¬¡å…¬å¼€å‘è¡Œ", ""]

    for keyword in search_keywords:
        params = {
            "stock": code6,
            "searchkey": keyword,
            "category": column_api,
            "pageNum": 1,
        }
        url = f"{CNINFO_SEARCH}?{urlencode(params, safe='')}"

        logging.info(f"[HTMLå…œåº•] å°è¯•æœç´¢å…³é”®è¯'{keyword}'ï¼š{code6}")

        try:
            r = html_session.get(url, timeout=20, proxies=PROXIES)
            if r.status_code != 200:
                logging.debug(f"HTML æ£€ç´¢å¤±è´¥ï¼šHTTP {r.status_code}")
                continue

            soup = BeautifulSoup(r.text, "html.parser")

            main = soup.find("div", class_="list-main")
            if main:
                items = main.find_all("div", class_="list-item")
                logging.debug(f"æ‰¾åˆ° {len(items)} æ¡å…¬å‘Š")

                for item in items[:5]:
                    company_span = item.find("span", class_="company-name")
                    company_name = company_span.get_text(strip=True) if company_span else ""

                    a = item.select_one("span.ahover.ell a")
                    if not a or not a.get("href"):
                        continue

                    detail_url = urljoin("https://www.cninfo.com.cn", a["href"])
                    qs = parse_qs(urlparse(detail_url).query)
                    oid = (qs.get("orgId") or [""])[0]

                    stock_code = (qs.get("stockCode") or [""])[0]
                    if stock_code and normalize_code(stock_code) == code6:
                        if oid:
                            logging.info(f"[HTMLå…œåº•] æˆåŠŸè§£æ orgIdï¼š{oid} ({company_name})")
                            return oid, company_name

            scripts = soup.find_all("script")
            for script in scripts:
                script_text = script.string or ""
                matches = re.findall(r'"orgId"\s*:\s*"([^"]+)"', script_text)
                if matches:
                    oid = matches[0]
                    logging.info(f"[HTMLå…œåº•] ä»è„šæœ¬ä¸­è§£æ orgIdï¼š{oid}")
                    return oid, name or ""

        except Exception as e:
            logging.debug(f"HTML è§£æå¼‚å¸¸ (keyword={keyword}): {e}")
            continue

    logging.warning(f"[HTMLå…œåº•] æ‰€æœ‰æœç´¢å…³é”®è¯å‡å¤±è´¥ï¼š{code6}")
    return None

# ----------------------- å…¬å‘ŠæŠ“å–ä¸ä¸‹è½½ -----------------------
def fetch_ipo_announcements(api_session: requests.Session, code: str, orgId: Optional[str],
                           column_api: str, page_size=100) -> List[dict]:
    """
    æŠ“å–IPOæ‹›è‚¡è¯´æ˜ä¹¦å…¬å‘Š
    """
    _, _, stock_suffix = detect_exchange(code)
    stock_field = f"{code},{orgId}" if orgId else f"{code}.{stock_suffix}"

    seDate = build_se_window_for_ipo(lookback_years=40)  # è¦†ç›–1988å¹´è‡³ä»Šçš„æ‰€æœ‰IPO

    all_list: List[dict] = []
    page = 1

    while True:
        payload = {
            "tabName": "fulltext",
            "column": column_api,
            "stock": stock_field,
            "category": "",  # ä¸é™åˆ¶categoryï¼Œé€šè¿‡searchkeyæœç´¢
            "seDate": seDate,
            "pageNum": str(page),
            "pageSize": str(page_size),
            "searchkey": "æ‹›è‚¡è¯´æ˜ä¹¦",  # å…³é”®ï¼šä½¿ç”¨æœç´¢å…³é”®è¯
            "plate": "",
            "isHLtitle": "true",
        }

        try:
            data = api_session.post(CNINFO_API, data=payload, timeout=20, proxies=PROXIES)
            if data.status_code >= 400:
                logging.warning(f"hisAnnouncement HTTP {data.status_code} ({code} é¡µ {page})")
                break
            data = data.json() if data.text.strip().startswith("{") else {}
        except RequestException as e:
            logging.warning(f"hisAnnouncement è¯·æ±‚å¼‚å¸¸ï¼ˆ{code} é¡µ {page}ï¼‰ï¼š{e}")
            break

        anns = (data or {}).get("announcements") or []
        if not anns: break
        all_list.extend(anns)
        if len(anns) < page_size: break
        page += 1

    return all_list

def pick_latest_ipo(anns: List[dict], code: str) -> Optional[dict]:
    """
    ä»å…¬å‘Šåˆ—è¡¨ä¸­é€‰æ‹©æœ€æ–°çš„æ‹›è‚¡è¯´æ˜ä¹¦
    æ”¯æŒPDFå’ŒHTMLæ ¼å¼ï¼ˆæ—©æœŸæ–‡æ¡£å¤šä¸ºHTMLï¼‰
    """
    code_normalized = normalize_code(code)
    cands = []

    for a in anns:
        ann_code = normalize_code(str(a.get("secCode", "")))
        if ann_code != code_normalized:
            continue

        title = a.get("announcementTitle", "")
        if not title_ok_for_ipo(title):
            continue

        adj = a.get("adjunctUrl", "")
        if not adj:
            continue

        # æ¥å—PDFå’ŒHTMLæ ¼å¼
        adj_lower = adj.lower()
        if not (adj_lower.endswith(".pdf") or adj_lower.endswith(".html") or adj_lower.endswith(".htm")):
            continue

        ts = parse_time_to_ms(a.get("announcementTime"))
        cands.append((ts, a))

    if not cands:
        return None

    cands.sort(key=lambda x: x[0], reverse=True)
    return cands[0][1]

def html_to_text(html_content: str) -> str:
    """
    å°†HTMLå†…å®¹è½¬æ¢ä¸ºçº¯æ–‡æœ¬
    æå–ä¸»è¦å†…å®¹ï¼Œå»é™¤HTMLæ ‡ç­¾å’Œæ ·å¼
    ä¼˜åŒ–ä¸­æ–‡æ˜¾ç¤ºå’Œæ ¼å¼
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        for element in soup(["script", "style", "meta", "link", "noscript"]):
            element.decompose()

        # è·å–æ–‡æœ¬å†…å®¹ï¼Œä½¿ç”¨separatorä¿æŒæ®µè½åˆ†éš”
        text = soup.get_text(separator='\n')

        # æ¸…ç†æ–‡æœ¬
        lines = []
        for line in text.splitlines():
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œå’ŒåªåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„è¡Œ
            if line and not line.replace('â”ˆ', '').replace('â”€', '').replace('ã€€', '').strip() == '':
                lines.append(line)

        # åˆå¹¶è¿ç»­çš„ç›¸åŒè¡Œ
        cleaned_lines = []
        prev_line = None
        for line in lines:
            if line != prev_line:
                cleaned_lines.append(line)
                prev_line = line

        text = '\n'.join(cleaned_lines)

        # æ·»åŠ æ–‡æ¡£å¤´éƒ¨ä¿¡æ¯
        header = "=" * 60 + "\n"
        header += "IPOæ‹›è‚¡è¯´æ˜ä¹¦ - çº¯æ–‡æœ¬ç‰ˆ\n"
        header += "æœ¬æ–‡æ¡£ç”±HTMLè‡ªåŠ¨è½¬æ¢ç”Ÿæˆ\n"
        header += "=" * 60 + "\n\n"

        return header + text
    except Exception as e:
        logging.warning(f"HTMLè½¬æ–‡æœ¬å¤±è´¥: {e}")
        return html_content

def download_html_resilient(session: requests.Session, url: str, path: str,
                            referer: Optional[str] = None, max_retries=3, convert_to_text=True):
    """
    ä¸‹è½½HTMLæ ¼å¼çš„å…¬å‘Šå¹¶ä¿å­˜
    convert_to_text: æ˜¯å¦åŒæ—¶ä¿å­˜çº¯æ–‡æœ¬ç‰ˆæœ¬
    æ”¯æŒä¸­æ–‡ç¼–ç è‡ªåŠ¨æ£€æµ‹ï¼ˆGB2312/GBK/UTF-8ï¼‰
    """
    last_err = None
    for attempt in range(1, max_retries+1):
        try:
            headers = {"Referer": referer} if referer else {}
            r = session.get(url, timeout=20, proxies=PROXIES, headers=headers)

            if r.status_code in RETRY_STATUS:
                time.sleep(RETRY_BACKOFF)
                continue
            if r.status_code >= 400:
                last_err = f"HTTP {r.status_code}"
                time.sleep(RETRY_BACKOFF)
                continue

            # ä¿å­˜HTMLå†…å®¹
            ensure_dir(os.path.dirname(path))

            # æ™ºèƒ½æ£€æµ‹ä¸­æ–‡ç¼–ç 
            # ä¼˜å…ˆä½¿ç”¨HTML metaæ ‡ç­¾æŒ‡å®šçš„ç¼–ç 
            detected_encoding = r.encoding
            if r.apparent_encoding:
                detected_encoding = r.apparent_encoding

            # å¯¹äºä¸­æ–‡å†…å®¹ï¼Œä¼˜å…ˆå°è¯•å¸¸è§ç¼–ç 
            if detected_encoding and detected_encoding.lower() in ['gb2312', 'gbk', 'gb18030']:
                try:
                    html_text = r.content.decode(detected_encoding)
                except:
                    html_text = r.content.decode('utf-8', errors='ignore')
            else:
                html_text = r.text

            # ä¿®å¤HTMLç¼–ç å£°æ˜ï¼Œç¡®ä¿ä¸å®é™…ç¼–ç ä¸€è‡´
            import re

            # å°†æ‰€æœ‰GB2312/GBKç¼–ç å£°æ˜æ›¿æ¢ä¸ºUTF-8
            # åŒ¹é… <meta http-equiv="Content-Type" content="text/html; charset=gb2312">
            html_text = re.sub(
                r'<meta\s+http-equiv=["\']?Content-Type["\']?\s+content=["\']?text/html;\s*charset=gb2312["\']?\s*/?>',
                '<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">',
                html_text,
                flags=re.IGNORECASE
            )

            # åŒ¹é… <meta charset="gb2312">
            html_text = re.sub(
                r'<meta\s+charset=["\']?(gb2312|gbk|gb18030)["\']?\s*/?>',
                '<meta charset="UTF-8">',
                html_text,
                flags=re.IGNORECASE
            )

            # å¦‚æœè¿˜æ²¡æœ‰ç¼–ç å£°æ˜ï¼Œæ·»åŠ ä¸€ä¸ª
            if not re.search(r'<meta\s+(charset=|http-equiv=["\']?Content-Type)', html_text, re.IGNORECASE):
                if '<head>' in html_text:
                    html_text = html_text.replace('<head>', '<head>\n<meta charset="UTF-8">')
                elif '<HEAD>' in html_text:
                    html_text = html_text.replace('<HEAD>', '<HEAD>\n<meta charset="UTF-8">')

            # ä¿å­˜HTMLæ–‡ä»¶ï¼ˆç»Ÿä¸€ä½¿ç”¨UTF-8ç¼–ç ï¼‰
            with open(path, "w", encoding="utf-8") as f:
                f.write(html_text)

            # åŒæ—¶ä¿å­˜çº¯æ–‡æœ¬ç‰ˆæœ¬
            if convert_to_text:
                text_content = html_to_text(html_text)
                text_path = path.replace('.html', '.txt').replace('.htm', '.txt')
                with open(text_path, "w", encoding="utf-8") as f:
                    f.write(text_content)
                logging.info(f"å·²è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼: {os.path.basename(text_path)}")

            return True, "ok"
        except RequestException as e:
            last_err = str(e)
            time.sleep(RETRY_BACKOFF)
        except Exception as e:
            last_err = str(e)
            time.sleep(RETRY_BACKOFF)

    return False, last_err or "HTTP 4xx/5xx"

def download_pdf_resilient(session: requests.Session, url: str, path: str,
                           referer: Optional[str] = None,
                           refresh_fn=None, max_retries=3):
    cur_url, cur_ref = url, referer
    last_err = None
    for attempt in range(1, max_retries+1):
        try:
            headers = {"Referer": cur_ref} if cur_ref else {}
            r = session.get(cur_url, timeout=20, stream=True, proxies=PROXIES, headers=headers)
            if r.status_code == 404 and refresh_fn:
                new_url, new_ref = refresh_fn()
                if new_url and new_url != cur_url:
                    logging.warning(f"404 åˆ·æ–°é“¾æ¥ï¼š\nold={cur_url}\nnew={new_url}")
                    cur_url, cur_ref = new_url, (new_ref or cur_ref)
                    continue
            if r.status_code in RETRY_STATUS:
                time.sleep(RETRY_BACKOFF); continue
            if r.status_code >= 400:
                last_err = f"HTTP {r.status_code}"
                time.sleep(RETRY_BACKOFF); continue

            head = r.raw.read(5); r.raw.decode_content = True
            if not head.startswith(b"%PDF-"):
                return False, "éPDFå†…å®¹"
            ensure_dir(os.path.dirname(path))
            with open(path, "wb") as f:
                f.write(head or b"")
                for chunk in r.iter_content(8192):
                    if chunk: f.write(chunk)
            return True, "ok"
        except RequestException as e:
            last_err = str(e)
            time.sleep(RETRY_BACKOFF)
    return False, last_err or "HTTP 4xx/5xx"

# ----------------------- å¤šè¿›ç¨‹æ”¯æŒ -----------------------
class SharedState:
    """çº¿ç¨‹å®‰å…¨çš„å…±äº«çŠ¶æ€ç®¡ç†å™¨"""
    def __init__(self, checkpoint_file: str, orgid_cache_file: str):
        self.checkpoint_file = checkpoint_file
        self.orgid_cache_file = orgid_cache_file
        self.lock = multiprocessing.Lock()

    def load_checkpoint(self) -> Dict[str, bool]:
        with self.lock:
            return load_json(self.checkpoint_file, {})

    def save_checkpoint(self, key: str):
        with self.lock:
            data = load_json(self.checkpoint_file, {})
            data[key] = True
            save_json(self.checkpoint_file, data)

    def load_orgid_cache(self) -> Dict[str, str]:
        with self.lock:
            return load_json(self.orgid_cache_file, {})

    def save_orgid(self, code: str, orgid: str):
        with self.lock:
            data = load_json(self.orgid_cache_file, {})
            data[code] = orgid
            save_json(self.orgid_cache_file, data)

    def get_orgid(self, code: str) -> Optional[str]:
        with self.lock:
            data = load_json(self.orgid_cache_file, {})
            return data.get(code)

def process_single_task(task_data: Tuple) -> Tuple[bool, Optional[Tuple]]:
    """
    å¤„ç†å•ä¸ªä¸‹è½½ä»»åŠ¡ï¼ˆåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­è¿è¡Œï¼‰
    """
    code, name, out_root, checkpoint_file, orgid_cache_file = task_data

    # æ¯ä¸ªè¿›ç¨‹åˆ›å»ºè‡ªå·±çš„ session
    api_session = make_session(HEADERS_API)
    html_session = make_session(HEADERS_HTML)

    # åˆ›å»ºå…±äº«çŠ¶æ€ç®¡ç†å™¨
    shared = SharedState(checkpoint_file, orgid_cache_file)

    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    key = f"{code}-IPO"
    checkpoint = shared.load_checkpoint()
    if checkpoint.get(key):
        return True, None

    exch_dir, column_api, _ = detect_exchange(code)
    real_company_name = name

    try:
        # orgId è·å–ç­–ç•¥
        orgId = shared.get_orgid(code)
        if not orgId:
            orgId = build_orgid(code)
            shared.save_orgid(code, orgId)
            logging.info(f"[{code}] orgId æ„é€ æ–¹æ³•ï¼š{orgId}")

        # æŠ“å–IPOå…¬å‘Š
        anns = fetch_ipo_announcements(api_session, code, orgId, column_api)

        if not anns:
            # å°è¯•é€šè¿‡æœç´¢APIè·å–çœŸå® orgId
            logging.warning(f"[{code}] æ„é€ çš„ orgId å¯èƒ½æ— æ•ˆï¼Œå°è¯•æœç´¢APIæ–¹æ³•...")
            result = get_orgid_via_search_api(api_session, code)
            if result:
                real_orgid, company_name = result
                real_company_name = company_name
                if real_orgid != orgId:
                    logging.info(f"[{code}] orgId æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid}")
                    orgId = real_orgid
                    shared.save_orgid(code, orgId)
                    anns = fetch_ipo_announcements(api_session, code, orgId, column_api)

            # HTMLå…œåº•
            if not anns:
                logging.warning(f"[{code}] æœç´¢APIå¤±è´¥ï¼Œä½¿ç”¨ HTML å…œåº•æ–¹æ³•...")
                html_result = get_orgid_via_html(code, name, html_session)
                if html_result:
                    real_orgid, company_name = html_result
                    if real_orgid != orgId:
                        logging.info(f"[{code}] [HTMLå…œåº•] orgId æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid} ({company_name})")
                        orgId = real_orgid
                        shared.save_orgid(code, orgId)
                        anns = fetch_ipo_announcements(api_session, code, orgId, column_api)

        if not anns:
            logging.error(f"[{code}] æœªè·å¾—å…¬å‘Šåˆ—è¡¨ï¼š{real_company_name}ï¼ˆ{code}ï¼‰IPO")
            return False, (code, real_company_name, "no announcements")

        # å–æœ€æ–°çš„æ‹›è‚¡è¯´æ˜ä¹¦
        best = pick_latest_ipo(anns, code)
        if not best:
            logging.error(f"[{code}] å…¬å‘Šè¿‡æ»¤åä¸ºç©ºï¼š{real_company_name}ï¼ˆ{code}ï¼‰IPO")
            return False, (code, real_company_name, "not found after filter")

        # ä¸‹è½½
        adj = best.get("adjunctUrl", "")
        doc_url = pdf_url_from_adj(adj)
        ts = parse_time_to_ms(best.get("announcementTime"))
        pub_date = ms_to_ddmmyyyy(ts)
        pub_year = ms_to_year(ts)

        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        adj_lower = adj.lower()
        is_html = adj_lower.endswith(".html") or adj_lower.endswith(".htm")
        file_ext = ".html" if is_html else ".pdf"

        out_dir = os.path.join(out_root, exch_dir, code)
        ensure_dir(out_dir)
        fname = f"{code}_{pub_year}_{pub_date}{file_ext}"
        out_path = os.path.join(out_dir, fname)

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸‹è½½å‡½æ•°
        if is_html:
            logging.info(f"[{code}] ä¸‹è½½HTMLæ ¼å¼æ–‡æ¡£ï¼š{doc_url}")
            ok, msg = download_html_resilient(html_session, doc_url, out_path, referer=None, max_retries=3)
        else:
            # å®šä¹‰åˆ·æ–°å‡½æ•°
            def refresh_fn():
                anns2 = fetch_ipo_announcements(api_session, code, orgId, column_api)
                b2 = pick_latest_ipo(anns2, code)
                if not b2: return None, None
                return pdf_url_from_adj(b2.get("adjunctUrl", "")), None

            logging.info(f"[{code}] ä¸‹è½½PDFæ ¼å¼æ–‡æ¡£ï¼š{doc_url}")
            ok, msg = download_pdf_resilient(html_session, doc_url, out_path, referer=None,
                                            refresh_fn=refresh_fn, max_retries=3)

        if ok:
            logging.info(f"[{code}] ä¿å­˜æˆåŠŸï¼š{out_path}")
            shared.save_checkpoint(key)
            return True, None
        else:
            logging.error(f"[{code}] ä¸‹è½½å¤±è´¥ï¼š{real_company_name}ï¼ˆ{code}ï¼‰IPO - {msg}")
            return False, (code, real_company_name, f"download failed: {msg}")

    except Exception as e:
        logging.error(f"[{code}] å¤„ç†å¼‚å¸¸ï¼š{real_company_name}ï¼ˆ{code}ï¼‰IPO - {e}")
        return False, (code, real_company_name, f"exception: {str(e)}")
    finally:
        time.sleep(random.uniform(0.5, 1.5))

def run_multiprocessing(input_csv: str, out_root: str, fail_csv: str,
                       workers: int = 4, debug=False):
    """
    å¤šè¿›ç¨‹å¹¶è¡Œä¸‹è½½æ¨¡å¼
    """
    if debug:
        logger.setLevel(logging.DEBUG)
        logging.info("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    tasks = read_tasks_from_csv(input_csv)
    total = len(tasks)
    print(f"å…±è¯»å–ä»»åŠ¡ï¼š{total} æ¡")
    print(f"ä½¿ç”¨ {workers} ä¸ªå¹¶è¡Œè¿›ç¨‹å¤„ç†")

    # å‡†å¤‡ä»»åŠ¡æ•°æ®
    task_data_list = [
        (code, name, out_root, CHECKPOINT_FILE, ORGID_CACHE_FILE)
        for code, name in tasks
    ]

    failures = []
    completed = 0

    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=workers) as executor:
        future_to_task = {
            executor.submit(process_single_task, task_data): task_data
            for task_data in task_data_list
        }

        with tqdm(total=total, desc="æŠ“å–è¿›åº¦", unit="ä»»åŠ¡") as pbar:
            for future in as_completed(future_to_task):
                task_data = future_to_task[future]
                code, name = task_data[:2]

                try:
                    success, failure_record = future.result()
                    if success:
                        completed += 1
                    elif failure_record:
                        failures.append(failure_record)
                except Exception as e:
                    logging.error(f"ä»»åŠ¡å¤„ç†å¼‚å¸¸ï¼š{name}ï¼ˆ{code}ï¼‰IPO - {e}")
                    failures.append((code, name, f"exception: {str(e)}"))

                pbar.update(1)

    # å†™å…¥å¤±è´¥è®°å½•
    if failures:
        with open(fail_csv, "w", encoding=CSV_ENCODING, newline="", errors="replace") as f:
            w = csv.writer(f)
            w.writerow(["code", "name", "reason"])
            w.writerows(failures)
        print(f"âŒ å†™å…¥å¤±è´¥è®°å½•ï¼š{fail_csv}ï¼ˆç¼–ç ï¼š{CSV_ENCODING}ï¼‰")
        print(f"âœ… æˆåŠŸï¼š{completed}/{total} ({completed*100//total}%)")
    else:
        print("âœ… å…¨éƒ¨æˆåŠŸï¼Œæ— å¤±è´¥è®°å½•ã€‚")

# ----------------------- ä¸»æµç¨‹ -----------------------
def run(input_csv: str, out_root: str, fail_csv: str, watch_log=False, debug=False):
    if debug:
        logger.setLevel(logging.DEBUG)
        logging.info("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")

    if watch_log:
        start_tail()

    api_session = make_session(HEADERS_API)
    html_session = make_session(HEADERS_HTML)

    checkpoint: Dict[str, bool] = load_json(CHECKPOINT_FILE, {})
    orgid_cache: Dict[str, str] = load_json(ORGID_CACHE_FILE, {})

    tasks = read_tasks_from_csv(input_csv)
    total = len(tasks)
    print(f"å…±è¯»å–ä»»åŠ¡ï¼š{total} æ¡")

    failures = []
    last_code = None

    for code, name in tqdm(tasks, desc="æŠ“å–è¿›åº¦", unit="ä»»åŠ¡"):
        key = f"{code}-IPO"
        if checkpoint.get(key):
            continue

        if last_code == code:
            time.sleep(INTER_SAME_STOCK_GAP)
        last_code = code

        exch_dir, column_api, stock_suffix = detect_exchange(code)
        logging.info(f"æ­£åœ¨æŠ“å–ï¼š{name}ï¼ˆ{code}ï¼‰ IPOæ‹›è‚¡è¯´æ˜ä¹¦ [{column_api}]")

        real_company_name = name

        # orgId è·å–ç­–ç•¥
        orgId = orgid_cache.get(code)
        if not orgId:
            orgId = build_orgid(code)
            orgid_cache[code] = orgId
            save_json(ORGID_CACHE_FILE, orgid_cache)
            logging.info(f"[orgId] æ„é€ æ–¹æ³•ï¼š{orgId}")

        # æŠ“å–IPOå…¬å‘Š
        anns = fetch_ipo_announcements(api_session, code, orgId, column_api)

        if not anns:
            # æœç´¢APIæ–¹æ³•
            logging.warning(f"æ„é€ çš„ orgId å¯èƒ½æ— æ•ˆï¼Œå°è¯•æœç´¢APIæ–¹æ³•...")
            result = get_orgid_via_search_api(api_session, code)
            if result:
                real_orgid, api_company_name = result
                real_company_name = api_company_name
                if real_orgid != orgId:
                    logging.info(f"[orgId] æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid}")
                    orgId = real_orgid
                    orgid_cache[code] = orgId
                    save_json(ORGID_CACHE_FILE, orgid_cache)
                    anns = fetch_ipo_announcements(api_session, code, orgId, column_api)

            # HTMLå…œåº•
            if not anns:
                logging.warning(f"æœç´¢APIå¤±è´¥ï¼Œä½¿ç”¨ HTML å…œåº•æ–¹æ³•...")
                html_result = get_orgid_via_html(code, name, html_session)
                if html_result:
                    real_orgid, company_name = html_result
                    if real_orgid != orgId:
                        logging.info(f"[HTMLå…œåº•] orgId æ›´æ–°ä¸ºçœŸå®å€¼ï¼š{real_orgid} ({company_name})")
                        orgId = real_orgid
                        orgid_cache[code] = orgId
                        save_json(ORGID_CACHE_FILE, orgid_cache)
                        anns = fetch_ipo_announcements(api_session, code, orgId, column_api)

        if not anns:
            logging.error(f"æœªè·å¾—å…¬å‘Šåˆ—è¡¨ï¼š{real_company_name}ï¼ˆ{code}ï¼‰IPO")
            failures.append((code, real_company_name, "no announcements"))
            time.sleep(random.uniform(*INTER_COMBO_SLEEP_RANGE))
            continue

        # å–æœ€æ–°çš„æ‹›è‚¡è¯´æ˜ä¹¦
        best = pick_latest_ipo(anns, code)
        if not best:
            logging.error(f"å…¬å‘Šè¿‡æ»¤åä¸ºç©ºï¼š{real_company_name}ï¼ˆ{code}ï¼‰IPO")
            failures.append((code, real_company_name, "not found after filter"))
            time.sleep(random.uniform(*INTER_COMBO_SLEEP_RANGE))
            continue

        # ä¸‹è½½
        adj = best.get("adjunctUrl", "")
        doc_url = pdf_url_from_adj(adj)
        ts = parse_time_to_ms(best.get("announcementTime"))
        pub_date = ms_to_ddmmyyyy(ts)
        pub_year = ms_to_year(ts)

        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        adj_lower = adj.lower()
        is_html = adj_lower.endswith(".html") or adj_lower.endswith(".htm")
        file_ext = ".html" if is_html else ".pdf"

        out_dir = os.path.join(out_root, exch_dir, code)
        ensure_dir(out_dir)
        fname = f"{code}_{pub_year}_{pub_date}{file_ext}"
        out_path = os.path.join(out_dir, fname)

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸‹è½½å‡½æ•°
        if is_html:
            logging.info(f"ä¸‹è½½HTMLæ ¼å¼æ–‡æ¡£ï¼š{doc_url}")
            ok, msg = download_html_resilient(html_session, doc_url, out_path, referer=None, max_retries=3)
        else:
            # å®šä¹‰åˆ·æ–°å‡½æ•°
            def refresh_fn():
                anns2 = fetch_ipo_announcements(api_session, code, orgId, column_api)
                b2 = pick_latest_ipo(anns2, code)
                if not b2: return None, None
                return pdf_url_from_adj(b2.get("adjunctUrl", "")), None

            logging.debug(f"Downloading: url={doc_url} -> {out_path}")
            ok, msg = download_pdf_resilient(html_session, doc_url, out_path, referer=None, refresh_fn=refresh_fn, max_retries=3)

        if ok:
            logging.info(f"ä¿å­˜æˆåŠŸï¼š{out_path}")
            checkpoint[key] = True
            save_json(CHECKPOINT_FILE, checkpoint)
        else:
            logging.error(f"ä¸‹è½½å¤±è´¥ï¼š{real_company_name}ï¼ˆ{code}ï¼‰IPO - {msg}")
            failures.append((code, real_company_name, f"download failed: {msg}"))

        time.sleep(random.uniform(*INTER_COMBO_SLEEP_RANGE))

    # å¤±è´¥è®°å½•
    if failures:
        with open(fail_csv, "w", encoding=CSV_ENCODING, newline="", errors="replace") as f:
            w = csv.writer(f)
            w.writerow(["code", "name", "reason"])
            w.writerows(failures)
        print(f"âŒ å†™å…¥å¤±è´¥è®°å½•ï¼š{fail_csv}ï¼ˆç¼–ç ï¼š{CSV_ENCODING}ï¼‰")
    else:
        print("âœ… å…¨éƒ¨æˆåŠŸï¼Œæ— å¤±è´¥è®°å½•ã€‚")

# ----------------------- CLI -----------------------
if __name__ == "__main__":
    p = argparse.ArgumentParser(description="CNINFO IPOæ‹›è‚¡è¯´æ˜ä¹¦æŠ“å–å·¥å…·")
    p.add_argument("--input", required=True, help="è¾“å…¥ CSVï¼ˆcode,nameï¼‰")
    p.add_argument("--out", required=True, help="è¾“å‡ºæ ¹ç›®å½•")
    p.add_argument("--fail", required=True, help="å¤±è´¥è®°å½• CSV")
    p.add_argument("--workers", type=int, default=0, help="å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ0=é¡ºåºæ¨¡å¼ï¼Œæ¨è 4-8ï¼‰")
    p.add_argument("--watch-log", action="store_true", help="å®æ—¶æ»šåŠ¨æ˜¾ç¤º error.logï¼ˆä»…é¡ºåºæ¨¡å¼ï¼‰")
    p.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼ï¼ˆè¾“å‡ºæ›´å¤šæ—¥å¿—ï¼‰")
    args = p.parse_args()

    # å¤šè¿›ç¨‹æ¨¡å¼ or é¡ºåºæ¨¡å¼
    if args.workers > 0:
        if args.watch_log:
            print("âš ï¸  å¤šè¿›ç¨‹æ¨¡å¼ä¸‹ä¸æ”¯æŒ --watch-logï¼Œå·²å¿½ç•¥")
        run_multiprocessing(args.input, args.out, args.fail, workers=args.workers, debug=args.debug)
    else:
        run(args.input, args.out, args.fail, watch_log=args.watch_log, debug=args.debug)
