# -*- coding: utf-8 -*-
"""
å‘é‡åŒ–æœåŠ¡
å°è£…å®Œæ•´çš„å‘é‡åŒ–æµç¨‹ï¼šè¯»å–åˆ†å— -> ç”Ÿæˆå‘é‡ -> å­˜å‚¨åˆ°Milvus -> æ›´æ–°æ•°æ®åº“
"""

from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import uuid

from src.processing.ai.embedding.embedder_factory import get_embedder_by_mode
from src.storage.vector.milvus_client import get_milvus_client
from src.storage.metadata.postgres_client import get_postgres_client
from src.storage.metadata import crud
from src.storage.metadata.models import DocumentChunk, Document
from src.common.constants import MilvusCollection
from src.common.logger import LoggerMixin
from src.common.config import embedding_config
from src.common.utils import extract_text_from_table


class Vectorizer(LoggerMixin):
    """
    å‘é‡åŒ–æœåŠ¡
    è´Ÿè´£å°†æ–‡æ¡£åˆ†å—å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°Milvus
    """

    def __init__(
        self,
        embedder=None,
        milvus_client=None,
        pg_client=None
    ):
        """
        åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡

        Args:
            embedder: Embedderå®ä¾‹ï¼ˆé»˜è®¤åˆ›å»ºæ–°å®ä¾‹ï¼‰
            milvus_client: Milvuså®¢æˆ·ç«¯ï¼ˆé»˜è®¤åˆ›å»ºæ–°å®ä¾‹ï¼‰
            pg_client: PostgreSQLå®¢æˆ·ç«¯ï¼ˆé»˜è®¤åˆ›å»ºæ–°å®ä¾‹ï¼‰
        """
        # å¦‚æœæ²¡æœ‰æä¾› embedderï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©
        if embedder is None:
            # ä½¿ç”¨å·¥å‚æ¨¡å¼ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©æœ¬åœ°æ¨¡å‹æˆ– API
            self.embedder = get_embedder_by_mode()
        else:
            self.embedder = embedder
        self.milvus_client = milvus_client or get_milvus_client()
        self.pg_client = pg_client or get_postgres_client()
        
        # ç¡®ä¿Milvus Collectionå­˜åœ¨
        self._ensure_collection_exists()

    def _ensure_collection_exists(self):
        """ç¡®ä¿Milvus Collectionå­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        collection_name = MilvusCollection.DOCUMENTS
        dimension = self.embedder.get_model_dim()
        
        try:
            collection = self.milvus_client.get_collection(collection_name)
            if collection is None:
                self.logger.info(f"Collectionä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°Collection: {collection_name}, dim={dimension}")
                self.milvus_client.create_collection(
                    collection_name=collection_name,
                    dimension=dimension,
                    description="é‡‘èæ–‡æ¡£å‘é‡é›†åˆ",
                    index_type="IVF_FLAT",
                    metric_type="L2"
                )
            else:
                self.logger.debug(f"Collectionå·²å­˜åœ¨: {collection_name}")
        except Exception as e:
            self.logger.error(f"ç¡®ä¿Collectionå­˜åœ¨å¤±è´¥: {e}", exc_info=True)
            raise

    def vectorize_chunks(
        self,
        chunk_ids: List[Union[uuid.UUID, str]],
        force_revectorize: bool = False,
        progress_callback: Optional[callable] = None
    ) -> Dict[str, Any]:
        """
        å‘é‡åŒ–æŒ‡å®šçš„åˆ†å—

        Args:
            chunk_ids: åˆ†å—IDåˆ—è¡¨
            force_revectorize: æ˜¯å¦å¼ºåˆ¶é‡æ–°å‘é‡åŒ–ï¼ˆåˆ é™¤æ—§å‘é‡ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¯ä¸ªæ‰¹æ¬¡å®Œæˆåè°ƒç”¨
                               å›è°ƒå‚æ•°: dict with keys: batch_num, total_batches, batch_size,
                                        processed, total, vectorized_count, failed_count, batch_chunks

        Returns:
            å‘é‡åŒ–ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - success: æ˜¯å¦æˆåŠŸ
            - vectorized_count: æˆåŠŸå‘é‡åŒ–çš„æ•°é‡
            - failed_count: å¤±è´¥æ•°é‡
            - failed_chunks: å¤±è´¥çš„åˆ†å—IDåˆ—è¡¨
        """
        if not chunk_ids:
            return {
                "success": True,
                "vectorized_count": 0,
                "failed_count": 0,
                "failed_chunks": []
            }

        self.logger.info(f"å¼€å§‹å‘é‡åŒ– {len(chunk_ids)} ä¸ªåˆ†å—...")

        # è½¬æ¢UUIDæ ¼å¼
        chunk_ids_uuid = []
        for chunk_id in chunk_ids:
            if isinstance(chunk_id, str):
                chunk_ids_uuid.append(uuid.UUID(chunk_id))
            else:
                chunk_ids_uuid.append(chunk_id)

        vectorized_count = 0
        failed_count = 0
        failed_chunks = []

        # ä»æ•°æ®åº“è¯»å–åˆ†å—ä¿¡æ¯
        # æ³¨æ„ï¼šåœ¨ session å…³é—­å‰æå–æ‰€æœ‰éœ€è¦çš„æ•°æ®ï¼Œé¿å… DetachedInstanceError
        chunks_data = []
        with self.pg_client.get_session() as session:
            for chunk_id in chunk_ids_uuid:
                chunk = session.query(DocumentChunk).filter(
                    DocumentChunk.id == chunk_id
                ).first()

                if not chunk:
                    self.logger.warning(f"åˆ†å—ä¸å­˜åœ¨: {chunk_id}")
                    failed_count += 1
                    failed_chunks.append(str(chunk_id))
                    continue

                # æ£€æŸ¥æ˜¯å¦å·²å‘é‡åŒ–ï¼ˆä½¿ç”¨ vectorized_at åˆ¤æ–­ï¼‰
                if not force_revectorize and chunk.vectorized_at:
                    self.logger.debug(f"åˆ†å—å·²å‘é‡åŒ–ï¼Œè·³è¿‡: {chunk_id}, vectorized_at={chunk.vectorized_at}")
                    continue

                # è·å–å…³è”çš„Documentä¿¡æ¯
                doc = session.query(Document).filter(
                    Document.id == chunk.document_id
                ).first()

                if not doc:
                    self.logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {chunk.document_id}")
                    failed_count += 1
                    failed_chunks.append(str(chunk_id))
                    continue

                # å¤„ç†è¡¨æ ¼chunkï¼šæå–æ–‡æœ¬å†…å®¹
                chunk_text = chunk.chunk_text
                if chunk.is_table:
                    # ä»è¡¨æ ¼HTMLä¸­æå–æ–‡æœ¬å†…å®¹
                    extracted_text = extract_text_from_table(chunk.chunk_text)
                    
                    # æ£€æŸ¥æå–çš„æ–‡æœ¬æ˜¯å¦æœ‰å®é™…å†…å®¹ï¼ˆè‡³å°‘10ä¸ªå­—ç¬¦ï¼‰
                    if not extracted_text or len(extracted_text.strip()) < 10:
                        self.logger.debug(
                            f"åˆ†å—åŒ…å«è¡¨æ ¼ä½†æ— æœ‰æ•ˆæ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡å‘é‡åŒ–: "
                            f"chunk_id={chunk_id}, extracted_length={len(extracted_text) if extracted_text else 0}"
                        )
                        continue
                    
                    # ä½¿ç”¨æå–çš„æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
                    chunk_text = extracted_text
                    self.logger.debug(
                        f"åˆ†å—åŒ…å«è¡¨æ ¼ï¼Œå·²æå–æ–‡æœ¬å†…å®¹: chunk_id={chunk_id}, "
                        f"original_length={len(chunk.chunk_text)}, extracted_length={len(chunk_text)}"
                    )

                # åœ¨ session å…³é—­å‰æå–æ‰€æœ‰éœ€è¦çš„æ•°æ®
                chunks_data.append({
                    "chunk_id": str(chunk.id),
                    "document_id": str(chunk.document_id),
                    "chunk_text": chunk_text,  # ä½¿ç”¨å¤„ç†åçš„æ–‡æœ¬ï¼ˆè¡¨æ ¼chunkä½¿ç”¨æå–çš„æ–‡æœ¬ï¼‰
                    "stock_code": doc.stock_code,
                    "company_name": doc.company_name,
                    "doc_type": doc.doc_type,
                    "year": doc.year,
                    "quarter": doc.quarter or 0,
                    # ä¿å­˜å¯¹è±¡å¼•ç”¨ï¼ˆç”¨äºåç»­æ›´æ–°ï¼Œä½†éœ€è¦é‡æ–°æŸ¥è¯¢ï¼‰
                    "_chunk_id_uuid": chunk.id,
                    "_document_id_uuid": chunk.document_id,
                })

        if not chunks_data:
            self.logger.warning("æ²¡æœ‰éœ€è¦å‘é‡åŒ–çš„åˆ†å—")
            return {
                "success": True,
                "vectorized_count": 0,
                "failed_count": failed_count,
                "failed_chunks": failed_chunks
            }

        # æ‰¹é‡å¤„ç†
        batch_size = embedding_config.EMBEDDING_BATCH_SIZE
        total_batches = (len(chunks_data) + batch_size - 1) // batch_size
        for i in range(0, len(chunks_data), batch_size):
            batch = chunks_data[i:i + batch_size]
            batch_num = i // batch_size + 1
            processed = min(i + batch_size, len(chunks_data))
            progress_pct = processed / len(chunks_data) * 100

            self.logger.info(
                f"ğŸ“¦ æ‰¹æ¬¡ [{batch_num}/{total_batches}] | "
                f"æœ¬æ‰¹ {len(batch)} é¡¹ | "
                f"æ€»è¿›åº¦ {processed}/{len(chunks_data)} ({progress_pct:.1f}%)"
            )

            try:
                result = self._vectorize_batch(batch, force_revectorize)
                vectorized_count += result["vectorized_count"]
                failed_count += result["failed_count"]
                failed_chunks.extend(result["failed_chunks"])

                # æ˜¾ç¤ºæ‰¹æ¬¡ç»“æœ
                self.logger.info(f"   âœ“ æˆåŠŸ {result['vectorized_count']} å¤±è´¥ {result['failed_count']}")

                # è°ƒç”¨è¿›åº¦å›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
                if progress_callback:
                    try:
                        progress_callback({
                            "batch_num": batch_num,
                            "total_batches": total_batches,
                            "batch_size": len(batch),
                            "processed": processed,
                            "total": len(chunks_data),
                            "vectorized_count": result["vectorized_count"],
                            "failed_count": result["failed_count"],
                            "batch_chunks": batch,  # å½“å‰æ‰¹æ¬¡çš„åˆ†å—æ•°æ®
                        })
                    except Exception as callback_error:
                        # å›è°ƒå¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
                        self.logger.warning(f"è¿›åº¦å›è°ƒæ‰§è¡Œå¤±è´¥: {callback_error}", exc_info=True)
            except Exception as e:
                self.logger.error(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)
                # æ ‡è®°æ•´æ‰¹ä¸ºå¤±è´¥ï¼Œå¹¶æ›´æ–°æ•°æ®åº“çŠ¶æ€
                error_message = f"æ‰¹é‡å‘é‡åŒ–å¼‚å¸¸: {str(e)[:200]}"
                with self.pg_client.get_session() as session:
                    for item in batch:
                        failed_count += 1
                        failed_chunks.append(item["chunk_id"])
                        # æ›´æ–°å¤±è´¥çŠ¶æ€
                        crud.update_chunk_status(
                            session=session,
                            chunk_id=item["_chunk_id_uuid"],
                            status='failed',
                            error_message=error_message,
                            increment_retry=True
                        )
                    session.commit()

        self.logger.info(
            f"å‘é‡åŒ–å®Œæˆ: æˆåŠŸ={vectorized_count}, å¤±è´¥={failed_count}"
        )

        return {
            "success": True,
            "vectorized_count": vectorized_count,
            "failed_count": failed_count,
            "failed_chunks": failed_chunks
        }

    def _vectorize_batch(
        self,
        chunks_data: List[Dict],
        force_revectorize: bool = False
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å‘é‡åŒ–ä¸€æ‰¹åˆ†å—

        Args:
            chunks_data: åˆ†å—æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«chunkå’Œdocument
            force_revectorize: æ˜¯å¦å¼ºåˆ¶é‡æ–°å‘é‡åŒ–

        Returns:
            æ‰¹é‡å‘é‡åŒ–ç»“æœ
        """
        if not chunks_data:
            return {
                "vectorized_count": 0,
                "failed_count": 0,
                "failed_chunks": []
            }

        # æå–æ–‡æœ¬ï¼ˆæ•°æ®å·²ç»åœ¨ session å…³é—­å‰æå–ï¼‰
        texts = []
        chunk_ids_list = []
        for item in chunks_data:
            texts.append(item["chunk_text"])
            chunk_ids_list.append(item["chunk_id"])

        # æ‰¹é‡ç”Ÿæˆå‘é‡
        failed_embedding_indices = set()  # åœ¨tryå¤–éƒ¨å®šä¹‰ï¼Œç¡®ä¿åç»­å¯ä»¥ä½¿ç”¨
        
        try:
            embeddings = self.embedder.embed_batch(texts)
            
            # âœ… éªŒè¯ embeddings æ ¼å¼
            self.logger.debug(
                f"embed_batch è¿”å›:\n"
                f"  embeddings ç±»å‹: {type(embeddings)}\n"
                f"  embeddings æ•°é‡: {len(embeddings) if embeddings else 0}\n"
                f"  ç¬¬ä¸€ä¸ª embedding ç±»å‹: {type(embeddings[0]) if embeddings and len(embeddings) > 0 else 'N/A'}\n"
                f"  ç¬¬ä¸€ä¸ª embedding é•¿åº¦: {len(embeddings[0]) if embeddings and len(embeddings) > 0 and hasattr(embeddings[0], '__len__') else 'N/A'}"
            )
            
            # ç¡®ä¿ embeddings æ˜¯äºŒç»´åˆ—è¡¨ List[List[float]]
            if embeddings and len(embeddings) > 0:
                first_emb = embeddings[0]
                if isinstance(first_emb, (int, float)):
                    # embeddings è¢«å±•å¹³äº†ï¼Œéœ€è¦é‡æ–°æ•´å½¢
                    self.logger.error(
                        f"âŒ embeddings æ ¼å¼é”™è¯¯: è¢«å±•å¹³ä¸ºä¸€ç»´åˆ—è¡¨ã€‚"
                        f"ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ {type(first_emb)}ï¼Œåº”è¯¥æ˜¯ listã€‚"
                    )
                    # å°è¯•ä¿®å¤ï¼šå¦‚æœçŸ¥é“ç»´åº¦ï¼Œå¯ä»¥é‡æ–°æ•´å½¢
                    expected_dim = self.embedder.get_model_dim()
                    if len(embeddings) % expected_dim == 0:
                        num_vectors = len(embeddings) // expected_dim
                        self.logger.warning(
                            f"å°è¯•ä¿®å¤: å°† {len(embeddings)} ä¸ªå…ƒç´ é‡æ–°æ•´å½¢ä¸º {num_vectors} ä¸ª {expected_dim} ç»´å‘é‡"
                        )
                        reshaped = []
                        for i in range(num_vectors):
                            reshaped.append(embeddings[i * expected_dim:(i + 1) * expected_dim])
                        embeddings = reshaped
                    else:
                        raise ValueError(
                            f"æ— æ³•ä¿®å¤å±•å¹³çš„ embeddings: é•¿åº¦ {len(embeddings)} ä¸èƒ½è¢«ç»´åº¦ {expected_dim} æ•´é™¤"
                        )
            
            # âœ… æ£€æŸ¥æ˜¯å¦æœ‰é›¶å‘é‡ï¼ˆè¡¨ç¤ºéƒ¨åˆ†å¤±è´¥ï¼‰
            if embeddings and len(embeddings) > 0:
                dimension = len(embeddings[0])
                zero_vector = [0.0] * dimension
                
                for i, emb in enumerate(embeddings):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é›¶å‘é‡ï¼ˆæ‰€æœ‰å…ƒç´ éƒ½æ˜¯0æˆ–æ¥è¿‘0ï¼‰
                    if isinstance(emb, list) and len(emb) == dimension:
                        if all(abs(x) < 1e-10 for x in emb):  # ä½¿ç”¨å°çš„é˜ˆå€¼åˆ¤æ–­é›¶å‘é‡
                            failed_embedding_indices.add(i)
                            chunk_text_preview = chunks_data[i].get("chunk_text", "")[:200]
                            self.logger.warning(
                                f"æ£€æµ‹åˆ°é›¶å‘é‡ï¼ˆç´¢å¼• {i}ï¼‰ï¼Œè¡¨ç¤ºè¯¥åˆ†å—å‘é‡åŒ–å¤±è´¥:\n"
                                f"  Chunk ID: {chunks_data[i]['chunk_id']}\n"
                                f"  è‚¡ç¥¨ä»£ç : {chunks_data[i].get('stock_code', 'N/A')}\n"
                                f"  Chunk Text (å‰200å­—ç¬¦):\n"
                                f"  {chunk_text_preview}\n"
                                f"  {'...' if len(chunks_data[i].get('chunk_text', '')) > 200 else ''}"
                            )
                
                # å¦‚æœæœ‰å¤±è´¥çš„å‘é‡ï¼Œè®°å½•ä½†ç»§ç»­å¤„ç†æˆåŠŸçš„
                if failed_embedding_indices:
                    self.logger.warning(
                        f"âš ï¸  æ£€æµ‹åˆ° {len(failed_embedding_indices)} ä¸ªé›¶å‘é‡ï¼ˆéƒ¨åˆ†å¤±è´¥ï¼‰ï¼Œ"
                        f"å°†è·³è¿‡è¿™äº›åˆ†å—ï¼Œç»§ç»­å¤„ç†æˆåŠŸçš„åˆ†å—"
                    )
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå‘é‡å¤±è´¥: {e}", exc_info=True)

            # æ‰“å°æ‰€æœ‰å¤±è´¥åˆ†å—çš„ chunk_text
            self.logger.error("=" * 80)
            self.logger.error(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥ï¼Œå…± {len(chunks_data)} ä¸ªåˆ†å—:")
            self.logger.error("=" * 80)
            for i, item in enumerate(chunks_data, 1):
                chunk_text_preview = item.get("chunk_text", "")[:200]  # å‰200å­—ç¬¦
                self.logger.error(
                    f"\nå¤±è´¥åˆ†å— {i}/{len(chunks_data)}:\n"
                    f"  Chunk ID: {item['chunk_id']}\n"
                    f"  Document ID: {item['document_id']}\n"
                    f"  è‚¡ç¥¨ä»£ç : {item.get('stock_code', 'N/A')}\n"
                    f"  Chunk Text (å‰200å­—ç¬¦):\n"
                    f"  {chunk_text_preview}\n"
                    f"  {'...' if len(item.get('chunk_text', '')) > 200 else ''}"
                )
            self.logger.error("=" * 80)

            # æ›´æ–°æ‰€æœ‰å¤±è´¥åˆ†å—çš„çŠ¶æ€
            error_message = f"ç”Ÿæˆå‘é‡å¤±è´¥: {str(e)[:200]}"
            with self.pg_client.get_session() as session:
                for item in chunks_data:
                    try:
                        crud.update_chunk_status(
                            session=session,
                            chunk_id=item["_chunk_id_uuid"],
                            status='failed',
                            error_message=error_message,
                            increment_retry=True
                        )
                    except Exception as update_error:
                        self.logger.warning(f"æ›´æ–°å¤±è´¥çŠ¶æ€å¼‚å¸¸: chunk_id={item['chunk_id']}, error={update_error}")
                session.commit()

            return {
                "vectorized_count": 0,
                "failed_count": len(chunks_data),
                "failed_chunks": [item["chunk_id"] for item in chunks_data]
            }

        if len(embeddings) != len(chunks_data):
            self.logger.error(
                f"å‘é‡æ•°é‡ä¸åŒ¹é…: æœŸæœ›={len(chunks_data)}, å®é™…={len(embeddings)}"
            )

            # æ‰“å°æ‰€æœ‰åˆ†å—çš„ chunk_text
            self.logger.error("=" * 80)
            self.logger.error("å‘é‡æ•°é‡ä¸åŒ¹é…ï¼Œæ‰€æœ‰åˆ†å—ä¿¡æ¯:")
            self.logger.error("=" * 80)
            for i, item in enumerate(chunks_data, 1):
                chunk_text_preview = item.get("chunk_text", "")[:200]
                self.logger.error(
                    f"\nåˆ†å— {i}/{len(chunks_data)}:\n"
                    f"  Chunk ID: {item['chunk_id']}\n"
                    f"  Document ID: {item['document_id']}\n"
                    f"  è‚¡ç¥¨ä»£ç : {item.get('stock_code', 'N/A')}\n"
                    f"  Chunk Text (å‰200å­—ç¬¦):\n"
                    f"  {chunk_text_preview}\n"
                    f"  {'...' if len(item.get('chunk_text', '')) > 200 else ''}"
                )
            self.logger.error("=" * 80)

            # æ›´æ–°æ‰€æœ‰å¤±è´¥åˆ†å—çš„çŠ¶æ€
            error_message = f"å‘é‡æ•°é‡ä¸åŒ¹é…: æœŸæœ›={len(chunks_data)}, å®é™…={len(embeddings)}"
            with self.pg_client.get_session() as session:
                for item in chunks_data:
                    try:
                        crud.update_chunk_status(
                            session=session,
                            chunk_id=item["_chunk_id_uuid"],
                            status='failed',
                            error_message=error_message,
                            increment_retry=True
                        )
                    except Exception as update_error:
                        self.logger.warning(f"æ›´æ–°å¤±è´¥çŠ¶æ€å¼‚å¸¸: chunk_id={item['chunk_id']}, error={update_error}")
                session.commit()

            return {
                "vectorized_count": 0,
                "failed_count": len(chunks_data),
                "failed_chunks": [item["chunk_id"] for item in chunks_data]
            }

        # âœ… è¿‡æ»¤æ‰é›¶å‘é‡ï¼ˆå¤±è´¥çš„åˆ†å—ï¼‰ï¼Œåªå¤„ç†æˆåŠŸçš„
        successful_data = []
        successful_embeddings = []
        failed_from_zero_vector = []
        
        for i, (item, emb) in enumerate(zip(chunks_data, embeddings)):
            if i in failed_embedding_indices:
                # è¿™æ˜¯é›¶å‘é‡ï¼Œæ ‡è®°ä¸ºå¤±è´¥
                failed_from_zero_vector.append(item["chunk_id"])
            else:
                # æˆåŠŸçš„å‘é‡ï¼Œæ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
                successful_data.append(item)
                successful_embeddings.append(emb)
        
        if failed_from_zero_vector:
            self.logger.info(
                f"è¿‡æ»¤æ‰ {len(failed_from_zero_vector)} ä¸ªå¤±è´¥çš„åˆ†å—ï¼ˆé›¶å‘é‡ï¼‰ï¼Œ"
                f"ç»§ç»­å¤„ç† {len(successful_data)} ä¸ªæˆåŠŸçš„åˆ†å—"
            )
        
        # å¦‚æœæ²¡æœ‰æˆåŠŸçš„åˆ†å—ï¼Œç›´æ¥è¿”å›
        if not successful_data:
            self.logger.warning("æ‰€æœ‰åˆ†å—éƒ½å¤±è´¥ï¼ˆé›¶å‘é‡ï¼‰ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            return {
                "vectorized_count": 0,
                "failed_count": len(chunks_data),
                "failed_chunks": [item["chunk_id"] for item in chunks_data]
            }

        # å‡†å¤‡Milvusæ’å…¥æ•°æ®ï¼ˆåªåŒ…å«æˆåŠŸçš„åˆ†å—ï¼‰
        collection_name = MilvusCollection.DOCUMENTS
        document_ids = []
        chunk_ids = []
        stock_codes = []
        company_names = []
        doc_types = []
        years = []
        quarters = []

        for item in successful_data:
            document_ids.append(item["document_id"])
            chunk_ids.append(item["chunk_id"])
            stock_codes.append(item["stock_code"])
            company_names.append(item["company_name"])
            doc_types.append(item["doc_type"])
            years.append(item["year"])
            quarters.append(item["quarter"])

        # æ’å…¥Milvusï¼ˆåªæ’å…¥æˆåŠŸçš„å‘é‡ï¼‰
        try:
            # è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
            self.logger.debug(
                f"å‡†å¤‡æ’å…¥ Milvus:\n"
                f"  chunk_ids æ•°é‡: {len(chunk_ids)}\n"
                f"  embeddings æ•°é‡: {len(successful_embeddings)}\n"
                f"  ç¬¬ä¸€ä¸ª embedding ç»´åº¦: {len(successful_embeddings[0]) if successful_embeddings else 'N/A'}\n"
                f"  ç¬¬ä¸€ä¸ª embedding ç±»å‹: {type(successful_embeddings[0]) if successful_embeddings else 'N/A'}"
            )
            
            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            if len(successful_embeddings) != len(chunk_ids):
                self.logger.error(
                    f"æ•°æ®ä¸ä¸€è‡´: embeddings={len(successful_embeddings)}, chunk_ids={len(chunk_ids)}"
                )
            
            # éªŒè¯å‘é‡æ ¼å¼
            for i, emb in enumerate(successful_embeddings[:3]):  # åªæ£€æŸ¥å‰3ä¸ª
                if not isinstance(emb, list):
                    self.logger.error(f"å‘é‡ {i} ä¸æ˜¯åˆ—è¡¨ç±»å‹: {type(emb)}")
                elif len(emb) < 10:
                    self.logger.error(f"å‘é‡ {i} ç»´åº¦å¼‚å¸¸å°: {len(emb)}")
            
            vector_ids = self.milvus_client.insert_vectors(
                collection_name=collection_name,
                embeddings=successful_embeddings,
                document_ids=document_ids,
                chunk_ids=chunk_ids,
                stock_codes=stock_codes,
                company_names=company_names,
                doc_types=doc_types,
                years=years,
                quarters=quarters
            )
        except Exception as e:
            self.logger.error(f"æ’å…¥Milvuså¤±è´¥: {e}", exc_info=True)

            # æ‰“å°æ‰€æœ‰åˆ†å—çš„ chunk_text
            self.logger.error("=" * 80)
            self.logger.error("æ’å…¥Milvuså¤±è´¥ï¼Œæ‰€æœ‰åˆ†å—ä¿¡æ¯:")
            self.logger.error("=" * 80)
            for i, item in enumerate(chunks_data, 1):
                chunk_text_preview = item.get("chunk_text", "")[:200]
                self.logger.error(
                    f"\nåˆ†å— {i}/{len(chunks_data)}:\n"
                    f"  Chunk ID: {item['chunk_id']}\n"
                    f"  Document ID: {item['document_id']}\n"
                    f"  è‚¡ç¥¨ä»£ç : {item.get('stock_code', 'N/A')}\n"
                    f"  Chunk Text (å‰200å­—ç¬¦):\n"
                    f"  {chunk_text_preview}\n"
                    f"  {'...' if len(item.get('chunk_text', '')) > 200 else ''}"
                )
            self.logger.error("=" * 80)

            # æ›´æ–°æ‰€æœ‰å¤±è´¥åˆ†å—çš„çŠ¶æ€ï¼ˆæ³¨æ„ï¼šè¿™é‡Œ chunks_data ä»ç„¶æ˜¯å®Œæ•´åˆ—è¡¨ï¼‰
            error_message = f"æ’å…¥Milvuså¤±è´¥: {str(e)[:200]}"
            with self.pg_client.get_session() as session:
                for item in chunks_data:
                    try:
                        crud.update_chunk_status(
                            session=session,
                            chunk_id=item["_chunk_id_uuid"],
                            status='failed',
                            error_message=error_message,
                            increment_retry=True
                        )
                    except Exception as update_error:
                        self.logger.warning(f"æ›´æ–°å¤±è´¥çŠ¶æ€å¼‚å¸¸: chunk_id={item['chunk_id']}, error={update_error}")
                session.commit()

            return {
                "vectorized_count": 0,
                "failed_count": len(chunks_data),
                "failed_chunks": [item["chunk_id"] for item in chunks_data]
            }

        if len(vector_ids) != len(successful_data):
            self.logger.error(
                f"Milvusè¿”å›çš„å‘é‡IDæ•°é‡ä¸åŒ¹é…: æœŸæœ›={len(successful_data)}, å®é™…={len(vector_ids)}"
            )

        # æ›´æ–°æ•°æ®åº“ï¼ˆé‡æ–°æŸ¥è¯¢å¯¹è±¡ï¼Œå› ä¸ºä¹‹å‰çš„å¯¹è±¡å·² detachedï¼‰
        # æ³¨æ„ï¼šåªå¤„ç†æˆåŠŸçš„åˆ†å—ï¼Œå¤±è´¥çš„åˆ†å—å·²ç»åœ¨ failed_from_zero_vector ä¸­
        vectorized_count = 0
        failed_count = len(failed_from_zero_vector)  # ä»é›¶å‘é‡å¼€å§‹çš„å¤±è´¥æ•°
        failed_chunks = failed_from_zero_vector.copy()  # å¤åˆ¶é›¶å‘é‡å¤±è´¥çš„åˆ†å—
        model_name = self.embedder.get_model_name()

        with self.pg_client.get_session() as session:
            # åªå¤„ç†æˆåŠŸçš„åˆ†å—
            for i, item in enumerate(successful_data):
                chunk_id_uuid = item["_chunk_id_uuid"]
                vector_id = str(vector_ids[i]) if i < len(vector_ids) else None

                try:
                    # é‡æ–°æŸ¥è¯¢ chunkï¼ˆå› ä¸ºä¹‹å‰çš„å¯¹è±¡å·² detachedï¼‰
                    chunk = session.query(DocumentChunk).filter(
                        DocumentChunk.id == chunk_id_uuid
                    ).first()

                    if not chunk:
                        failed_count += 1
                        failed_chunks.append(item["chunk_id"])
                        chunk_text_preview = item.get("chunk_text", "")[:200]
                        self.logger.warning(
                            f"åˆ†å—ä¸å­˜åœ¨: {chunk_id_uuid}\n"
                            f"  Document ID: {item['document_id']}\n"
                            f"  è‚¡ç¥¨ä»£ç : {item.get('stock_code', 'N/A')}\n"
                            f"  Chunk Text (å‰200å­—ç¬¦):\n"
                            f"  {chunk_text_preview}\n"
                            f"  {'...' if len(item.get('chunk_text', '')) > 200 else ''}"
                        )
                        continue

                    # æ³¨æ„ï¼šç”±äº Milvus ä½¿ç”¨ chunk_id ä½œä¸ºä¸»é”®ï¼Œ
                    # é‡æ–°æ’å…¥ç›¸åŒ chunk_id çš„å‘é‡æ—¶ä¼šè‡ªåŠ¨è¦†ç›–æ—§å‘é‡ï¼ˆupsert è¡Œä¸ºï¼‰
                    # å› æ­¤ä¸éœ€è¦æ‰‹åŠ¨åˆ é™¤æ—§å‘é‡

                    # æ›´æ–°åˆ†å—çš„å‘é‡åŒ–ä¿¡æ¯ï¼ˆä¸å†éœ€è¦ vector_idï¼Œå› ä¸º Milvus ä¸»é”®å°±æ˜¯ chunk_idï¼‰
                    success = crud.update_chunk_embedding(
                        session=session,
                        chunk_id=chunk_id_uuid,
                        embedding_model=model_name
                    )

                    if success:
                        vectorized_count += 1
                        self.logger.debug(
                            f"æ›´æ–°åˆ†å—å‘é‡åŒ–ä¿¡æ¯æˆåŠŸ: chunk_id={chunk_id_uuid}, model={model_name}"
                        )
                    else:
                        failed_count += 1
                        failed_chunks.append(item["chunk_id"])
                        chunk_text_preview = item.get("chunk_text", "")[:200]
                        self.logger.warning(
                            f"æ›´æ–°åˆ†å—å‘é‡IDå¤±è´¥: chunk_id={chunk_id_uuid}\n"
                            f"  Document ID: {item['document_id']}\n"
                            f"  è‚¡ç¥¨ä»£ç : {item.get('stock_code', 'N/A')}\n"
                            f"  Chunk Text (å‰200å­—ç¬¦):\n"
                            f"  {chunk_text_preview}\n"
                            f"  {'...' if len(item.get('chunk_text', '')) > 200 else ''}"
                        )

                except Exception as e:
                    failed_count += 1
                    failed_chunks.append(item["chunk_id"])
                    chunk_text_preview = item.get("chunk_text", "")[:200]
                    self.logger.error(
                        f"æ›´æ–°åˆ†å—å‘é‡IDå¼‚å¸¸: chunk_id={chunk_id_uuid}, error={e}\n"
                        f"  Document ID: {item['document_id']}\n"
                        f"  è‚¡ç¥¨ä»£ç : {item.get('stock_code', 'N/A')}\n"
                        f"  Chunk Text (å‰200å­—ç¬¦):\n"
                        f"  {chunk_text_preview}\n"
                        f"  {'...' if len(item.get('chunk_text', '')) > 200 else ''}",
                        exc_info=True
                    )

            # ç»Ÿä¸€å¤„ç†æ‰€æœ‰å¤±è´¥çš„åˆ†å—ï¼šæ›´æ–°çŠ¶æ€ä¸º 'failed'
            # åŒ…æ‹¬ï¼š1) é›¶å‘é‡å¤±è´¥ 2) æ•°æ®åº“æ›´æ–°å¤±è´¥ 3) å…¶ä»–å¼‚å¸¸
            all_failed_chunk_ids = set()

            # æ”¶é›†æ‰€æœ‰å¤±è´¥çš„ chunk_idï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
            all_failed_chunk_ids.update(failed_chunks)

            # ä» chunks_data æ‰¾åˆ°å¯¹åº”çš„ UUIDï¼ˆç”¨äºæ›´æ–°æ•°æ®åº“ï¼‰
            failed_chunk_uuids = {}
            for item in chunks_data:
                if item["chunk_id"] in all_failed_chunk_ids:
                    failed_chunk_uuids[item["chunk_id"]] = item["_chunk_id_uuid"]

            # æ‰¹é‡æ›´æ–°å¤±è´¥åˆ†å—çš„çŠ¶æ€
            for chunk_id_str, chunk_id_uuid in failed_chunk_uuids.items():
                try:
                    crud.update_chunk_status(
                        session=session,
                        chunk_id=chunk_id_uuid,
                        status='failed',
                        error_message='å‘é‡åŒ–å¤±è´¥ï¼ˆé›¶å‘é‡æˆ–å¤„ç†å¼‚å¸¸ï¼‰',
                        increment_retry=True
                    )
                    self.logger.debug(f"å·²æ ‡è®°å¤±è´¥çŠ¶æ€: chunk_id={chunk_id_str}")
                except Exception as e:
                    self.logger.warning(f"æ›´æ–°å¤±è´¥çŠ¶æ€å¼‚å¸¸: chunk_id={chunk_id_str}, error={e}")

            session.commit()

        return {
            "vectorized_count": vectorized_count,
            "failed_count": failed_count,
            "failed_chunks": failed_chunks
        }

    def vectorize_document(
        self,
        document_id: Union[uuid.UUID, str],
        force_revectorize: bool = False
    ) -> Dict[str, Any]:
        """
        å‘é‡åŒ–æ–‡æ¡£çš„æ‰€æœ‰åˆ†å—

        Args:
            document_id: æ–‡æ¡£ID
            force_revectorize: æ˜¯å¦å¼ºåˆ¶é‡æ–°å‘é‡åŒ–

        Returns:
            å‘é‡åŒ–ç»“æœå­—å…¸
        """
        # è·å–æ–‡æ¡£çš„æ‰€æœ‰åˆ†å—
        with self.pg_client.get_session() as session:
            chunks = crud.get_document_chunks(session, document_id)
            
            if not chunks:
                return {
                    "success": True,
                    "vectorized_count": 0,
                    "failed_count": 0,
                    "failed_chunks": [],
                    "message": "æ–‡æ¡£æ²¡æœ‰åˆ†å—"
                }

            chunk_ids = [chunk.id for chunk in chunks]

        # å‘é‡åŒ–æ‰€æœ‰åˆ†å—
        return self.vectorize_chunks(chunk_ids, force_revectorize=force_revectorize)


# å…¨å±€Vectorizerå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_vectorizer: Optional[Vectorizer] = None


def get_vectorizer() -> Vectorizer:
    """
    è·å–å…¨å±€Vectorizerå®ä¾‹ï¼ˆå•ä¾‹ï¼‰

    Returns:
        Vectorizerå®ä¾‹

    Example:
        >>> from src.processing.ai.embedding.vectorizer import get_vectorizer
        >>> vectorizer = get_vectorizer()
        >>> result = vectorizer.vectorize_chunks(chunk_ids)
    """
    global _vectorizer
    if _vectorizer is None:
        _vectorizer = Vectorizer()
    return _vectorizer
