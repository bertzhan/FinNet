# -*- coding: utf-8 -*-
"""
Dagster çˆ¬è™«ä½œä¸šå®šä¹‰
é›†æˆæ–°çš„ ingestion å±‚çˆ¬è™«åˆ° Dagster è°ƒåº¦ç³»ç»Ÿ

æŒ‰ç…§ plan.md è®¾è®¡ï¼š
- æ•°æ®é‡‡é›†å±‚ï¼ˆIngestion Layerï¼‰â†’ Dagster è°ƒåº¦
- æ”¯æŒå®šæ—¶è°ƒåº¦ã€æ•°æ®è´¨é‡æ£€æŸ¥ã€å¯è§†åŒ–ç›‘æ§
"""

import os
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

from dagster import (
    job,
    op,
    schedule,
    sensor,
    DefaultSensorStatus,
    DefaultScheduleStatus,
    RunRequest,
    Field,
    get_dagster_logger,
    asset,
    AssetMaterialization,
    MetadataValue,
)

# å¯¼å…¥æ–°çš„çˆ¬è™«æ¨¡å—
from src.ingestion.a_share import ReportCrawler, CninfoIPOProspectusCrawler
from src.ingestion.base.base_crawler import CrawlTask, CrawlResult
from src.common.constants import Market, DocType
from src.common.config import common_config
from src.storage.metadata.quarantine_manager import QuarantineManager
from src.storage.metadata import get_postgres_client, crud

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(common_config.PROJECT_ROOT)
DEFAULT_OUTPUT_ROOT = PROJECT_ROOT / "downloads"


# ==================== é…ç½® Schema ====================

# ä½¿ç”¨ config_schema å­—å…¸æ–¹å¼ï¼ˆå…¼å®¹æ‰€æœ‰ Dagster ç‰ˆæœ¬ï¼‰
REPORT_CRAWL_CONFIG_SCHEMA = {
    "output_root": Field(
        str,
        is_required=False,
        description="Output root directory (default: downloads/)"
    ),
    "workers": Field(
        int,
        default_value=4,
        description="Number of parallel workers (1-16)"
    ),
    "enable_minio": Field(
        bool,
        default_value=True,
        description="Enable MinIO upload"
    ),
    "enable_postgres": Field(
        bool,
        default_value=True,
        description="Enable PostgreSQL metadata recording"
    ),
    "year": Field(
        int,
        is_required=False,
        description="Year to crawl (None = auto: current and previous quarter, specified = all quarters of that year)"
    ),
    "limit": Field(
        int,
        is_required=False,
        description="Limit number of companies to crawl (None = all companies)"
    ),
    "stock_codes": Field(
        list,
        is_required=False,
        description="List of stock codes to crawl (None = use limit filter, specified = only crawl these codes). Example: ['000001', '000002']"
    ),
}

IPO_CRAWL_CONFIG_SCHEMA = {
    "output_root": Field(
        str,
        is_required=False,
        description="Output root directory (default: downloads/)"
    ),
    "workers": Field(
        int,
        default_value=4,
        description="Number of parallel workers (1-16)"
    ),
    "enable_minio": Field(
        bool,
        default_value=True,
        description="Enable MinIO upload"
    ),
    "enable_postgres": Field(
        bool,
        default_value=True,
        description="Enable PostgreSQL metadata recording"
    ),
    "limit": Field(
        int,
        is_required=False,
        description="Limit number of companies to crawl (None = all companies)"
    ),
    "stock_codes": Field(
        list,
        is_required=False,
        description="List of stock codes to crawl (None = use limit filter, specified = only crawl these codes). Example: ['000001', '000002']"
    ),
}


# ==================== è¾…åŠ©å‡½æ•° ====================

def load_company_list_from_db(
    limit: Optional[int] = None,
    stock_codes: Optional[List[str]] = None,
    logger=None
) -> List[Dict[str, str]]:
    """
    ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨

    Args:
        limit: é™åˆ¶è¿”å›æ•°é‡ï¼ˆNone è¡¨ç¤ºè¿”å›æ‰€æœ‰ï¼‰
        stock_codes: æŒ‰è‚¡ç¥¨ä»£ç åˆ—è¡¨è¿‡æ»¤ï¼ˆNone è¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
        logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¦‚æœä¸º Noneï¼Œä½¿ç”¨æ ‡å‡† loggingï¼‰

    Returns:
        å…¬å¸åˆ—è¡¨ [{'code': '000001', 'name': 'å¹³å®‰é“¶è¡Œ'}, ...]
    """
    if logger is None:
        import logging
        logger = logging.getLogger(__name__)

    companies = []
    try:
        pg_client = get_postgres_client()
        with pg_client.get_session() as session:
            # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¼˜å…ˆä½¿ç”¨è‚¡ç¥¨ä»£ç è¿‡æ»¤
            if stock_codes:
                from src.storage.metadata.models import ListedCompany
                # æŸ¥è¯¢æŒ‡å®šçš„è‚¡ç¥¨ä»£ç 
                listed_companies = session.query(ListedCompany).filter(
                    ListedCompany.code.in_(stock_codes)
                ).all()
                logger.info(f"æŒ‰è‚¡ç¥¨ä»£ç è¿‡æ»¤: æŒ‡å®š {len(stock_codes)} ä¸ªä»£ç ï¼Œæ‰¾åˆ° {len(listed_companies)} å®¶å…¬å¸")
            else:
                # ä½¿ç”¨åŸæœ‰çš„ limit è¿‡æ»¤
                listed_companies = crud.get_all_listed_companies(session, limit=limit)

            for company in listed_companies:
                companies.append({
                    'code': company.code,
                    'name': company.name
                })

        if stock_codes:
            logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(companies)} å®¶å…¬å¸ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç : {stock_codes}ï¼‰")
        else:
            logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(companies)} å®¶å…¬å¸")
    except Exception as e:
        logger.error(f"ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“å¤±è´¥

    return companies


def calculate_quarters() -> tuple:
    """
    è®¡ç®—å½“å‰å­£åº¦å’Œä¸Šä¸€å­£åº¦
    
    Returns:
        (current_year, current_quarter, prev_year, prev_quarter)
    """
    now = datetime.now()
    current_year = now.year
    current_month = now.month
    
    if current_month <= 3:
        current_quarter = 1
    elif current_month <= 6:
        current_quarter = 2
    elif current_month <= 9:
        current_quarter = 3
    else:
        current_quarter = 4
    
    # è®¡ç®—ä¸Šä¸€å­£åº¦
    if current_quarter == 1:
        prev_year = current_year - 1
        prev_quarter = 4
    else:
        prev_year = current_year
        prev_quarter = current_quarter - 1
    
    return current_year, current_quarter, prev_year, prev_quarter


# ==================== Dagster Ops ====================

@op(config_schema=REPORT_CRAWL_CONFIG_SCHEMA)
def crawl_a_share_reports_op(context) -> Dict:
    """
    çˆ¬å–Aè‚¡å®šæœŸæŠ¥å‘Šï¼ˆå¹´æŠ¥/å­£æŠ¥ï¼‰
    
    æŒ‰ç…§ plan.md è®¾è®¡ï¼š
    - è‡ªåŠ¨è®¡ç®—ä¸Šä¸€å­£åº¦å’Œå½“å‰å­£åº¦
    - æ‰¹é‡çˆ¬å–æ‰€æœ‰ä¸Šå¸‚å…¬å¸æŠ¥å‘Š
    - è‡ªåŠ¨ä¸Šä¼ åˆ° MinIOï¼ˆBronzeå±‚ï¼‰
    - è‡ªåŠ¨è®°å½•åˆ° PostgreSQL
    """
    config = context.op_config
    logger = get_dagster_logger()
    
    # è§£æé…ç½®
    output_root = config.get("output_root") or str(DEFAULT_OUTPUT_ROOT)
    workers = config.get("workers", 4)
    enable_minio = config.get("enable_minio", True)
    enable_postgres = config.get("enable_postgres", True)
    
    # è®¡ç®—å¹´ä»½å’Œå­£åº¦
    year = config.get("year")
    
    if year is None:
        # è‡ªåŠ¨è®¡ç®—ï¼šçˆ¬å–å½“å‰å­£åº¦å’Œä¸Šä¸€å­£åº¦
        current_year, current_quarter, prev_year, prev_quarter = calculate_quarters()
        years_quarters = [
            (prev_year, prev_quarter),
            (current_year, current_quarter),
        ]
        logger.info(f"è‡ªåŠ¨è®¡ç®—å­£åº¦: {prev_year}Q{prev_quarter}, {current_year}Q{current_quarter}")
    else:
        # æŒ‡å®šå¹´ä»½ï¼šçˆ¬å–è¯¥å¹´çš„æ‰€æœ‰å­£åº¦ï¼ˆQ1, Q2, Q3, Q4ï¼‰
        years_quarters = [
            (year, 1),  # Q1: å­£åº¦æŠ¥å‘Š
            (year, 2),  # Q2: åŠå¹´æŠ¥
            (year, 3),  # Q3: å­£åº¦æŠ¥å‘Š
            (year, 4),  # Q4: å¹´æŠ¥
        ]
        logger.info(f"æŒ‡å®šå¹´ä»½ {year}ï¼Œå°†çˆ¬å–è¯¥å¹´çš„æ‰€æœ‰å­£åº¦æŠ¥å‘Š: Q1, Q2, Q3, Q4")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_root, exist_ok=True)

    # ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨
    limit = config.get("limit")
    stock_codes = config.get("stock_codes")

    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    if stock_codes:
        logger.info(f"ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç : {stock_codes}ï¼‰...")
    elif limit is not None:
        logger.info(f"ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨ï¼ˆé™åˆ¶å‰ {limit} å®¶ï¼‰...")
    else:
        logger.info("ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨...")

    try:
        companies = load_company_list_from_db(
            limit=limit,
            stock_codes=stock_codes,
            logger=logger
        )
        if not companies:
            if stock_codes:
                logger.warning(f"âš ï¸ å…¬å¸åˆ—è¡¨ä¸ºç©ºï¼Œæœªæ‰¾åˆ°æŒ‡å®šçš„è‚¡ç¥¨ä»£ç : {stock_codes}")
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°æŒ‡å®šçš„è‚¡ç¥¨ä»£ç : {stock_codes}",
                    "total": 0,
                    "success_count": 0,
                    "fail_count": 0,
                    "results": []
                }
            else:
                logger.warning("âš ï¸ å…¬å¸åˆ—è¡¨ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ update_listed_companies_job æ›´æ–°å…¬å¸åˆ—è¡¨")
                return {
                    "success": False,
                    "error": "å…¬å¸åˆ—è¡¨ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ update_listed_companies_job æ›´æ–°å…¬å¸åˆ—è¡¨",
                    "total": 0,
                    "success_count": 0,
                    "fail_count": 0,
                    "results": []
                }

        # æ„å»ºè¿‡æ»¤ä¿¡æ¯
        filter_info = ""
        if stock_codes:
            filter_info = f"ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç : {stock_codes}ï¼‰"
        elif limit is not None:
            filter_info = f"ï¼ˆé™åˆ¶ä¸ºå‰ {limit} å®¶ï¼‰"
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(companies)} å®¶å…¬å¸{filter_info}")
    except Exception as e:
        logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨å¤±è´¥: {str(e)}",
            "total": 0,
            "success_count": 0,
            "fail_count": 0,
            "results": []
        }
    
    # è®°å½•é…ç½®ä¿¡æ¯
    logger.info(f"çˆ¬è™«é…ç½®: enable_minio={enable_minio}, enable_postgres={enable_postgres}, workers={workers}" + (f", limit={limit}" if limit is not None else ""))
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä¸å†ä½¿ç”¨ old_pdf_dirï¼‰
    crawler = ReportCrawler(
        enable_minio=enable_minio,
        enable_postgres=enable_postgres,
        workers=workers
    )
    
    # éªŒè¯ MinIO é…ç½®
    if enable_minio:
        if crawler.enable_minio and crawler.minio_client:
            logger.info("âœ… MinIO å·²å¯ç”¨ä¸”å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.error(f"âŒ MinIO é…ç½®å¼‚å¸¸: enable_minio={crawler.enable_minio}, client={crawler.minio_client is not None}")
    else:
        logger.warning("âš ï¸ MinIO æœªå¯ç”¨ï¼ˆé…ç½®ä¸­ enable_minio=Falseï¼‰")
    
    # ç”Ÿæˆä»»åŠ¡åˆ—è¡¨
    tasks = []
    for company in companies:
        for y, q in years_quarters:
            # æ ¹æ®å­£åº¦è‡ªåŠ¨ç¡®å®šæ–‡æ¡£ç±»å‹
            # Q1, Q3: å­£åº¦æŠ¥å‘Š (quarterly_report)
            # Q2: åŠå¹´æŠ¥ (interim_report)
            # Q4: å¹´æŠ¥ (annual_report)
            if q == 4:
                task_doc_type = DocType.ANNUAL_REPORT
            elif q == 2:
                task_doc_type = DocType.INTERIM_REPORT
            else:
                task_doc_type = DocType.QUARTERLY_REPORT

            tasks.append(CrawlTask(
                stock_code=company['code'],
                company_name=company['name'],
                market=Market.A_SHARE,
                doc_type=task_doc_type,
                year=y,
                quarter=q
            ))
    
    logger.info(f"ç”Ÿæˆ {len(tasks)} ä¸ªçˆ¬å–ä»»åŠ¡ï¼ˆ{len(companies)} å®¶å…¬å¸ Ã— {len(years_quarters)} ä¸ªå­£åº¦ï¼‰")
    
    # æ‰§è¡Œæ‰¹é‡çˆ¬å–ï¼ˆå®æ—¶è®°å½•è¿›åº¦å’Œèµ„äº§ï¼‰
    results = []
    success_count = 0
    fail_count = 0
    total = len(tasks)
    
    try:
        # è‡ªå·±å¾ªç¯è°ƒç”¨ crawl()ï¼Œä»¥ä¾¿å®æ—¶è®°å½•è¿›åº¦å’Œ AssetMaterialization
        for idx, task in enumerate(tasks, 1):
            # å®æ—¶è¿›åº¦æ—¥å¿—ï¼ˆæ¯10ä¸ªæˆ–æ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€ä¸ªï¼‰
            if idx % 10 == 0 or idx % max(1, total // 10) == 0 or idx == total:
                progress_pct = idx / total * 100
                logger.info(
                    f"ğŸ“¦ [{idx}/{total}] {progress_pct:.1f}% | "
                    f"æ­£åœ¨çˆ¬å–: {task.stock_code} - {task.company_name} "
                    f"{task.year}Q{task.quarter if task.quarter else ''}"
                )
            
            # æ‰§è¡Œå•ä¸ªä»»åŠ¡
            try:
                result = crawler.crawl(task)
                results.append(result)
                
                # å®æ—¶è®°å½• AssetMaterializationï¼ˆæˆåŠŸæ—¶ç«‹å³è®°å½•ï¼‰
                if result.success:
                    success_count += 1
                    
                    # æ ¹æ®å­£åº¦ç¡®å®šæ–‡æ¡£ç±»å‹å­—ç¬¦ä¸²ï¼ˆç”¨äºèµ„äº§keyï¼‰
                    if result.task.quarter == 4:
                        doc_type_str = "annual_report"
                    elif result.task.quarter == 2:
                        doc_type_str = "interim_report"
                    else:
                        doc_type_str = "quarterly_report"
                    
                    # ç«‹å³è®°å½• AssetMaterializationï¼Œæ— éœ€ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    try:
                        context.log_event(
                            AssetMaterialization(
                                asset_key=["bronze", "a_share", doc_type_str, str(result.task.year), f"Q{result.task.quarter}"],
                                description=f"{result.task.company_name} {result.task.year} Q{result.task.quarter}",
                                metadata={
                                    "stock_code": MetadataValue.text(result.task.stock_code),
                                    "company_name": MetadataValue.text(result.task.company_name),
                                    "minio_path": MetadataValue.text(result.minio_object_path or ""),
                                    "file_size": MetadataValue.int(result.file_size or 0),
                                    "file_hash": MetadataValue.text(result.file_hash or ""),
                                    "document_id": MetadataValue.text(str(result.document_id) if result.document_id else ""),
                                    "progress": MetadataValue.text(f"{idx}/{total} ({idx/total*100:.1f}%)"),
                                }
                            )
                        )
                        logger.debug(f"âœ… å·²è®°å½•èµ„äº§: {result.task.stock_code} {result.task.year} Q{result.task.quarter}")
                    except Exception as e:
                        logger.warning(f"è®°å½• AssetMaterialization å¤±è´¥ (task={result.task.stock_code}): {e}")
                else:
                    fail_count += 1
                    logger.warning(
                        f"âŒ çˆ¬å–å¤±è´¥: {task.stock_code} ({task.company_name}) "
                        f"{task.year} Q{task.quarter} - {result.error_message}"
                    )
            except KeyboardInterrupt:
                # ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ï¼ˆCtrl+Cï¼‰
                logger.warning(f"âš ï¸ çˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­: {task.stock_code}")
                raise
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ Dagster ä¸­æ–­å¼‚å¸¸
                error_type = type(e).__name__
                if "Interrupt" in error_type or "Interrupted" in error_type:
                    logger.warning(f"âš ï¸ çˆ¬å–è¢«ä¸­æ–­: {task.stock_code}, error_type={error_type}")
                    raise
                
                fail_count += 1
                logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {task.stock_code} - {e}", exc_info=True)
                # åˆ›å»ºå¤±è´¥ç»“æœ
                from src.ingestion.base.base_crawler import CrawlResult
                results.append(CrawlResult(
                    task=task,
                    success=False,
                    error_message=str(e)
                ))
        
        logger.info(f"âœ… çˆ¬å–å®Œæˆ: æˆåŠŸ {success_count}/{total}, å¤±è´¥ {fail_count}/{total}")
        
        # è®°å½•å¤±è´¥ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯
        if fail_count > 0:
            logger.warning(f"âš ï¸ æœ‰ {fail_count} ä¸ªä»»åŠ¡å¤±è´¥ï¼Œè¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼š")
            failed_results = [r for r in results if not r.success]
            for i, result in enumerate(failed_results[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                logger.error(
                    f"  å¤±è´¥ä»»åŠ¡ {i}: {result.task.stock_code} ({result.task.company_name}) "
                    f"{result.task.year} Q{result.task.quarter} - {result.error_message}"
                )
            if fail_count > 10:
                logger.warning(f"  ... è¿˜æœ‰ {fail_count - 10} ä¸ªå¤±è´¥ä»»åŠ¡")
    
    except KeyboardInterrupt:
        logger.warning("âš ï¸ æ‰¹é‡çˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
        raise
    except Exception as e:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Dagster ä¸­æ–­å¼‚å¸¸
        error_type = type(e).__name__
        if "Interrupt" in error_type or "Interrupted" in error_type:
            logger.warning(f"âš ï¸ æ‰¹é‡çˆ¬å–è¢«ä¸­æ–­: {error_type}")
            raise
        
        logger.error(f"âŒ æ‰¹é‡çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"æ‰¹é‡çˆ¬å–å¼‚å¸¸: {str(e)}",
            "total": len(tasks),
            "success_count": 0,
            "fail_count": len(tasks),
            "results": []
        }
    
    # è¿”å›ç»“æœ
    return {
        "success": True,
        "output_root": output_root,
        "total": len(results),
        "success_count": success_count,
        "fail_count": fail_count,
        "results": [
            {
                "stock_code": r.task.stock_code,
                "company_name": r.task.company_name,
                "year": r.task.year,
                "quarter": r.task.quarter,
                "doc_type": r.task.doc_type.value if r.task.doc_type else "quarterly_report",
                "success": r.success,
                "minio_object_path": r.minio_object_path if r.success else None,
                "document_id": r.document_id if r.success else None,
                "error": r.error_message if not r.success else None,
            }
            for r in results
        ]
    }


@op(config_schema=IPO_CRAWL_CONFIG_SCHEMA)
def crawl_a_share_ipo_op(context) -> Dict:
    """
    çˆ¬å–Aè‚¡IPOæ‹›è‚¡è¯´æ˜ä¹¦
    
    æŒ‰ç…§ plan.md è®¾è®¡ï¼š
    - æ‰¹é‡çˆ¬å–æ‰€æœ‰IPOæ‹›è‚¡è¯´æ˜ä¹¦
    - è‡ªåŠ¨ä¸Šä¼ åˆ° MinIOï¼ˆBronzeå±‚ï¼‰
    - è‡ªåŠ¨è®°å½•åˆ° PostgreSQL
    """
    config = context.op_config
    logger = get_dagster_logger()
    
    # è§£æé…ç½®
    output_root = config.get("output_root") or str(DEFAULT_OUTPUT_ROOT)
    workers = config.get("workers", 4)
    enable_minio = config.get("enable_minio", True)
    enable_postgres = config.get("enable_postgres", True)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_root, exist_ok=True)
    
    # ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨
    limit = config.get("limit")
    stock_codes = config.get("stock_codes")

    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    if stock_codes:
        logger.info(f"ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç : {stock_codes}ï¼‰...")
    elif limit is not None:
        logger.info(f"ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨ï¼ˆé™åˆ¶å‰ {limit} å®¶ï¼‰...")
    else:
        logger.info("ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨...")

    try:
        companies = load_company_list_from_db(
            limit=limit,
            stock_codes=stock_codes,
            logger=logger
        )
        if not companies:
            if stock_codes:
                logger.warning(f"âš ï¸ å…¬å¸åˆ—è¡¨ä¸ºç©ºï¼Œæœªæ‰¾åˆ°æŒ‡å®šçš„è‚¡ç¥¨ä»£ç : {stock_codes}")
                return {
                    "success": False,
                    "error": f"æœªæ‰¾åˆ°æŒ‡å®šçš„è‚¡ç¥¨ä»£ç : {stock_codes}",
                    "total": 0,
                    "success_count": 0,
                    "fail_count": 0,
                    "results": []
                }
            else:
                logger.warning("âš ï¸ å…¬å¸åˆ—è¡¨ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ update_listed_companies_job æ›´æ–°å…¬å¸åˆ—è¡¨")
                return {
                    "success": False,
                    "error": "å…¬å¸åˆ—è¡¨ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ update_listed_companies_job æ›´æ–°å…¬å¸åˆ—è¡¨",
                    "total": 0,
                    "success_count": 0,
                    "fail_count": 0,
                    "results": []
                }

        # æ„å»ºè¿‡æ»¤ä¿¡æ¯
        filter_info = ""
        if stock_codes:
            filter_info = f"ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç : {stock_codes}ï¼‰"
        elif limit is not None:
            filter_info = f"ï¼ˆé™åˆ¶ä¸ºå‰ {limit} å®¶ï¼‰"
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(companies)} å®¶å…¬å¸{filter_info}")
    except Exception as e:
        logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"ä»æ•°æ®åº“åŠ è½½å…¬å¸åˆ—è¡¨å¤±è´¥: {str(e)}",
            "total": 0,
            "success_count": 0,
            "fail_count": 0,
            "results": []
        }
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä¸å†ä½¿ç”¨ old_pdf_dirï¼‰
    crawler = CninfoIPOProspectusCrawler(
        enable_minio=enable_minio,
        enable_postgres=enable_postgres,
        workers=workers
    )
    
    # ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ï¼ˆIPOä¸éœ€è¦yearå’Œquarterï¼‰
    tasks = [
        CrawlTask(
            stock_code=company['code'],
            company_name=company['name'],
            market=Market.A_SHARE,
            doc_type=DocType.IPO_PROSPECTUS,
            year=None,
            quarter=None
        )
        for company in companies
    ]
    
    logger.info(f"ç”Ÿæˆ {len(tasks)} ä¸ªIPOçˆ¬å–ä»»åŠ¡")
    
    # æ‰§è¡Œæ‰¹é‡çˆ¬å–ï¼ˆå®æ—¶è®°å½•è¿›åº¦å’Œèµ„äº§ï¼‰
    results = []
    success_count = 0
    fail_count = 0
    minio_upload_count = 0
    minio_fail_count = 0
    total = len(tasks)
    
    try:
        # è‡ªå·±å¾ªç¯è°ƒç”¨ crawl()ï¼Œä»¥ä¾¿å®æ—¶è®°å½•è¿›åº¦å’Œ AssetMaterialization
        for idx, task in enumerate(tasks, 1):
            # å®æ—¶è¿›åº¦æ—¥å¿—ï¼ˆæ¯10ä¸ªæˆ–æ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€ä¸ªï¼‰
            if idx % 10 == 0 or idx % max(1, total // 10) == 0 or idx == total:
                progress_pct = idx / total * 100
                logger.info(
                    f"ğŸ“¦ [{idx}/{total}] {progress_pct:.1f}% | "
                    f"æ­£åœ¨çˆ¬å–IPO: {task.stock_code} - {task.company_name}"
                )
            
            # æ‰§è¡Œå•ä¸ªä»»åŠ¡
            try:
                result = crawler.crawl(task)
                results.append(result)
                
                # å®æ—¶è®°å½• AssetMaterializationï¼ˆæˆåŠŸæ—¶ç«‹å³è®°å½•ï¼‰
                if result.success:
                    success_count += 1
                    
                    # ç»Ÿè®¡ MinIO ä¸Šä¼ æƒ…å†µ
                    if result.minio_object_path:
                        minio_upload_count += 1
                    else:
                        minio_fail_count += 1
                    
                    # ç«‹å³è®°å½• AssetMaterializationï¼Œæ— éœ€ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    try:
                        context.log_event(
                            AssetMaterialization(
                                asset_key=["bronze", "a_share", "ipo_prospectus"],
                                description=f"{result.task.company_name} IPOæ‹›è‚¡è¯´æ˜ä¹¦",
                                metadata={
                                    "stock_code": MetadataValue.text(result.task.stock_code),
                                    "company_name": MetadataValue.text(result.task.company_name),
                                    "minio_path": MetadataValue.text(result.minio_object_path or ""),
                                    "file_size": MetadataValue.int(result.file_size or 0),
                                    "file_hash": MetadataValue.text(result.file_hash or ""),
                                    "document_id": MetadataValue.text(str(result.document_id) if result.document_id else ""),
                                    "progress": MetadataValue.text(f"{idx}/{total} ({idx/total*100:.1f}%)"),
                                }
                            )
                        )
                        logger.debug(f"âœ… å·²è®°å½•èµ„äº§: {result.task.stock_code} IPO")
                    except Exception as e:
                        logger.warning(f"è®°å½• AssetMaterialization å¤±è´¥ (task={result.task.stock_code}): {e}")
                else:
                    fail_count += 1
                    logger.warning(
                        f"âŒ IPOçˆ¬å–å¤±è´¥: {task.stock_code} ({task.company_name}) - {result.error_message}"
                    )
            except KeyboardInterrupt:
                # ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ï¼ˆCtrl+Cï¼‰
                logger.warning(f"âš ï¸ IPOçˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­: {task.stock_code}")
                raise
            except Exception as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ Dagster ä¸­æ–­å¼‚å¸¸
                error_type = type(e).__name__
                if "Interrupt" in error_type or "Interrupted" in error_type:
                    logger.warning(f"âš ï¸ IPOçˆ¬å–è¢«ä¸­æ–­: {task.stock_code}, error_type={error_type}")
                    raise
                
                fail_count += 1
                logger.error(f"âŒ IPOä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {task.stock_code} - {e}", exc_info=True)
                # åˆ›å»ºå¤±è´¥ç»“æœ
                from src.ingestion.base.base_crawler import CrawlResult
                results.append(CrawlResult(
                    task=task,
                    success=False,
                    error_message=str(e)
                ))
        
        logger.info(f"âœ… IPOçˆ¬å–å®Œæˆ: æˆåŠŸ {success_count}/{total}, å¤±è´¥ {fail_count}/{total}")
        if enable_minio:
            logger.info(f"MinIO ä¸Šä¼ : æˆåŠŸ {minio_upload_count}, å¤±è´¥ {minio_fail_count}")
            if minio_fail_count > 0:
                logger.warning(f"âš ï¸ æœ‰ {minio_fail_count} ä¸ªæ–‡ä»¶ä¸‹è½½æˆåŠŸä½†æœªä¸Šä¼ åˆ° MinIO")
        else:
            logger.warning("âš ï¸ MinIO æœªå¯ç”¨ï¼Œæ–‡ä»¶æœªä¸Šä¼ ")
    
    except KeyboardInterrupt:
        logger.warning("âš ï¸ IPOæ‰¹é‡çˆ¬å–è¢«ç”¨æˆ·ä¸­æ–­")
        raise
    except Exception as e:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Dagster ä¸­æ–­å¼‚å¸¸
        error_type = type(e).__name__
        if "Interrupt" in error_type or "Interrupted" in error_type:
            logger.warning(f"âš ï¸ IPOæ‰¹é‡çˆ¬å–è¢«ä¸­æ–­: {error_type}")
            raise
        
        logger.error(f"âŒ IPOæ‰¹é‡çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"IPOæ‰¹é‡çˆ¬å–å¼‚å¸¸: {str(e)}",
            "total": len(tasks),
            "success_count": 0,
            "fail_count": len(tasks),
            "results": []
        }
    
    return {
        "success": True,
        "output_root": output_root,
        "total": len(results),
        "success_count": success_count,
        "fail_count": fail_count,
        "results": [
            {
                "stock_code": r.task.stock_code,
                "company_name": r.task.company_name,
                "success": r.success,
                "minio_object_path": r.minio_object_path if r.success else None,
                "document_id": r.document_id if r.success else None,
                "error": r.error_message if not r.success else None,
            }
            for r in results
        ]
    }


@op
def validate_crawl_results_op(context, crawl_results: Dict) -> Dict:
    """
    éªŒè¯çˆ¬å–ç»“æœï¼ˆæ•°æ®è´¨é‡æ£€æŸ¥ï¼‰
    
    æŒ‰ç…§ plan.md 7.1 å…¨é“¾è·¯éªŒè¯æ¶æ„ï¼š
    - æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
    - æ•°æ®é‡æ£€æŸ¥
    - å…ƒæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    """
    logger = get_dagster_logger()
    
    if not crawl_results.get("success"):
        logger.warning("çˆ¬å–å¤±è´¥ï¼Œè·³è¿‡éªŒè¯")
        return {
            "validated": False,
            "reason": "çˆ¬å–å¤±è´¥",
            "validated_count": 0,
            "failed_count": 0
        }
    
    results = crawl_results.get("results", [])
    validated_count = 0
    failed_count = 0
    quarantined_count = 0
    
    # åˆå§‹åŒ–éš”ç¦»ç®¡ç†å™¨
    quarantine_manager = None
    try:
        quarantine_manager = QuarantineManager()
        logger.info("éš”ç¦»ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.warning(f"éš”ç¦»ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡è‡ªåŠ¨éš”ç¦»")
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    failed_results = []
    passed_results = []
    
    for result_info in results:
        stock_code = result_info.get("stock_code", "æœªçŸ¥")
        company_name = result_info.get("company_name", "æœªçŸ¥")
        year = result_info.get("year")
        quarter = result_info.get("quarter")
        doc_type = result_info.get("doc_type", "quarterly_report")
        minio_path = result_info.get("minio_object_path")
        doc_id = result_info.get("document_id")
        
        # å¦‚æœä»»åŠ¡æœ¬èº«å¤±è´¥ï¼Œè®°å½•å¤±è´¥åŸå› 
        if not result_info.get("success"):
            error_msg = result_info.get("error", "æœªçŸ¥é”™è¯¯")
            reason = f"çˆ¬å–å¤±è´¥: {error_msg}"
            failed_results.append({
                "stock_code": stock_code,
                "company_name": company_name,
                "year": year,
                "quarter": quarter,
                "reason": reason
            })
            failed_count += 1
            
            # å¦‚æœæ–‡ä»¶å·²ä¸Šä¼ ä½†çˆ¬å–å¤±è´¥ï¼Œéš”ç¦»æ–‡ä»¶
            if quarantine_manager and minio_path and doc_id:
                try:
                    quarantine_manager.quarantine_document(
                        document_id=doc_id,
                        source_type="a_share",
                        doc_type=doc_type,
                        original_path=minio_path,
                        failure_stage="ingestion_failed",
                        failure_reason=reason,
                        failure_details=error_msg
                    )
                    quarantined_count += 1
                    logger.info(f"âœ… å·²éš”ç¦»çˆ¬å–å¤±è´¥çš„æ–‡æ¡£: {minio_path}")
                except Exception as e:
                    logger.error(f"âŒ éš”ç¦»å¤±è´¥: {e}")
            continue
        
        # æ£€æŸ¥1: MinIOè·¯å¾„æ˜¯å¦å­˜åœ¨
        if not minio_path:
            logger.warning(f"ç¼ºå°‘MinIOè·¯å¾„: {stock_code}")
            reason = "ç¼ºå°‘MinIOè·¯å¾„"
            failed_results.append({
                "stock_code": stock_code,
                "company_name": company_name,
                "year": year,
                "quarter": quarter,
                "reason": reason
            })
            failed_count += 1
            continue
        
        # æ£€æŸ¥2: æ•°æ®åº“IDæ˜¯å¦å­˜åœ¨
        if not doc_id:
            logger.warning(f"ç¼ºå°‘æ•°æ®åº“ID: {stock_code}")
            
            # å…ˆå°è¯•é‡æ–°åˆ›å»ºæ•°æ®åº“è®°å½•
            retry_success = False
            if minio_path and stock_code and company_name and year:
                try:
                    from src.storage.metadata import get_postgres_client, crud
                    from src.common.constants import Market, DocType
                    
                    pg_client = get_postgres_client()
                    with pg_client.get_session() as session:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆå¯èƒ½åœ¨å…¶ä»–åœ°æ–¹å·²åˆ›å»ºï¼‰
                        existing_doc = crud.get_document_by_path(session, minio_path)
                        if existing_doc:
                            doc_id = existing_doc.id
                            logger.info(f"âœ… å‘ç°å·²å­˜åœ¨çš„æ•°æ®åº“è®°å½•: id={doc_id}")
                            retry_success = True
                        else:
                            # å°è¯•åˆ›å»ºæ–°è®°å½•
                            # å°†doc_typeå­—ç¬¦ä¸²è½¬æ¢ä¸ºDocTypeæšä¸¾
                            doc_type_enum = None
                            if doc_type == "quarterly_report":
                                doc_type_enum = DocType.QUARTERLY_REPORT
                            elif doc_type == "annual_report":
                                doc_type_enum = DocType.ANNUAL_REPORT
                            elif doc_type == "interim_report":
                                doc_type_enum = DocType.INTERIM_REPORT
                            elif doc_type == "ipo_prospectus":
                                doc_type_enum = DocType.IPO_PROSPECTUS
                            else:
                                doc_type_enum = DocType.QUARTERLY_REPORT  # é»˜è®¤å€¼
                            
                            doc = crud.create_document(
                                session=session,
                                stock_code=stock_code,
                                company_name=company_name,
                                market=Market.A_SHARE.value,
                                doc_type=doc_type_enum.value,
                                year=year,
                                quarter=quarter,
                                minio_object_path=minio_path,
                                file_size=None,  # éªŒè¯é˜¶æ®µæ— æ³•è·å–æ–‡ä»¶å¤§å°
                                file_hash=None,  # éªŒè¯é˜¶æ®µæ— æ³•è·å–æ–‡ä»¶å“ˆå¸Œ
                                metadata=None
                            )
                            doc_id = doc.id
                            logger.info(f"âœ… é‡æ–°åˆ›å»ºæ•°æ®åº“è®°å½•æˆåŠŸ: id={doc_id}")
                            retry_success = True
                except Exception as e:
                    logger.error(f"âŒ é‡æ–°åˆ›å»ºæ•°æ®åº“è®°å½•å¤±è´¥: {e}", exc_info=True)
            
            # å¦‚æœé‡æ–°åˆ›å»ºå¤±è´¥ï¼Œåˆ™éš”ç¦»æ–‡ä»¶
            if not retry_success:
                reason = "ç¼ºå°‘æ•°æ®åº“IDä¸”é‡æ–°åˆ›å»ºå¤±è´¥"
                failed_results.append({
                    "stock_code": stock_code,
                    "company_name": company_name,
                    "year": year,
                    "quarter": quarter,
                    "reason": reason
                })
                failed_count += 1
                
                # éš”ç¦»æ–‡ä»¶ï¼ˆå¦‚æœå·²ä¸Šä¼ åˆ°MinIOï¼‰
                if quarantine_manager and minio_path:
                    try:
                        quarantine_manager.quarantine_document(
                            document_id=None,
                            source_type="a_share",
                            doc_type=doc_type,
                            original_path=minio_path,
                            failure_stage="validation_failed",
                            failure_reason=reason,
                            failure_details="æ–‡æ¡£è®°å½•æœªæˆåŠŸåˆ›å»ºåˆ°æ•°æ®åº“ï¼Œä¸”é‡æ–°åˆ›å»ºå¤±è´¥"
                        )
                        quarantined_count += 1
                        logger.info(f"âœ… å·²éš”ç¦»ç¼ºå°‘æ•°æ®åº“IDçš„æ–‡æ¡£: {minio_path}")
                    except Exception as e:
                        logger.error(f"âŒ éš”ç¦»å¤±è´¥: {e}")
                continue
            
            # é‡æ–°åˆ›å»ºæˆåŠŸï¼Œç»§ç»­éªŒè¯æµç¨‹ï¼ˆä¸continueï¼Œç»§ç»­æ‰§è¡Œåé¢çš„éªŒè¯ï¼‰
            logger.info(f"âœ… æ•°æ®åº“è®°å½•å·²æ¢å¤: {stock_code}, document_id={doc_id}")
        
        # æ£€æŸ¥3: æ–‡ä»¶å¤§å°åˆç†æ€§ï¼ˆPDFåº”è¯¥>10KBï¼‰
        # è¿™ä¸ªä¿¡æ¯åœ¨crawl_resultsä¸­æ²¡æœ‰ï¼Œéœ€è¦ä»æ•°æ®åº“æŸ¥è¯¢
        # æš‚æ—¶è·³è¿‡ï¼Œåç»­å¯ä»¥ä»PostgreSQLæŸ¥è¯¢
        
        # éªŒè¯é€šè¿‡
        passed_results.append({
            "stock_code": stock_code,
            "company_name": company_name,
            "year": year,
            "quarter": quarter,
            "minio_path": minio_path,
            "document_id": doc_id
        })
        validated_count += 1
    
    logger.info(f"éªŒè¯å®Œæˆ: é€šè¿‡ {validated_count}, å¤±è´¥ {failed_count}, éš”ç¦» {quarantined_count}")
    
    # æ•°æ®è´¨é‡æŒ‡æ ‡
    total = len(results)
    success_rate = validated_count / total if total > 0 else 0
    
    # è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡
    context.log_event(
        AssetMaterialization(
            asset_key=["quality_metrics", "crawl_validation"],
            description=f"çˆ¬å–æ•°æ®è´¨é‡æ£€æŸ¥: é€šè¿‡ç‡ {success_rate:.2%}",
            metadata={
                "total": MetadataValue.int(total),
                "validated": MetadataValue.int(validated_count),
                "failed": MetadataValue.int(failed_count),
                "quarantined": MetadataValue.int(quarantined_count),
                "success_rate": MetadataValue.float(success_rate),
            }
        )
    )
    
    return {
        "validated": True,
        "total": total,
        "passed": validated_count,
        "failed": failed_count,
        "quarantined": quarantined_count,
        "success_rate": success_rate,
        "passed_results": passed_results[:10],  # æœ€å¤šè¿”å›10ä¸ªé€šè¿‡çš„ä»»åŠ¡
        "failed_results": failed_results[:10]   # æœ€å¤šè¿”å›10ä¸ªå¤±è´¥çš„ä»»åŠ¡
    }


# ==================== Dagster Jobs ====================

@job
def crawl_a_share_reports_job():
    """
    Aè‚¡å®šæœŸæŠ¥å‘Šçˆ¬å–ä½œä¸š
    
    å®Œæ•´æµç¨‹ï¼š
    1. çˆ¬å–å­£åº¦æŠ¥å‘Š/å¹´æŠ¥
    2. éªŒè¯çˆ¬å–ç»“æœ
    """
    crawl_results = crawl_a_share_reports_op()
    validate_crawl_results_op(crawl_results)


@job
def crawl_a_share_ipo_job():
    """
    Aè‚¡IPOæ‹›è‚¡è¯´æ˜ä¹¦çˆ¬å–ä½œä¸š
    
    å®Œæ•´æµç¨‹ï¼š
    1. çˆ¬å–IPOæ‹›è‚¡è¯´æ˜ä¹¦
    2. éªŒè¯çˆ¬å–ç»“æœ
    """
    crawl_results = crawl_a_share_ipo_op()
    validate_crawl_results_op(crawl_results)


# ==================== Schedules ====================

@schedule(
    job=crawl_a_share_reports_job,
    cron_schedule="0 2 * * *",  # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨
)
def daily_crawl_reports_schedule(context):
    """
    æ¯æ—¥å®šæ—¶çˆ¬å–Aè‚¡æŠ¥å‘Š
    """
    return RunRequest()


@schedule(
    job=crawl_a_share_ipo_job,
    cron_schedule="0 3 * * *",  # æ¯å¤©å‡Œæ™¨3ç‚¹æ‰§è¡Œ
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢
)
def daily_crawl_ipo_schedule(context):
    """
    æ¯æ—¥å®šæ—¶çˆ¬å–IPOæ‹›è‚¡è¯´æ˜ä¹¦
    """
    return RunRequest()


# ==================== Sensors ====================

@sensor(
    job=crawl_a_share_reports_job,
    default_status=DefaultSensorStatus.STOPPED,
)
def manual_trigger_reports_sensor(context):
    """
    æ‰‹åŠ¨è§¦å‘çˆ¬å–æŠ¥å‘Šä¼ æ„Ÿå™¨
    å¯ä»¥é€šè¿‡Dagster UIæ‰‹åŠ¨è§¦å‘
    """
    return RunRequest()


@sensor(
    job=crawl_a_share_ipo_job,
    default_status=DefaultSensorStatus.STOPPED,
)
def manual_trigger_ipo_sensor(context):
    """
    æ‰‹åŠ¨è§¦å‘çˆ¬å–IPOä¼ æ„Ÿå™¨
    å¯ä»¥é€šè¿‡Dagster UIæ‰‹åŠ¨è§¦å‘
    """
    return RunRequest()
