# -*- coding: utf-8 -*-
"""
Dagster PDF è§£æä½œä¸šå®šä¹‰
è‡ªåŠ¨æ‰«æå¾…è§£æçš„æ–‡æ¡£å¹¶è°ƒç”¨ MinerU è§£æå™¨

æŒ‰ç…§ plan.md è®¾è®¡ï¼š
- å¤„ç†å±‚ï¼ˆProcessing Layerï¼‰â†’ Dagster è°ƒåº¦
- PDF è§£æ â†’ Silver å±‚ï¼ˆtext_cleanedï¼‰
"""

import os
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

from dagster import (
    job,
    op,
    schedule,
    sensor,
    DefaultSensorStatus,
    DefaultScheduleStatus,
    RunRequest,
    Field,
    get_dagster_logger,
    asset,
    AssetMaterialization,
    MetadataValue,
)

# å¯¼å…¥è§£æå™¨å’Œæ•°æ®åº“æ¨¡å—
from src.processing.ai.pdf_parser import get_mineru_parser
from src.storage.metadata.postgres_client import get_postgres_client
from src.storage.metadata import crud
from src.storage.metadata.models import Document, ParseTask
from src.common.constants import DocumentStatus, DocType, Market
from src.common.config import common_config

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(common_config.PROJECT_ROOT)


# ==================== é…ç½® Schema ====================

PARSE_CONFIG_SCHEMA = {
    "batch_size": Field(
        int,
        default_value=50,
        description="æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆ1-50ï¼‰"
    ),
    "parser_type": Field(
        str,
        default_value="mineru",
        description="è§£æå™¨ç±»å‹ï¼šmineru æˆ– docling"
    ),
    "market": Field(
        str,
        is_required=False,
        description="å¸‚åœºè¿‡æ»¤ï¼ˆa_share/hk_stock/us_stockï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰å¸‚åœº"
    ),
    "doc_type": Field(
        str,
        is_required=False,
        description="æ–‡æ¡£ç±»å‹è¿‡æ»¤ï¼ˆquarterly_report/annual_report/ipo_prospectusï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰ç±»å‹"
    ),
    "stock_codes": Field(
        list,
        is_required=False,
        description="æŒ‰è‚¡ç¥¨ä»£ç åˆ—è¡¨è¿‡æ»¤ï¼ˆNone = ä¸è¿‡æ»¤ï¼ŒæŒ‡å®šåå°†åªå¤„ç†è¿™äº›è‚¡ç¥¨ä»£ç çš„æ–‡æ¡£ï¼‰ã€‚ä¾‹å¦‚: ['000001', '000002']"
    ),
    "limit": Field(
        int,
        is_required=False,
        description="æœ¬æ¬¡ä½œä¸šæœ€å¤šå¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ŒNone æˆ–ä¸è®¾ç½®è¡¨ç¤ºå¤„ç†å…¨éƒ¨"
    ),
    "force_reparse": Field(
        bool,
        default_value=False,
        description="æ˜¯å¦å¼ºåˆ¶é‡æ–°è§£æå·²è§£æçš„æ–‡æ¡£ï¼ˆé»˜è®¤ Falseï¼Œåªè§£ææœªè§£æçš„æ–‡æ¡£ï¼‰"
    ),
    "enable_silver_upload": Field(
        bool,
        default_value=True,
        description="æ˜¯å¦ä¸Šä¼ è§£æç»“æœåˆ° Silver å±‚"
    ),
    "start_page_id": Field(
        int,
        default_value=0,
        description="èµ·å§‹é¡µç ï¼ˆä»0å¼€å§‹ï¼‰ï¼Œé»˜è®¤0ï¼ˆè§£æå…¨éƒ¨ï¼‰"
    ),
    "end_page_id": Field(
        int,
        is_required=False,
        description="ç»“æŸé¡µç ï¼ˆä»0å¼€å§‹ï¼‰ï¼ŒNone è¡¨ç¤ºè§£æåˆ°æœ€åã€‚ä¾‹å¦‚ï¼šè®¾ç½®ä¸º 2 è¡¨ç¤ºåªè§£æå‰3é¡µï¼ˆ0, 1, 2ï¼‰"
    ),
}


# ==================== Dagster Ops ====================

@op(config_schema=PARSE_CONFIG_SCHEMA)
def scan_pending_documents_op(context) -> Dict:
    """
    æ‰«æå¾…è§£æçš„æ–‡æ¡£

    æŸ¥æ‰¾çŠ¶æ€ä¸º 'crawled' çš„æ–‡æ¡£ï¼Œå‡†å¤‡è¿›è¡Œ PDF è§£æ
    æ”¯æŒ force_reparse å¼ºåˆ¶é‡æ–°è§£æå·²è§£æçš„æ–‡æ¡£

    Returns:
        åŒ…å«å¾…è§£ææ–‡æ¡£åˆ—è¡¨çš„å­—å…¸
    """
    config = context.op_config
    logger = get_dagster_logger()

    batch_size = config.get("batch_size", 10)
    limit = config.get("limit")
    market_filter = config.get("market")
    doc_type_filter = config.get("doc_type")
    stock_codes_filter = config.get("stock_codes")
    force_reparse = config.get("force_reparse", False)

    logger.info(f"å¼€å§‹æ‰«æå¾…è§£ææ–‡æ¡£...")
    logger.info(f"é…ç½®: batch_size={batch_size}, limit={limit}, market={market_filter}, doc_type={doc_type_filter}, stock_codes={stock_codes_filter}, force_reparse={force_reparse}")

    pg_client = get_postgres_client()

    try:
        with pg_client.get_session() as session:
            # æ ¹æ® force_reparse å†³å®šæŸ¥è¯¢å“ªäº›çŠ¶æ€çš„æ–‡æ¡£
            # æ³¨æ„ï¼šè¿™é‡Œä¸åº”ç”¨ limitï¼Œå…ˆè·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£
            if force_reparse:
                # å¼ºåˆ¶é‡æ–°è§£æï¼šæŸ¥è¯¢ 'crawled' å’Œ 'parsed' çŠ¶æ€çš„æ–‡æ¡£
                logger.info("å¼ºåˆ¶é‡æ–°è§£ææ¨¡å¼ï¼šå°†è§£æ 'crawled' å’Œ 'parsed' çŠ¶æ€çš„æ–‡æ¡£")
                documents_crawled = crud.get_documents_by_status(
                    session=session,
                    status=DocumentStatus.CRAWLED.value,
                    limit=None,  # ä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰
                    offset=0
                )
                documents_parsed = crud.get_documents_by_status(
                    session=session,
                    status=DocumentStatus.PARSED.value,
                    limit=None,  # ä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰
                    offset=0
                )
                documents = documents_crawled + documents_parsed
                # å»é‡ï¼ˆå¦‚æœæœ‰é‡å¤ï¼‰
                seen_ids = set()
                unique_documents = []
                for doc in documents:
                    if doc.id not in seen_ids:
                        seen_ids.add(doc.id)
                        unique_documents.append(doc)
                documents = unique_documents
            else:
                # æ­£å¸¸æ¨¡å¼ï¼šåªæŸ¥æ‰¾çŠ¶æ€ä¸º 'crawled' çš„æ–‡æ¡£
                documents = crud.get_documents_by_status(
                    session=session,
                    status=DocumentStatus.CRAWLED.value,
                    limit=None,  # ä¸é™åˆ¶ï¼Œè·å–æ‰€æœ‰
                    offset=0
                )

            logger.info(f"æŸ¥è¯¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£ï¼ˆåº”ç”¨è¿‡æ»¤å‰ï¼‰")

            # åº”ç”¨å¸‚åœºè¿‡æ»¤
            if market_filter:
                documents = [d for d in documents if d.market == market_filter]
                logger.info(f"å¸‚åœºè¿‡æ»¤å: {len(documents)} ä¸ªæ–‡æ¡£")

            # åº”ç”¨æ–‡æ¡£ç±»å‹è¿‡æ»¤
            if doc_type_filter:
                documents = [d for d in documents if d.doc_type == doc_type_filter]
                logger.info(f"æ–‡æ¡£ç±»å‹è¿‡æ»¤å: {len(documents)} ä¸ªæ–‡æ¡£")

                logger.info(f"è¡Œä¸šè¿‡æ»¤å: {len(documents)} ä¸ªæ–‡æ¡£")

            # åº”ç”¨è‚¡ç¥¨ä»£ç è¿‡æ»¤
            if stock_codes_filter:
                logger.info(f"æŒ‰è‚¡ç¥¨ä»£ç è¿‡æ»¤: {stock_codes_filter}")
                documents = [d for d in documents if d.stock_code in stock_codes_filter]
                logger.info(f"è‚¡ç¥¨ä»£ç è¿‡æ»¤å: {len(documents)} ä¸ªæ–‡æ¡£")

            # åº”ç”¨ limitï¼ˆæœ€ååº”ç”¨ï¼‰
            if limit and len(documents) > limit:
                logger.info(f"åº”ç”¨ limit={limit}ï¼Œä» {len(documents)} ä¸ªæ–‡æ¡£ä¸­æˆªå–å‰ {limit} ä¸ª")
                documents = documents[:limit]
            
            # åªå¤„ç† PDF æ–‡æ¡£
            pdf_documents = [
                d for d in documents
                if d.minio_object_path and d.minio_object_path.endswith('.pdf')
            ]
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦åœ¨ MinIO ä¸­å­˜åœ¨ï¼ˆé¿å…è§£æä¸å­˜åœ¨çš„æ–‡ä»¶ï¼‰
            from src.storage.object_store.minio_client import MinIOClient
            minio_client = MinIOClient()
            existing_pdf_documents = []
            for doc in pdf_documents:
                if minio_client.file_exists(doc.minio_object_path):
                    existing_pdf_documents.append(doc)
                else:
                    logger.warning(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: document_id={doc.id}, path={doc.minio_object_path}")
            
            pdf_documents = existing_pdf_documents
            logger.info(f"æ‰¾åˆ° {len(pdf_documents)} ä¸ªå¾…è§£æçš„ PDF æ–‡æ¡£ï¼ˆå·²è¿‡æ»¤ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼‰")
            
            # å°†æ–‡æ¡£åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆä¾¿äº Dagster ä¼ é€’ï¼‰
            document_list = []
            for doc in pdf_documents:
                document_list.append({
                    "document_id": doc.id,
                    "stock_code": doc.stock_code,
                    "company_name": doc.company_name,
                    "market": doc.market,
                    "doc_type": doc.doc_type,
                    "year": doc.year,
                    "quarter": doc.quarter,
                    "minio_object_path": doc.minio_object_path,
                    "file_size": doc.file_size,
                    "file_hash": doc.file_hash,
                })
            
            # æŒ‰æ‰¹æ¬¡åˆ†ç»„
            batches = []
            for i in range(0, len(document_list), batch_size):
                batch = document_list[i:i + batch_size]
                batches.append(batch)
            
            logger.info(f"åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} ä¸ªæ–‡æ¡£")

            return {
                "success": True,
                "total_documents": len(document_list),
                "total_batches": len(batches),
                "batches": batches,
                "documents": document_list,  # ä¿ç•™å®Œæ•´åˆ—è¡¨ç”¨äºåç»­å¤„ç†
                "force_reparse": force_reparse,  # ä¼ é€’ force_reparse é…ç½®
            }
            
    except Exception as e:
        logger.error(f"æ‰«æå¾…è§£ææ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error_message": str(e),
            "total_documents": 0,
            "total_batches": 0,
            "batches": [],
            "documents": [],
        }


@op(
    config_schema={
        "enable_silver_upload": Field(
            bool,
            default_value=True,
            description="æ˜¯å¦ä¸Šä¼ è§£æç»“æœåˆ° Silver å±‚"
        ),
        "start_page_id": Field(
            int,
            default_value=0,
            description="èµ·å§‹é¡µç ï¼ˆä»0å¼€å§‹ï¼‰ï¼Œé»˜è®¤0"
        ),
        "end_page_id": Field(
            int,
            is_required=False,
            description="ç»“æŸé¡µç ï¼ˆä»0å¼€å§‹ï¼‰ï¼ŒNone è¡¨ç¤ºè§£æåˆ°æœ€åã€‚ä¾‹å¦‚ï¼šè®¾ç½®ä¸º 4 è¡¨ç¤ºè§£æå‰5é¡µï¼ˆ0-4ï¼‰"
        ),
    }
)
def parse_documents_op(context, scan_result: Dict) -> Dict:
    """
    æ‰¹é‡è§£ææ–‡æ¡£
    
    å¯¹æ‰«æåˆ°çš„æ–‡æ¡£è¿›è¡Œ PDF è§£æï¼Œå¹¶ä¿å­˜åˆ° Silver å±‚
    
    Args:
        scan_result: scan_pending_documents_op çš„è¿”å›ç»“æœ
        
    Returns:
        è§£æç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ scan_result æ˜¯å¦ä¸º None
    if scan_result is None:
        logger.error("scan_result ä¸º Noneï¼Œæ— æ³•ç»§ç»­è§£æ")
        return {
            "success": False,
            "error_message": "scan_result ä¸º None",
            "parsed_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    # å®‰å…¨è·å– config
    config = context.op_config if hasattr(context, 'op_config') else {}

    # è°ƒè¯•ï¼šæ‰“å°é…ç½®ä¿¡æ¯
    logger.info(f"ğŸ” DEBUG: config = {config}")
    logger.info(f"ğŸ” DEBUG: hasattr(context, 'op_config') = {hasattr(context, 'op_config')}")

    enable_silver_upload = config.get("enable_silver_upload", True) if config else True
    start_page_id = config.get("start_page_id", 0) if config else 0
    end_page_id = config.get("end_page_id") if config else None

    # ä» scan_result ä¸­è·å– force_reparse é…ç½®
    force_reparse = scan_result.get("force_reparse", False)

    logger.info(f"ğŸ” DEBUG: enable_silver_upload={enable_silver_upload}, start_page_id={start_page_id}, end_page_id={end_page_id}, force_reparse={force_reparse}")

    if end_page_id is not None:
        logger.info(f"ğŸ“„ é¡µé¢èŒƒå›´: {start_page_id} - {end_page_id} (å…± {end_page_id - start_page_id + 1} é¡µ)")
    else:
        logger.info(f"ğŸ“„ é¡µé¢èŒƒå›´: {start_page_id} - æœ€å (è§£æå…¨éƒ¨)")

    if force_reparse:
        logger.info(f"âš ï¸  å¼ºåˆ¶é‡æ–°è§£ææ¨¡å¼ï¼šå°†é‡æ–°è§£æå·²è§£æçš„æ–‡æ¡£")
    
    if not scan_result.get("success"):
        logger.error(f"æ‰«æå¤±è´¥ï¼Œè·³è¿‡è§£æ: {scan_result.get('error_message')}")
        return {
            "success": False,
            "error_message": "æ‰«æå¤±è´¥",
            "parsed_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    documents = scan_result.get("documents", [])
    if not documents:
        logger.info("æ²¡æœ‰å¾…è§£æçš„æ–‡æ¡£")
        return {
            "success": True,
            "parsed_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    logger.info(f"ğŸš€ å¼€å§‹è§£æ: å…± {len(documents)} ä¸ªæ–‡æ¡£")

    # åˆå§‹åŒ–è§£æå™¨
    parser = get_mineru_parser()

    pg_client = get_postgres_client()

    parsed_count = 0
    failed_count = 0
    skipped_count = 0
    failed_documents = []

    for idx, doc_info in enumerate(documents):
        document_id = doc_info["document_id"]
        stock_code = doc_info["stock_code"]
        company_name = doc_info.get("company_name", "")
        minio_path = doc_info["minio_object_path"]

        # æ˜¾ç¤ºè¿›åº¦ï¼šæ¯10ä¸ªæˆ–æ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€ä¸ª
        total = len(documents)
        if (idx + 1) % 10 == 0 or (idx + 1) % max(1, total // 10) == 0 or (idx + 1) == total:
            progress_pct = (idx + 1) / total * 100
            logger.info(f"ğŸ“¦ [{idx+1}/{total}] {progress_pct:.1f}% | è§£æ: {stock_code} - {company_name}")
        
        try:
            # æ³¨æ„ï¼šparse_document æ–¹æ³•å†…éƒ¨ä¼šé‡æ–°æŸ¥è¯¢æ•°æ®åº“è·å– Document å¯¹è±¡
            # æ‰€ä»¥è¿™é‡Œç›´æ¥ä¼ é€’ document_id å³å¯ï¼Œä¸éœ€è¦ä¼ é€’ Document å¯¹è±¡
            result = parser.parse_document(
                document_id=document_id,
                save_to_silver=enable_silver_upload,
                start_page_id=start_page_id,
                end_page_id=end_page_id,
                force_reparse=force_reparse  # ä¼ é€’å¼ºåˆ¶é‡æ–°è§£ææ ‡å¿—
            )
            
            # æ£€æŸ¥ result æ˜¯å¦ä¸º None
            if result is None:
                failed_count += 1
                error_msg = "è§£æå™¨è¿”å› None"
                logger.error(f"âŒ è§£æå¤±è´¥: document_id={document_id}, error={error_msg}")
                failed_documents.append({
                    "document_id": document_id,
                    "stock_code": stock_code,
                    "error": error_msg
                })
                continue
            
            if result.get("success"):
                parsed_count += 1
                output_path = result.get("output_path", "N/A")
                text_length = result.get("extracted_text_length", 0)
                page_count = result.get("page_count", 0)
                logger.info(
                    f"âœ… è§£ææˆåŠŸ: document_id={document_id}, "
                    f"output_path={output_path}, "
                    f"text_length={text_length}"
                )
                
                # è®°å½•èµ„äº§ç‰©åŒ–ï¼ˆDagster æ•°æ®è¡€ç¼˜ï¼‰
                try:
                    market = doc_info.get("market", "")
                    doc_type = doc_info.get("doc_type", "")
                    year = doc_info.get("year")
                    quarter = doc_info.get("quarter")
                    
                    # æ„å»ºèµ„äº§key: ["silver", "parsed_documents", market, doc_type, stock_code, year, quarter]
                    asset_key = ["silver", "parsed_documents", market, doc_type, stock_code]
                    if year:
                        asset_key.append(str(year))
                    if quarter:
                        asset_key.append(f"Q{quarter}")
                    
                    # æ„å»ºçˆ¶èµ„äº§keyï¼ˆæŒ‡å‘bronzeå±‚ï¼‰
                    parent_asset_key = ["bronze", market, doc_type]
                    if year:
                        parent_asset_key.append(str(year))
                    if quarter:
                        parent_asset_key.append(f"Q{quarter}")
                    
                    context.log_event(
                        AssetMaterialization(
                            asset_key=asset_key,
                            description=f"{company_name} {year or ''} Q{quarter or ''} è§£æå®Œæˆ",
                            metadata={
                                "document_id": MetadataValue.text(str(document_id)),
                                "stock_code": MetadataValue.text(stock_code),
                                "company_name": MetadataValue.text(company_name or ""),
                                "market": MetadataValue.text(market),
                                "doc_type": MetadataValue.text(doc_type),
                                "year": MetadataValue.int(year) if year else MetadataValue.int(0),
                                "quarter": MetadataValue.int(quarter) if quarter else MetadataValue.int(0),
                                "text_length": MetadataValue.int(text_length),
                                "page_count": MetadataValue.int(page_count),
                                "silver_path": MetadataValue.text(output_path),
                                "parsed_at": MetadataValue.text(datetime.now().isoformat()),
                                "parent_asset_key": MetadataValue.text("/".join(parent_asset_key)),
                            }
                        )
                    )
                except Exception as e:
                    logger.warning(f"è®°å½• AssetMaterialization äº‹ä»¶å¤±è´¥ (document_id={document_id}): {e}")
            else:
                failed_count += 1
                error_msg = result.get("error_message", "æœªçŸ¥é”™è¯¯")
                logger.error(f"âŒ è§£æå¤±è´¥: document_id={document_id}, error={error_msg}")
                failed_documents.append({
                    "document_id": document_id,
                    "stock_code": stock_code,
                    "error": error_msg
                })
                

        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Dagster ä¸­æ–­å¼‚å¸¸ï¼ˆDagsterExecutionInterruptedError ç­‰ï¼‰
            error_type = type(e).__name__
            if "Interrupt" in error_type or "Interrupted" in error_type:
                logger.warning(f"âš ï¸ è§£æè¢«ä¸­æ–­: document_id={document_id}, error_type={error_type}")
                # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®© Dagster çŸ¥é“ä»»åŠ¡è¢«ä¸­æ–­
                raise
            
            failed_count += 1
            error_msg = str(e)
            logger.error(f"âŒ è§£æå¼‚å¸¸: document_id={document_id}, error={error_msg}", exc_info=True)
            failed_documents.append({
                "document_id": document_id,
                "stock_code": stock_code,
                "error": error_msg
            })
    
    logger.info(
        f"è§£æå®Œæˆ: æˆåŠŸ={parsed_count}, å¤±è´¥={failed_count}, è·³è¿‡={skipped_count}"
    )
    
    return {
        "success": True,
        "parsed_count": parsed_count,
        "failed_count": failed_count,
        "skipped_count": skipped_count,
        "total_documents": len(documents),
        "failed_documents": failed_documents[:10],  # æœ€å¤šè¿”å›10ä¸ªå¤±è´¥è®°å½•
    }


@op
def validate_parse_results_op(context, parse_results: Dict) -> Dict:
    """
    éªŒè¯è§£æç»“æœ
    
    æ£€æŸ¥è§£æç»“æœçš„è´¨é‡ï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        parse_results: parse_documents_op çš„è¿”å›ç»“æœ
        
    Returns:
        éªŒè¯ç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ parse_results æ˜¯å¦ä¸º Noneï¼ˆå¯èƒ½æ˜¯ä¸Šæ¸¸è¢«ä¸­æ–­ï¼‰
    if parse_results is None:
        logger.warning("parse_results ä¸º Noneï¼Œå¯èƒ½æ˜¯ä¸Šæ¸¸æ­¥éª¤è¢«ä¸­æ–­ï¼Œè·³è¿‡éªŒè¯")
        return {
            "success": False,
            "validation_passed": False,
            "error_message": "parse_results ä¸º Noneï¼ˆå¯èƒ½è¢«ä¸­æ–­ï¼‰",
        }
    
    if not parse_results.get("success"):
        logger.warning("è§£æä½œä¸šå¤±è´¥ï¼Œè·³è¿‡éªŒè¯")
        return {
            "success": False,
            "validation_passed": False,
        }
    
    parsed_count = parse_results.get("parsed_count", 0)
    failed_count = parse_results.get("failed_count", 0)
    total_documents = parse_results.get("total_documents", 0)
    
    # è®¡ç®—æˆåŠŸç‡
    success_rate = parsed_count / total_documents if total_documents > 0 else 0
    
    logger.info(f"è§£æç»“æœéªŒè¯:")
    logger.info(f"  æ€»æ–‡æ¡£æ•°: {total_documents}")
    logger.info(f"  æˆåŠŸè§£æ: {parsed_count}")
    logger.info(f"  è§£æå¤±è´¥: {failed_count}")
    logger.info(f"  æˆåŠŸç‡: {success_rate:.2%}")
    
    # éªŒè¯è§„åˆ™ï¼šæˆåŠŸç‡ >= 80% è®¤ä¸ºé€šè¿‡
    validation_passed = success_rate >= 0.8 if total_documents > 0 else True
    
    if not validation_passed:
        logger.warning(f"âš ï¸ è§£ææˆåŠŸç‡ {success_rate:.2%} ä½äºé˜ˆå€¼ 80%")
    
    # è®°å½•å¤±è´¥æ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰
    failed_documents = parse_results.get("failed_documents", [])
    if failed_documents:
        logger.warning(f"å¤±è´¥æ–‡æ¡£åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰:")
        for failed in failed_documents:
            logger.warning(f"  - document_id={failed['document_id']}, "
                         f"stock_code={failed['stock_code']}, "
                         f"error={failed['error']}")
    
    # è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡
    try:
        context.log_event(
            AssetMaterialization(
                asset_key=["quality_metrics", "parse_validation"],
                description=f"è§£ææ•°æ®è´¨é‡æ£€æŸ¥: é€šè¿‡ç‡ {success_rate:.2%}",
                metadata={
                    "total": MetadataValue.int(total_documents),
                    "parsed": MetadataValue.int(parsed_count),
                    "failed": MetadataValue.int(failed_count),
                    "success_rate": MetadataValue.float(success_rate),
                    "validation_passed": MetadataValue.bool(validation_passed),
                }
            )
        )
    except Exception as e:
        logger.warning(f"è®°å½•è´¨é‡æŒ‡æ ‡ AssetMaterialization äº‹ä»¶å¤±è´¥: {e}")
    
    return {
        "success": True,
        "validation_passed": validation_passed,
        "total_documents": total_documents,
        "parsed_count": parsed_count,
        "failed_count": failed_count,
        "success_rate": success_rate,
        "failed_documents_count": len(failed_documents),
    }


# ==================== Dagster Jobs ====================

@job(
    config={
        "ops": {
            "scan_pending_documents_op": {
                "config": {
                    "batch_size": 50,
                    # limit ä¸è®¾ç½®è¡¨ç¤ºå¤„ç†å…¨éƒ¨ï¼Œdoc_type æ˜¯å¯é€‰çš„ï¼Œä¸è®¾ç½®è¡¨ç¤ºæ‰€æœ‰ç±»å‹
                }
            },
            "parse_documents_op": {
                "config": {
                    "enable_silver_upload": True,
                    "start_page_id": 0,
                }
            }
        }
    },
    description="PDF è§£æä½œä¸š"
)
def parse_pdf_job():
    """
    PDF è§£æä½œä¸šï¼ˆè§£æå…¨éƒ¨é¡µé¢ï¼‰

    å®Œæ•´æµç¨‹ï¼š
    1. æ‰«æå¾…è§£ææ–‡æ¡£ï¼ˆçŠ¶æ€ä¸º 'crawled'ï¼‰
    2. æ‰¹é‡è§£ææ–‡æ¡£ï¼ˆè°ƒç”¨ MinerU è§£æå™¨ï¼‰
    3. éªŒè¯è§£æç»“æœ

    é»˜è®¤é…ç½®ï¼š
    - scan_pending_documents_op:
        - batch_size: 2 (å¹¶å‘è§£æ2ä¸ªæ–‡æ¡£)
        - limit: 10 (æœ€å¤šå¤„ç†10ä¸ªæ–‡æ¡£)
    - parse_documents_op:
        - start_page_id: 0
        - enable_silver_upload: True

    å¦‚éœ€æŒ‡å®šé¡µé¢èŒƒå›´ï¼Œè¯·åœ¨ Launchpad ä¸­é…ç½® end_page_idï¼š
    ops:
      parse_documents_op:
        config:
          enable_silver_upload: true
          start_page_id: 0
          end_page_id: 4  # ä¾‹å¦‚ï¼šåªè§£æå‰5é¡µï¼ˆ0-4ï¼‰
    """
    scan_result = scan_pending_documents_op()
    parse_results = parse_documents_op(scan_result)
    validate_parse_results_op(parse_results)


@job(
    config={
        "ops": {
            "scan_pending_documents_op": {
                "config": {
                    "batch_size": 50,
                    # limit ä¸è®¾ç½®è¡¨ç¤ºå¤„ç†å…¨éƒ¨ï¼Œdoc_type æ˜¯å¯é€‰çš„ï¼Œä¸è®¾ç½®è¡¨ç¤ºæ‰€æœ‰ç±»å‹
                }
            },
            "parse_documents_op": {
                "config": {
                    "enable_silver_upload": True,
                    "start_page_id": 0,
                    # ä¸è®¾ç½® end_page_idï¼Œè§£æå®Œæ•´æ–‡æ¡£
                }
            }
        }
    },
    description="PDF è§£æä½œä¸š - è§£æå®Œæ•´æ–‡æ¡£ï¼ˆæ‰€æœ‰é¡µï¼‰"
)
def parse_pdf_full_job():
    """
    PDF è§£æä½œä¸šï¼ˆè§£æå®Œæ•´æ–‡æ¡£ï¼‰

    å®Œæ•´æµç¨‹ï¼š
    1. æ‰«æå¾…è§£ææ–‡æ¡£ï¼ˆçŠ¶æ€ä¸º 'crawled'ï¼‰
    2. æ‰¹é‡è§£ææ–‡æ¡£ï¼ˆè°ƒç”¨ MinerU è§£æå™¨ï¼‰
    3. éªŒè¯è§£æç»“æœ

    é»˜è®¤é…ç½®ï¼š
    - scan_pending_documents_op:
        - batch_size: 5 (å¹¶å‘è§£æ5ä¸ªæ–‡æ¡£)
        - limit: 100 (æœ€å¤šå¤„ç†100ä¸ªæ–‡æ¡£)
    - parse_documents_op:
        - start_page_id: 0
        - end_page_id: None (è§£ææ‰€æœ‰é¡µé¢)

    âš ï¸ æ³¨æ„ï¼šè§£æå®Œæ•´æ–‡æ¡£å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
    """
    scan_result = scan_pending_documents_op()
    parse_results = parse_documents_op(scan_result)
    validate_parse_results_op(parse_results)


# ==================== Schedules ====================

@schedule(
    job=parse_pdf_job,
    cron_schedule="0 */2 * * *",  # æ¯2å°æ—¶æ‰§è¡Œä¸€æ¬¡
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨
)
def hourly_parse_schedule(context):
    """
    æ¯å°æ—¶å®šæ—¶è§£æä½œä¸š
    """
    return RunRequest()


@schedule(
    job=parse_pdf_job,
    cron_schedule="0 4 * * *",  # æ¯å¤©å‡Œæ™¨4ç‚¹æ‰§è¡Œï¼ˆçˆ¬å–å®Œæˆåï¼‰
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢
)
def daily_parse_schedule(context):
    """
    æ¯æ—¥å®šæ—¶è§£æä½œä¸šï¼ˆåœ¨çˆ¬å–ä½œä¸šä¹‹åæ‰§è¡Œï¼‰
    """
    return RunRequest()


# ==================== Sensors ====================

@sensor(
    job=parse_pdf_job,
    default_status=DefaultSensorStatus.STOPPED,
)
def manual_trigger_parse_sensor(context):
    """
    æ‰‹åŠ¨è§¦å‘è§£æä¼ æ„Ÿå™¨
    å¯ä»¥é€šè¿‡ Dagster UI æ‰‹åŠ¨è§¦å‘
    """
    return RunRequest()
