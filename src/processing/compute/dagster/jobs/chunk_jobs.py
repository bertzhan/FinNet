# -*- coding: utf-8 -*-
"""
Dagster æ–‡æœ¬åˆ†å—ä½œä¸šå®šä¹‰
è‡ªåŠ¨æ‰«æå·²è§£æçš„æ–‡æ¡£å¹¶æ‰§è¡Œæ–‡æœ¬åˆ†å—

æŒ‰ç…§ plan.md è®¾è®¡ï¼š
- å¤„ç†å±‚ï¼ˆProcessing Layerï¼‰â†’ Dagster è°ƒåº¦
- æ–‡æœ¬åˆ†å— â†’ Silver å±‚ï¼ˆstructure.json, chunks.jsonï¼‰
"""

import os
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

from dagster import (
    job,
    op,
    schedule,
    sensor,
    DefaultSensorStatus,
    DefaultScheduleStatus,
    RunRequest,
    Field,
    get_dagster_logger,
)

# å¯¼å…¥åˆ†å—æœåŠ¡å’Œæ•°æ®åº“æ¨¡å—
from src.processing.text import get_text_chunker
from src.storage.metadata.postgres_client import get_postgres_client
from src.storage.metadata import crud
from src.storage.metadata.models import Document, ParsedDocument
from src.common.constants import DocType, Market
from src.common.config import common_config

# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(common_config.PROJECT_ROOT)


# ==================== é…ç½® Schema ====================

CHUNK_CONFIG_SCHEMA = {
    "batch_size": Field(
        int,
        default_value=10,
        description="æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆ1-50ï¼‰"
    ),
    "market": Field(
        str,
        is_required=False,
        description="å¸‚åœºè¿‡æ»¤ï¼ˆa_share/hk_stock/us_stockï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰å¸‚åœº"
    ),
    "doc_type": Field(
        str,
        is_required=False,
        description="æ–‡æ¡£ç±»å‹è¿‡æ»¤ï¼ˆquarterly_report/annual_report/ipo_prospectusï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰ç±»å‹"
    ),
    "stock_codes": Field(
        list,
        is_required=False,
        description="æŒ‰è‚¡ç¥¨ä»£ç åˆ—è¡¨è¿‡æ»¤ï¼ˆNone = ä¸è¿‡æ»¤ï¼ŒæŒ‡å®šåå°†åªå¤„ç†è¿™äº›è‚¡ç¥¨ä»£ç çš„æ–‡æ¡£ï¼‰ã€‚ä¾‹å¦‚: ['000001', '000002']"
    ),
    "limit": Field(
        int,
        default_value=100,
        description="æœ¬æ¬¡ä½œä¸šæœ€å¤šå¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆ1-1000ï¼‰"
    ),
    "force_rechunk": Field(
        bool,
        default_value=False,
        description="æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†å—ï¼ˆåˆ é™¤æ—§åˆ†å—ï¼‰"
    ),
}


# ==================== Dagster Ops ====================

@op(config_schema=CHUNK_CONFIG_SCHEMA)
def scan_parsed_documents_op(context) -> Dict:
    """
    æ‰«æå·²è§£æä½†æœªåˆ†å—çš„æ–‡æ¡£
    
    æŸ¥æ‰¾çŠ¶æ€ä¸º 'parsed' ä¸”æœ‰ markdown_path ä½†æœªåˆ†å—çš„æ–‡æ¡£
    
    Returns:
        åŒ…å«å¾…åˆ†å—æ–‡æ¡£åˆ—è¡¨çš„å­—å…¸
    """
    config = context.op_config
    logger = get_dagster_logger()
    
    batch_size = config.get("batch_size", 10)
    limit = config.get("limit", 100)
    market_filter = config.get("market")
    doc_type_filter = config.get("doc_type")
    stock_codes_filter = config.get("stock_codes")

    logger.info(f"å¼€å§‹æ‰«æå¾…åˆ†å—æ–‡æ¡£...")
    force_rechunk = config.get("force_rechunk", False)
    logger.info(f"é…ç½®: batch_size={batch_size}, limit={limit}, market={market_filter}, doc_type={doc_type_filter}, stock_codes={stock_codes_filter}, force_rechunk={force_rechunk}")
    
    pg_client = get_postgres_client()
    
    try:
        with pg_client.get_session() as session:
            # æ„å»ºæŸ¥è¯¢
            query = session.query(ParsedDocument).join(
                Document, ParsedDocument.document_id == Document.id
            ).filter(
                ParsedDocument.markdown_path.isnot(None),
                ParsedDocument.markdown_path != ""
            )
            
            # åº”ç”¨å¸‚åœºè¿‡æ»¤
            if market_filter:
                query = query.filter(Document.market == market_filter)
            
            # åº”ç”¨æ–‡æ¡£ç±»å‹è¿‡æ»¤
            if doc_type_filter:
                query = query.filter(Document.doc_type == doc_type_filter)

            # åº”ç”¨è‚¡ç¥¨ä»£ç è¿‡æ»¤
            if stock_codes_filter:
                logger.info(f"æŒ‰è‚¡ç¥¨ä»£ç è¿‡æ»¤: {stock_codes_filter}")
                query = query.filter(Document.stock_code.in_(stock_codes_filter))

            # è¿‡æ»¤æ‰å·²åˆ†å—çš„æ–‡æ¡£ï¼ˆé™¤é force_rechunkï¼‰
            if not force_rechunk:
                query = query.filter(ParsedDocument.chunks_count == 0)
                logger.info("è¿‡æ»¤æ¡ä»¶: åªæ‰«ææœªåˆ†å—çš„æ–‡æ¡£ (chunks_count == 0)")
            else:
                logger.info("âš ï¸ force_rechunk=True: å°†æ‰«ææ‰€æœ‰æ–‡æ¡£ï¼ˆåŒ…æ‹¬å·²åˆ†å—çš„ï¼‰")
            
            # é™åˆ¶æ•°é‡å¹¶æ‰§è¡ŒæŸ¥è¯¢
            parsed_docs = query.limit(limit).all()
            
            logger.info(f"æ‰¾åˆ° {len(parsed_docs)} ä¸ªå¾…åˆ†å—çš„æ–‡æ¡£")
            
            # å°†æ–‡æ¡£åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆä¾¿äº Dagster ä¼ é€’ï¼‰
            document_list = []
            for parsed_doc in parsed_docs:
                doc = session.query(Document).filter(Document.id == parsed_doc.document_id).first()
                if not doc:
                    continue
                
                document_list.append({
                    "document_id": str(doc.id),
                    "parsed_document_id": str(parsed_doc.id),
                    "stock_code": doc.stock_code,
                    "company_name": doc.company_name,
                    "market": doc.market,
                    "doc_type": doc.doc_type,
                    "year": doc.year,
                    "quarter": doc.quarter,
                    "markdown_path": parsed_doc.markdown_path,
                    "chunks_count": parsed_doc.chunks_count or 0,
                })
            
            # æŒ‰æ‰¹æ¬¡åˆ†ç»„
            batches = []
            for i in range(0, len(document_list), batch_size):
                batch = document_list[i:i + batch_size]
                batches.append(batch)
            
            logger.info(f"åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} ä¸ªæ–‡æ¡£")
            
            return {
                "success": True,
                "total_documents": len(document_list),
                "total_batches": len(batches),
                "batches": batches,
                "documents": document_list,
            }
            
    except Exception as e:
        logger.error(f"æ‰«æå¾…åˆ†å—æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error_message": str(e),
            "total_documents": 0,
            "total_batches": 0,
            "batches": [],
            "documents": [],
        }


@op(
    config_schema={
        "force_rechunk": Field(
            bool,
            default_value=False,
            description="æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†å—ï¼ˆåˆ é™¤æ—§åˆ†å—ï¼‰"
        ),
    }
)
def chunk_documents_op(context, scan_result: Dict) -> Dict:
    """
    æ‰¹é‡æ‰§è¡Œåˆ†å—
    
    å¯¹æ‰«æåˆ°çš„æ–‡æ¡£è¿›è¡Œæ–‡æœ¬åˆ†å—ï¼Œå¹¶ä¿å­˜åˆ° Silver å±‚å’Œæ•°æ®åº“
    
    Args:
        scan_result: scan_parsed_documents_op çš„è¿”å›ç»“æœ
        
    Returns:
        åˆ†å—ç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ scan_result æ˜¯å¦ä¸º None
    if scan_result is None:
        logger.error("scan_result ä¸º Noneï¼Œæ— æ³•ç»§ç»­åˆ†å—")
        return {
            "success": False,
            "error_message": "scan_result ä¸º None",
            "chunked_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    # å®‰å…¨è·å– config
    config = context.op_config if hasattr(context, 'op_config') else {}
    force_rechunk = config.get("force_rechunk", False) if config else False
    
    if not scan_result.get("success"):
        logger.error(f"æ‰«æå¤±è´¥ï¼Œè·³è¿‡åˆ†å—: {scan_result.get('error_message')}")
        return {
            "success": False,
            "error_message": "æ‰«æå¤±è´¥",
            "chunked_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    documents = scan_result.get("documents", [])
    if not documents:
        logger.info("æ²¡æœ‰å¾…åˆ†å—çš„æ–‡æ¡£")
        return {
            "success": True,
            "chunked_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    logger.info(f"ğŸš€ å¼€å§‹åˆ†å—: å…± {len(documents)} ä¸ªæ–‡æ¡£")

    # åˆå§‹åŒ–åˆ†å—æœåŠ¡
    chunker = get_text_chunker()

    chunked_count = 0
    failed_count = 0
    skipped_count = 0
    failed_documents = []

    for idx, doc_info in enumerate(documents):
        document_id = doc_info["document_id"]
        stock_code = doc_info["stock_code"]
        company_name = doc_info.get("company_name", "")
        markdown_path = doc_info["markdown_path"]

        # æ˜¾ç¤ºè¿›åº¦ï¼šæ¯10ä¸ªæˆ–æ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€ä¸ª
        total = len(documents)
        if (idx + 1) % 10 == 0 or (idx + 1) % max(1, total // 10) == 0 or (idx + 1) == total:
            progress_pct = (idx + 1) / total * 100
            logger.info(f"ğŸ“¦ [{idx+1}/{total}] {progress_pct:.1f}% | åˆ†å—: {stock_code} - {company_name}")
        
        try:
            result = chunker.chunk_document(
                document_id=document_id,
                force_rechunk=force_rechunk
            )
            
            # æ£€æŸ¥ result æ˜¯å¦ä¸º None
            if result is None:
                failed_count += 1
                error_msg = "åˆ†å—æœåŠ¡è¿”å› None"
                logger.error(f"âŒ åˆ†å—å¤±è´¥: document_id={document_id}, error={error_msg}")
                failed_documents.append({
                    "document_id": document_id,
                    "stock_code": stock_code,
                    "error": error_msg
                })
                continue
            
            if result.get("success"):
                chunked_count += 1
                chunks_count = result.get("chunks_count", 0)
                structure_path = result.get("structure_path", "N/A")
                chunks_path = result.get("chunks_path", "N/A")
                logger.info(
                    f"âœ… åˆ†å—æˆåŠŸ: document_id={document_id}, "
                    f"chunks_count={chunks_count}, "
                    f"structure_path={structure_path}, "
                    f"chunks_path={chunks_path}"
                )
            else:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡ï¼ˆå·²åˆ†å—ï¼‰
                if "å·²åˆ†å—" in result.get("error_message", ""):
                    skipped_count += 1
                    logger.info(f"â­ï¸ è·³è¿‡å·²åˆ†å—æ–‡æ¡£: document_id={document_id}")
                else:
                    failed_count += 1
                    error_msg = result.get("error_message", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ åˆ†å—å¤±è´¥: document_id={document_id}, error={error_msg}")
                    failed_documents.append({
                        "document_id": document_id,
                        "stock_code": stock_code,
                        "error": error_msg
                    })
                
        except Exception as e:
            failed_count += 1
            error_msg = str(e)
            logger.error(f"âŒ åˆ†å—å¼‚å¸¸: document_id={document_id}, error={error_msg}", exc_info=True)
            failed_documents.append({
                "document_id": document_id,
                "stock_code": stock_code,
                "error": error_msg
            })
    
    logger.info(
        f"åˆ†å—å®Œæˆ: æˆåŠŸ={chunked_count}, å¤±è´¥={failed_count}, è·³è¿‡={skipped_count}"
    )
    
    return {
        "success": True,
        "chunked_count": chunked_count,
        "failed_count": failed_count,
        "skipped_count": skipped_count,
        "total_documents": len(documents),
        "failed_documents": failed_documents[:10],  # æœ€å¤šè¿”å›10ä¸ªå¤±è´¥è®°å½•
    }


@op
def validate_chunk_results_op(context, chunk_results: Dict) -> Dict:
    """
    éªŒè¯åˆ†å—ç»“æœ
    
    æ£€æŸ¥åˆ†å—ç»“æœçš„è´¨é‡ï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        chunk_results: chunk_documents_op çš„è¿”å›ç»“æœ
        
    Returns:
        éªŒè¯ç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ chunk_results æ˜¯å¦ä¸º None
    if chunk_results is None:
        logger.error("chunk_results ä¸º Noneï¼Œæ— æ³•éªŒè¯")
        return {
            "success": False,
            "validation_passed": False,
            "error_message": "chunk_results ä¸º None",
        }
    
    if not chunk_results.get("success"):
        logger.warning("åˆ†å—ä½œä¸šå¤±è´¥ï¼Œè·³è¿‡éªŒè¯")
        return {
            "success": False,
            "validation_passed": False,
        }
    
    chunked_count = chunk_results.get("chunked_count", 0)
    failed_count = chunk_results.get("failed_count", 0)
    total_documents = chunk_results.get("total_documents", 0)
    
    # è®¡ç®—æˆåŠŸç‡
    success_rate = chunked_count / total_documents if total_documents > 0 else 0
    
    logger.info(f"åˆ†å—ç»“æœéªŒè¯:")
    logger.info(f"  æ€»æ–‡æ¡£æ•°: {total_documents}")
    logger.info(f"  æˆåŠŸåˆ†å—: {chunked_count}")
    logger.info(f"  åˆ†å—å¤±è´¥: {failed_count}")
    logger.info(f"  æˆåŠŸç‡: {success_rate:.2%}")
    
    # éªŒè¯è§„åˆ™ï¼šæˆåŠŸç‡ >= 80% è®¤ä¸ºé€šè¿‡
    validation_passed = success_rate >= 0.8 if total_documents > 0 else True
    
    if not validation_passed:
        logger.warning(f"âš ï¸ åˆ†å—æˆåŠŸç‡ {success_rate:.2%} ä½äºé˜ˆå€¼ 80%")
    
    # è®°å½•å¤±è´¥æ–‡æ¡£ï¼ˆå¦‚æœæœ‰ï¼‰
    failed_documents = chunk_results.get("failed_documents", [])
    if failed_documents:
        logger.warning(f"å¤±è´¥æ–‡æ¡£åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰:")
        for failed in failed_documents:
            logger.warning(f"  - document_id={failed['document_id']}, "
                         f"stock_code={failed['stock_code']}, "
                         f"error={failed['error']}")
    
    return {
        "success": True,
        "validation_passed": validation_passed,
        "total_documents": total_documents,
        "chunked_count": chunked_count,
        "failed_count": failed_count,
        "success_rate": success_rate,
        "failed_documents_count": len(failed_documents),
    }


# ==================== Dagster Jobs ====================

@job(
    config={
        "ops": {
            "scan_parsed_documents_op": {
                "config": {
                    "batch_size": 5,
                    "limit": 20,
                    # doc_type æ˜¯å¯é€‰çš„ï¼Œä¸è®¾ç½®è¡¨ç¤ºæ‰€æœ‰ç±»å‹
                }
            },
            "chunk_documents_op": {
                "config": {
                    "force_rechunk": False,
                }
            }
        }
    },
    description="æ–‡æœ¬åˆ†å—ä½œä¸š - é»˜è®¤é…ç½®"
)
def chunk_documents_job():
    """
    æ–‡æœ¬åˆ†å—ä½œä¸š

    å®Œæ•´æµç¨‹ï¼š
    1. æ‰«æå·²è§£ææ–‡æ¡£ï¼ˆçŠ¶æ€ä¸º 'parsed'ï¼Œæœ‰ markdown_pathï¼Œæœªåˆ†å—ï¼‰
    2. æ‰¹é‡æ‰§è¡Œåˆ†å—ï¼ˆè°ƒç”¨ TextChunkerï¼‰
    3. éªŒè¯åˆ†å—ç»“æœ

    é»˜è®¤é…ç½®ï¼š
    - scan_parsed_documents_op:
        - batch_size: 5 (æ¯æ‰¹å¤„ç†5ä¸ªæ–‡æ¡£)
        - limit: 20 (æœ€å¤šå¤„ç†20ä¸ªæ–‡æ¡£)
    - chunk_documents_op:
        - force_rechunk: False (ä¸å¼ºåˆ¶é‡æ–°åˆ†å—)
    """
    scan_result = scan_parsed_documents_op()
    chunk_results = chunk_documents_op(scan_result)
    validate_chunk_results_op(chunk_results)


# ==================== Schedules ====================

@schedule(
    job=chunk_documents_job,
    cron_schedule="0 */2 * * *",  # æ¯2å°æ—¶æ‰§è¡Œä¸€æ¬¡
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨
)
def hourly_chunk_schedule(context):
    """
    æ¯å°æ—¶å®šæ—¶åˆ†å—ä½œä¸š
    """
    return RunRequest()


@schedule(
    job=chunk_documents_job,
    cron_schedule="0 5 * * *",  # æ¯å¤©å‡Œæ™¨5ç‚¹æ‰§è¡Œï¼ˆè§£æå®Œæˆåï¼‰
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢
)
def daily_chunk_schedule(context):
    """
    æ¯æ—¥å®šæ—¶åˆ†å—ä½œä¸šï¼ˆåœ¨è§£æä½œä¸šä¹‹åæ‰§è¡Œï¼‰
    """
    return RunRequest()


# ==================== Sensors ====================

@sensor(
    job=chunk_documents_job,
    default_status=DefaultSensorStatus.STOPPED,
)
def manual_trigger_chunk_sensor(context):
    """
    æ‰‹åŠ¨è§¦å‘åˆ†å—ä¼ æ„Ÿå™¨
    å¯ä»¥é€šè¿‡ Dagster UI æ‰‹åŠ¨è§¦å‘
    """
    return RunRequest()
