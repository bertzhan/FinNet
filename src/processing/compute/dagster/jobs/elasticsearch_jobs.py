# -*- coding: utf-8 -*-
"""
Dagster Elasticsearch ç´¢å¼•ä½œä¸šå®šä¹‰
è‡ªåŠ¨æ‰«æå·²åˆ†å—çš„æ–‡æ¡£å¹¶ç´¢å¼•åˆ° Elasticsearch

æŒ‰ç…§ plan.md è®¾è®¡ï¼š
- å¤„ç†å±‚ï¼ˆProcessing Layerï¼‰â†’ Dagster è°ƒåº¦
- å…¨æ–‡æ£€ç´¢ç´¢å¼• â†’ Elasticsearch å­˜å‚¨
"""

from datetime import datetime
from typing import List, Dict, Optional

from dagster import (
    job,
    op,
    schedule,
    sensor,
    DefaultSensorStatus,
    DefaultScheduleStatus,
    RunRequest,
    Field,
    get_dagster_logger,
)

# å¯¼å…¥ Elasticsearch å®¢æˆ·ç«¯å’Œæ•°æ®åº“æ¨¡å—
from src.storage.elasticsearch import get_elasticsearch_client
from src.storage.metadata.postgres_client import get_postgres_client
from src.storage.metadata import crud
from src.storage.metadata.models import Document, DocumentChunk
from src.common.constants import DocType, Market
from src.common.config import common_config


# ==================== é…ç½® Schema ====================

ELASTICSEARCH_CONFIG_SCHEMA = {
    "batch_size": Field(
        int,
        default_value=100,
        description="æ¯æ‰¹ç´¢å¼•çš„åˆ†å—æ•°é‡ï¼ˆ1-1000ï¼‰"
    ),
    "market": Field(
        str,
        is_required=False,
        description="å¸‚åœºè¿‡æ»¤ï¼ˆa_share/hk_stock/us_stockï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰å¸‚åœº"
    ),
    "doc_type": Field(
        str,
        is_required=False,
        description="æ–‡æ¡£ç±»å‹è¿‡æ»¤ï¼ˆquarterly_report/annual_report/ipo_prospectusï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰ç±»å‹"
    ),
    "stock_codes": Field(
        list,
        is_required=False,
        description="æŒ‰è‚¡ç¥¨ä»£ç åˆ—è¡¨è¿‡æ»¤ï¼ˆNone = ä¸è¿‡æ»¤ï¼ŒæŒ‡å®šåå°†åªå¤„ç†è¿™äº›è‚¡ç¥¨ä»£ç çš„æ–‡æ¡£åˆ†å—ï¼‰ã€‚ä¾‹å¦‚: ['000001', '000002']"
    ),
    "limit": Field(
        int,
        is_required=False,
        description="æœ¬æ¬¡ä½œä¸šæœ€å¤šå¤„ç†çš„åˆ†å—æ•°é‡ï¼ˆä¸è®¾ç½®åˆ™å¤„ç†å…¨éƒ¨ï¼‰"
    ),
    "force_reindex": Field(
        bool,
        default_value=False,
        description="æ˜¯å¦å¼ºåˆ¶é‡æ–°ç´¢å¼•ï¼ˆåˆ é™¤æ—§ç´¢å¼•ï¼‰"
    ),
}


# ==================== Dagster Ops ====================

@op(config_schema=ELASTICSEARCH_CONFIG_SCHEMA)
def scan_chunked_documents_op(context) -> Dict:
    """
    æ‰«æå·²åˆ†å—çš„æ–‡æ¡£åˆ†å—ï¼ˆç”¨äºç´¢å¼•åˆ° Elasticsearchï¼‰
    
    æŸ¥æ‰¾å·²åˆ†å—ä½†å¯èƒ½æœªç´¢å¼•åˆ° Elasticsearch çš„åˆ†å—
    
    Returns:
        åŒ…å«å¾…ç´¢å¼•åˆ†å—åˆ—è¡¨çš„å­—å…¸
    """
    config = context.op_config
    logger = get_dagster_logger()
    
    batch_size = config.get("batch_size", 100)
    limit = config.get("limit")  # ä¸è®¾ç½®é»˜è®¤å€¼ï¼ŒNone è¡¨ç¤ºå¤„ç†å…¨éƒ¨
    market_filter = config.get("market")
    doc_type_filter = config.get("doc_type")
    stock_codes_filter = config.get("stock_codes")
    force_reindex = config.get("force_reindex", False)

    logger.info(f"å¼€å§‹æ‰«æå¾…ç´¢å¼•åˆ†å—...")
    logger.info(
        f"é…ç½®: batch_size={batch_size}, limit={'å…¨éƒ¨' if limit is None else limit}, "
        f"market={market_filter}, doc_type={doc_type_filter}, "
        f"stock_codes={stock_codes_filter}, force_reindex={force_reindex}"
    )
    
    pg_client = get_postgres_client()
    
    try:
        with pg_client.get_session() as session:
            # æ„å»ºæŸ¥è¯¢ï¼šæŸ¥æ‰¾å·²åˆ†å—çš„åˆ†å—ï¼ˆchunk_text ä¸ä¸ºç©ºï¼‰
            query = session.query(DocumentChunk).join(
                Document, DocumentChunk.document_id == Document.id
            ).filter(
                DocumentChunk.chunk_text.isnot(None),
                DocumentChunk.chunk_text != ""
            )
            
            # å¦‚æœä¸æ˜¯å¼ºåˆ¶é‡æ–°ç´¢å¼•ï¼ŒåªæŸ¥è¯¢æœªç´¢å¼•åˆ° ES çš„åˆ†å—
            if not force_reindex:
                query = query.filter(DocumentChunk.es_indexed_at.is_(None))
            
            # åº”ç”¨å¸‚åœºè¿‡æ»¤
            if market_filter:
                query = query.filter(Document.market == market_filter)
            
            # åº”ç”¨æ–‡æ¡£ç±»å‹è¿‡æ»¤
            if doc_type_filter:
                query = query.filter(Document.doc_type == doc_type_filter)

            # åº”ç”¨è‚¡ç¥¨ä»£ç è¿‡æ»¤
            if stock_codes_filter:
                logger.info(f"æŒ‰è‚¡ç¥¨ä»£ç è¿‡æ»¤: {stock_codes_filter}")
                query = query.filter(Document.stock_code.in_(stock_codes_filter))

            # é™åˆ¶æ•°é‡å¹¶æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå¦‚æœ limit ä¸º Noneï¼Œåˆ™ä¸é™åˆ¶ï¼‰
            if limit is not None:
                chunks = query.limit(limit).all()
            else:
                chunks = query.all()
            
            logger.info(f"æ‰¾åˆ° {len(chunks)} ä¸ªå·²åˆ†å—çš„åˆ†å—")
            
            # å°†åˆ†å—åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆä¾¿äº Dagster ä¼ é€’ï¼‰
            chunk_list = []
            for chunk in chunks:
                doc = session.query(Document).filter(Document.id == chunk.document_id).first()
                if not doc:
                    continue
                
                chunk_list.append({
                    "chunk_id": str(chunk.id),
                    "document_id": str(chunk.document_id),
                    "chunk_index": chunk.chunk_index,
                    "chunk_text": chunk.chunk_text,
                    "title": chunk.title or "",
                    "title_level": chunk.title_level,
                    "chunk_size": chunk.chunk_size,
                    "is_table": chunk.is_table or False,
                    "stock_code": doc.stock_code,
                    "company_name": doc.company_name,
                    "market": doc.market,
                    "doc_type": doc.doc_type,
                    "year": doc.year,
                    "quarter": doc.quarter,
                    "publish_date": doc.publish_date.isoformat() if doc.publish_date else None,
                })
            
            # æŒ‰æ‰¹æ¬¡åˆ†ç»„
            batches = []
            for i in range(0, len(chunk_list), batch_size):
                batch = chunk_list[i:i + batch_size]
                batches.append(batch)
            
            logger.info(f"åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} ä¸ªåˆ†å—")
            
            return {
                "success": True,
                "total_chunks": len(chunk_list),
                "total_batches": len(batches),
                "batches": batches,
                "chunks": chunk_list,
            }
            
    except Exception as e:
        logger.error(f"æ‰«æå¾…ç´¢å¼•åˆ†å—å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error_message": str(e),
            "total_chunks": 0,
            "total_batches": 0,
            "batches": [],
            "chunks": [],
        }


@op(
    config_schema={
        "force_reindex": Field(
            bool,
            default_value=False,
            description="æ˜¯å¦å¼ºåˆ¶é‡æ–°ç´¢å¼•ï¼ˆåˆ é™¤æ—§ç´¢å¼•ï¼‰"
        ),
    }
)
def index_chunks_to_elasticsearch_op(context, scan_result: Dict) -> Dict:
    """
    æ‰¹é‡ç´¢å¼•åˆ†å—åˆ° Elasticsearch
    
    å¯¹æ‰«æåˆ°çš„åˆ†å—è¿›è¡Œç´¢å¼•ï¼Œå¹¶ä¿å­˜åˆ° Elasticsearch
    
    Args:
        scan_result: scan_chunked_documents_op çš„è¿”å›ç»“æœ
        
    Returns:
        ç´¢å¼•ç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ scan_result æ˜¯å¦ä¸º None
    if scan_result is None:
        logger.error("scan_result ä¸º Noneï¼Œæ— æ³•ç»§ç»­ç´¢å¼•")
        return {
            "success": False,
            "error_message": "scan_result ä¸º None",
            "indexed_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    # å®‰å…¨è·å– config
    config = context.op_config if hasattr(context, 'op_config') else {}
    force_reindex = config.get("force_reindex", False) if config else False
    
    if not scan_result.get("success"):
        logger.error(f"æ‰«æå¤±è´¥ï¼Œè·³è¿‡ç´¢å¼•: {scan_result.get('error_message')}")
        return {
            "success": False,
            "error_message": "æ‰«æå¤±è´¥",
            "indexed_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    chunks = scan_result.get("chunks", [])
    if not chunks:
        logger.info("æ²¡æœ‰å¾…ç´¢å¼•çš„åˆ†å—")
        return {
            "success": True,
            "indexed_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    logger.info(f"å¼€å§‹ç´¢å¼• {len(chunks)} ä¸ªåˆ†å—åˆ° Elasticsearch...")
    
    # åˆå§‹åŒ– Elasticsearch å®¢æˆ·ç«¯
    es_client = get_elasticsearch_client()
    
    # ç¡®ä¿ç´¢å¼•å­˜åœ¨
    index_name = "chunks"
    try:
        es_client.create_index(index_name)
    except Exception as e:
        logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
        return {
            "success": False,
            "error_message": f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}",
            "indexed_count": 0,
            "failed_count": len(chunks),
            "skipped_count": 0,
        }
    
    indexed_count = 0
    failed_count = 0
    skipped_count = 0
    failed_chunks = []
    
    # æ‰¹é‡ç´¢å¼•ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    batch_size = 100
    total_batches = (len(chunks) + batch_size - 1) // batch_size
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        batch_num = i // batch_size + 1
        processed = min(i + batch_size, len(chunks))
        progress_pct = processed / len(chunks) * 100

        logger.info(
            f"ğŸ“¦ æ‰¹æ¬¡ [{batch_num}/{total_batches}] | "
            f"æœ¬æ‰¹ {len(batch)} é¡¹ | "
            f"æ€»è¿›åº¦ {processed}/{len(chunks)} ({progress_pct:.1f}%)"
        )

        try:
            # å‡†å¤‡æ–‡æ¡£æ•°æ®
            documents = []
            for chunk in batch:
                # æ„å»º Elasticsearch æ–‡æ¡£
                doc = {
                    "id": chunk["chunk_id"],
                    "document_id": chunk["document_id"],
                    "chunk_index": chunk["chunk_index"],
                    "chunk_text": chunk["chunk_text"],
                    "title": chunk.get("title", ""),
                    "title_level": chunk.get("title_level"),
                    "chunk_size": chunk.get("chunk_size", 0),
                    "is_table": chunk.get("is_table", False),
                    "stock_code": chunk["stock_code"],
                    "company_name": chunk["company_name"],
                    "market": chunk["market"],
                    "doc_type": chunk["doc_type"],
                    "year": chunk["year"],
                    "quarter": chunk.get("quarter"),
                    "publish_date": chunk.get("publish_date"),
                    "created_at": datetime.now().isoformat(),
                }
                documents.append(doc)
            
            # æ‰¹é‡ç´¢å¼•
            result = es_client.bulk_index_documents(
                index_name=index_name,
                documents=documents,
                document_id_field="id"
            )
            
            success_count = result.get("success_count", 0)
            failed_items = result.get("failed_items", [])
            
            indexed_count += success_count
            failed_count += len(failed_items)
            
            if failed_items:
                for item in failed_items[:10]:  # åªè®°å½•å‰10ä¸ªå¤±è´¥é¡¹
                    failed_chunks.append(item.get("_id", "unknown"))

            # æ˜¾ç¤ºæ‰¹æ¬¡ç»“æœ
            logger.info(f"   âœ“ æˆåŠŸ {success_count} å¤±è´¥ {len(failed_items)}")
            
        except Exception as e:
            logger.error(f"æ‰¹é‡ç´¢å¼•å¤±è´¥ï¼ˆæ‰¹æ¬¡ {i//batch_size + 1}ï¼‰: {e}", exc_info=True)
            failed_count += len(batch)
            failed_chunks.extend([chunk["chunk_id"] for chunk in batch])
    
    # åˆ·æ–°ç´¢å¼•ï¼ˆä½¿æ–°ç´¢å¼•çš„æ–‡æ¡£ç«‹å³å¯æœç´¢ï¼‰
    try:
        es_client.refresh_index(index_name)
    except Exception as e:
        logger.warning(f"åˆ·æ–°ç´¢å¼•å¤±è´¥: {e}")
    
    # æ›´æ–°æˆåŠŸç´¢å¼•çš„åˆ†å—çš„ es_indexed_at å­—æ®µ
    if indexed_count > 0:
        try:
            pg_client = get_postgres_client()
            # æ”¶é›†æˆåŠŸç´¢å¼•çš„ chunk_idï¼ˆæ’é™¤å¤±è´¥çš„ï¼‰
            failed_set = set(failed_chunks)
            success_chunk_ids = [
                chunk["chunk_id"] for chunk in chunks
                if chunk["chunk_id"] not in failed_set
            ]
            
            if success_chunk_ids:
                with pg_client.get_session() as session:
                    from sqlalchemy import update
                    import uuid
                    
                    # æ‰¹é‡æ›´æ–° es_indexed_at å­—æ®µ
                    stmt = update(DocumentChunk).where(
                        DocumentChunk.id.in_([uuid.UUID(cid) for cid in success_chunk_ids])
                    ).values(es_indexed_at=datetime.now())
                    
                    session.execute(stmt)
                    session.commit()
                    
                logger.info(f"å·²æ›´æ–° {len(success_chunk_ids)} ä¸ªåˆ†å—çš„ es_indexed_at å­—æ®µ")
        except Exception as e:
            logger.error(f"æ›´æ–° es_indexed_at å­—æ®µå¤±è´¥: {e}", exc_info=True)
    
    logger.info(
        f"ç´¢å¼•å®Œæˆ: æˆåŠŸ={indexed_count}, å¤±è´¥={failed_count}, è·³è¿‡={skipped_count}"
    )
    
    if failed_chunks:
        logger.warning(f"å¤±è´¥çš„åˆ†å—æ•°é‡: {len(failed_chunks)}")
        # åªè®°å½•å‰10ä¸ªå¤±è´¥çš„åˆ†å—
        for chunk_id in failed_chunks[:10]:
            logger.warning(f"  - å¤±è´¥åˆ†å—: {chunk_id}")
    
    return {
        "success": True,
        "indexed_count": indexed_count,
        "failed_count": failed_count,
        "skipped_count": skipped_count,
        "total_chunks": len(chunks),
        "failed_chunks": failed_chunks[:10],  # æœ€å¤šè¿”å›10ä¸ªå¤±è´¥è®°å½•
    }


@op
def validate_elasticsearch_results_op(context, index_results: Dict) -> Dict:
    """
    éªŒè¯ Elasticsearch ç´¢å¼•ç»“æœ
    
    æ£€æŸ¥ç´¢å¼•ç»“æœçš„è´¨é‡ï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        index_results: index_chunks_to_elasticsearch_op çš„è¿”å›ç»“æœ
        
    Returns:
        éªŒè¯ç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ index_results æ˜¯å¦ä¸º None
    if index_results is None:
        logger.error("index_results ä¸º Noneï¼Œæ— æ³•éªŒè¯")
        return {
            "success": False,
            "error_message": "index_results ä¸º None",
            "validation_passed": False,
        }
    
    if not index_results.get("success"):
        logger.error(f"ç´¢å¼•å¤±è´¥: {index_results.get('error_message')}")
        return {
            "success": False,
            "error_message": index_results.get("error_message"),
            "validation_passed": False,
        }
    
    indexed_count = index_results.get("indexed_count", 0)
    failed_count = index_results.get("failed_count", 0)
    total_chunks = index_results.get("total_chunks", 0)
    
    # è®¡ç®—æˆåŠŸç‡
    if total_chunks > 0:
        success_rate = indexed_count / total_chunks
    else:
        success_rate = 0.0
    
    logger.info(
        f"ç´¢å¼•éªŒè¯: æ€»æ•°={total_chunks}, "
        f"æˆåŠŸ={indexed_count}, å¤±è´¥={failed_count}, "
        f"æˆåŠŸç‡={success_rate:.2%}"
    )
    
    # éªŒè¯è§„åˆ™ï¼šæˆåŠŸç‡ >= 90%
    validation_passed = success_rate >= 0.9
    
    if not validation_passed:
        logger.warning(
            f"âš ï¸ ç´¢å¼•æˆåŠŸç‡ä½äº90%: {success_rate:.2%}, "
            f"å¤±è´¥æ•°é‡={failed_count}"
        )
    
    return {
        "success": True,
        "validation_passed": validation_passed,
        "total_chunks": total_chunks,
        "indexed_count": indexed_count,
        "failed_count": failed_count,
        "success_rate": success_rate,
        "failed_chunks_count": len(index_results.get("failed_chunks", [])),
    }


# ==================== Dagster Jobs ====================

@job(
    config={
        "ops": {
            "scan_chunked_documents_op": {
                "config": {
                    "batch_size": 100,
                    # limit ä¸è®¾ç½®åˆ™å¤„ç†å…¨éƒ¨ï¼Œmarket å’Œ doc_type æ˜¯å¯é€‰çš„ï¼Œä¸è®¾ç½®è¡¨ç¤ºæ‰€æœ‰ç±»å‹
                }
            },
            "index_chunks_to_elasticsearch_op": {
                "config": {
                    "force_reindex": False,
                }
            }
        }
    },
    description="Elasticsearch ç´¢å¼•ä½œä¸š - é»˜è®¤é…ç½®"
)
def elasticsearch_index_job():
    """
    Elasticsearch ç´¢å¼•ä½œä¸š

    å®Œæ•´æµç¨‹ï¼š
    1. æ‰«æå·²åˆ†å—çš„æ–‡æ¡£åˆ†å—
    2. æ‰¹é‡ç´¢å¼•åˆ° Elasticsearch
    3. éªŒè¯ç´¢å¼•ç»“æœ

    é»˜è®¤é…ç½®ï¼š
    - scan_chunked_documents_op:
        - batch_size: 100 (æ¯æ‰¹å¤„ç†100ä¸ªåˆ†å—)
        - limit: ä¸è®¾ç½® (å¤„ç†å…¨éƒ¨åˆ†å—)
    - index_chunks_to_elasticsearch_op:
        - force_reindex: False (ä¸å¼ºåˆ¶é‡æ–°ç´¢å¼•)
    """
    scan_result = scan_chunked_documents_op()
    index_results = index_chunks_to_elasticsearch_op(scan_result)
    validate_elasticsearch_results_op(index_results)


# ==================== Schedules ====================

@schedule(
    job=elasticsearch_index_job,
    cron_schedule="0 */3 * * *",  # æ¯3å°æ—¶æ‰§è¡Œä¸€æ¬¡
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨
)
def hourly_elasticsearch_schedule(context):
    """
    æ¯3å°æ—¶å®šæ—¶ç´¢å¼•ä½œä¸š
    """
    return RunRequest()


@schedule(
    job=elasticsearch_index_job,
    cron_schedule="0 6 * * *",  # æ¯å¤©å‡Œæ™¨6ç‚¹æ‰§è¡Œï¼ˆåœ¨åˆ†å—ä½œä¸šä¹‹åï¼‰
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢
)
def daily_elasticsearch_schedule(context):
    """
    æ¯æ—¥å®šæ—¶ç´¢å¼•ä½œä¸šï¼ˆåœ¨åˆ†å—ä½œä¸šä¹‹åæ‰§è¡Œï¼‰
    """
    return RunRequest()


# ==================== Sensors ====================

@sensor(
    job=elasticsearch_index_job,
    default_status=DefaultSensorStatus.STOPPED,
)
def manual_trigger_elasticsearch_sensor(context):
    """
    æ‰‹åŠ¨è§¦å‘ Elasticsearch ç´¢å¼•ä¼ æ„Ÿå™¨
    
    å¯ä»¥é€šè¿‡ Dagster UI æ‰‹åŠ¨è§¦å‘ç´¢å¼•ä½œä¸š
    """
    return RunRequest()
