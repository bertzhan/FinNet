# -*- coding: utf-8 -*-
"""
Dagster å‘é‡åŒ–ä½œä¸šå®šä¹‰
è‡ªåŠ¨æ‰«ææœªå‘é‡åŒ–çš„æ–‡æ¡£åˆ†å—å¹¶æ‰§è¡Œå‘é‡åŒ–

æŒ‰ç…§ plan.md è®¾è®¡ï¼š
- å¤„ç†å±‚ï¼ˆProcessing Layerï¼‰â†’ Dagster è°ƒåº¦
- å‘é‡åŒ– â†’ Milvus å­˜å‚¨
"""

from datetime import datetime
from typing import List, Dict, Optional

from dagster import (
    job,
    op,
    schedule,
    sensor,
    DefaultSensorStatus,
    DefaultScheduleStatus,
    RunRequest,
    Field,
    get_dagster_logger,
    AssetMaterialization,
    MetadataValue,
)

# å¯¼å…¥å‘é‡åŒ–æœåŠ¡å’Œæ•°æ®åº“æ¨¡å—
from src.processing.ai.embedding.vectorizer import get_vectorizer
from src.storage.metadata.postgres_client import get_postgres_client
from src.storage.metadata import crud
from src.storage.metadata.models import Document, DocumentChunk
from src.common.constants import DocType, Market
from src.common.config import common_config


# ==================== é…ç½® Schema ====================

VECTORIZE_CONFIG_SCHEMA = {
    "batch_size": Field(
        int,
        default_value=50,
        description="æ¯æ‰¹å‘é‡åŒ–çš„åˆ†å—æ•°é‡ï¼ˆ1-100ï¼‰"
    ),
    "market": Field(
        str,
        is_required=False,
        description="å¸‚åœºè¿‡æ»¤ï¼ˆa_share/hk_stock/us_stockï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰å¸‚åœº"
    ),
    "doc_type": Field(
        str,
        is_required=False,
        description="æ–‡æ¡£ç±»å‹è¿‡æ»¤ï¼ˆquarterly_report/annual_report/ipo_prospectusï¼‰ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰ç±»å‹"
    ),
    "stock_codes": Field(
        list,
        is_required=False,
        description="æŒ‰è‚¡ç¥¨ä»£ç åˆ—è¡¨è¿‡æ»¤ï¼ˆNone = ä¸è¿‡æ»¤ï¼ŒæŒ‡å®šåå°†åªå¤„ç†è¿™äº›è‚¡ç¥¨ä»£ç çš„æ–‡æ¡£åˆ†å—ï¼‰ã€‚ä¾‹å¦‚: ['000001', '000002']"
    ),
    "limit": Field(
        int,
        is_required=False,
        description="æœ¬æ¬¡ä½œä¸šæœ€å¤šå¤„ç†çš„åˆ†å—æ•°é‡ï¼ˆ1-10000ï¼‰ï¼ŒNone æˆ–ä¸è®¾ç½®è¡¨ç¤ºå¤„ç†å…¨éƒ¨æœªå‘é‡åŒ–çš„åˆ†å—"
    ),
    "force_revectorize": Field(
        bool,
        default_value=False,
        description="æ˜¯å¦å¼ºåˆ¶é‡æ–°å‘é‡åŒ–ï¼ˆåˆ é™¤æ—§å‘é‡ï¼‰"
    ),
}


# ==================== Dagster Ops ====================

@op(config_schema=VECTORIZE_CONFIG_SCHEMA)
def scan_unvectorized_chunks_op(context) -> Dict:
    """
    æ‰«ææœªå‘é‡åŒ–çš„æ–‡æ¡£åˆ†å—
    
    æŸ¥æ‰¾ vectorized_at IS NULL çš„åˆ†å—
    
    Returns:
        åŒ…å«å¾…å‘é‡åŒ–åˆ†å—åˆ—è¡¨çš„å­—å…¸
    """
    config = context.op_config
    logger = get_dagster_logger()
    
    batch_size = config.get("batch_size", 32)
    limit = config.get("limit")  # None è¡¨ç¤ºå¤„ç†å…¨éƒ¨
    market_filter = config.get("market")
    doc_type_filter = config.get("doc_type")
    stock_codes_filter = config.get("stock_codes")
    force_revectorize = config.get("force_revectorize", False)

    logger.info(f"å¼€å§‹æ‰«æå¾…å‘é‡åŒ–åˆ†å—...")
    if limit is not None:
        logger.info(
            f"é…ç½®: batch_size={batch_size}, limit={limit}, "
            f"market={market_filter}, doc_type={doc_type_filter}, "
            f"stock_codes={stock_codes_filter}, force_revectorize={force_revectorize}"
        )
    else:
        logger.info(
            f"é…ç½®: batch_size={batch_size}, limit=æ— é™åˆ¶ï¼ˆå¤„ç†å…¨éƒ¨ï¼‰, "
            f"market={market_filter}, doc_type={doc_type_filter}, "
            f"stock_codes={stock_codes_filter}, force_revectorize={force_revectorize}"
        )
    
    pg_client = get_postgres_client()
    
    try:
        with pg_client.get_session() as session:
            # æ„å»ºæŸ¥è¯¢ï¼šæŸ¥æ‰¾æœªå‘é‡åŒ–çš„åˆ†å—
            query = session.query(DocumentChunk).join(
                Document, DocumentChunk.document_id == Document.id
            )
            
            # åº”ç”¨è¿‡æ»¤æ¡ä»¶ï¼šä½¿ç”¨ status å­—æ®µ
            if not force_revectorize:
                # åªæ‰«æå¾…å¤„ç†å’Œå¤±è´¥çš„åˆ†å—ï¼ˆæ”¯æŒè‡ªåŠ¨é‡è¯•å¤±è´¥çš„åˆ†å—ï¼‰
                query = query.filter(DocumentChunk.status.in_(['pending', 'failed']))
                logger.info("è¿‡æ»¤æ¡ä»¶: åªæ‰«æå¾…å‘é‡åŒ–å’Œå¤±è´¥çš„åˆ†å— (status IN ('pending', 'failed'))")
            else:
                logger.info("âš ï¸ force_revectorize=True: å°†æ‰«ææ‰€æœ‰åˆ†å—ï¼ˆåŒ…æ‹¬å·²å‘é‡åŒ–çš„ï¼‰")
            
            # æ³¨æ„ï¼šä¸å†è¿‡æ»¤è¡¨æ ¼åˆ†å—ï¼Œè€Œæ˜¯åœ¨å‘é‡åŒ–æ—¶æå–è¡¨æ ¼æ–‡æœ¬å†…å®¹
            # è¡¨æ ¼åˆ†å—ä¼šåœ¨vectorizerä¸­æå–æ–‡æœ¬åå†å‘é‡åŒ–
            
            # åº”ç”¨å¸‚åœºè¿‡æ»¤
            if market_filter:
                query = query.filter(Document.market == market_filter)
            
            # åº”ç”¨æ–‡æ¡£ç±»å‹è¿‡æ»¤
            if doc_type_filter:
                query = query.filter(Document.doc_type == doc_type_filter)

            # åº”ç”¨è‚¡ç¥¨ä»£ç è¿‡æ»¤
            if stock_codes_filter:
                logger.info(f"æŒ‰è‚¡ç¥¨ä»£ç è¿‡æ»¤: {stock_codes_filter}")
                query = query.filter(Document.stock_code.in_(stock_codes_filter))

            # é™åˆ¶æ•°é‡å¹¶æ‰§è¡ŒæŸ¥è¯¢ï¼ˆåŒ…å«è¡¨æ ¼åˆ†å—ï¼Œå°†åœ¨å‘é‡åŒ–æ—¶æå–æ–‡æœ¬ï¼‰
            if limit is not None:
                chunks = query.limit(limit).all()
            else:
                # limit ä¸º None æ—¶ï¼Œå¤„ç†å…¨éƒ¨æœªå‘é‡åŒ–çš„åˆ†å—
                chunks = query.all()
            
            # ç»Ÿè®¡è¡¨æ ¼åˆ†å—æ•°é‡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            table_chunks_count = sum(1 for chunk in chunks if chunk.is_table)
            
            if table_chunks_count > 0:
                logger.info(
                    f"æ‰¾åˆ° {len(chunks)} ä¸ªå¾…å‘é‡åŒ–çš„åˆ†å—ï¼ˆå…¶ä¸­ {table_chunks_count} ä¸ªåŒ…å«è¡¨æ ¼ï¼Œ"
                    f"å°†åœ¨å‘é‡åŒ–æ—¶æå–æ–‡æœ¬å†…å®¹ï¼‰"
                )
            else:
                logger.info(f"æ‰¾åˆ° {len(chunks)} ä¸ªå¾…å‘é‡åŒ–çš„åˆ†å—")
            
            # å°†åˆ†å—åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆä¾¿äº Dagster ä¼ é€’ï¼‰
            chunk_list = []
            for chunk in chunks:
                doc = session.query(Document).filter(Document.id == chunk.document_id).first()
                if not doc:
                    continue
                
                chunk_list.append({
                    "chunk_id": str(chunk.id),
                    "document_id": str(chunk.document_id),
                    "chunk_index": chunk.chunk_index,
                    "chunk_text": chunk.chunk_text[:100] + "..." if len(chunk.chunk_text) > 100 else chunk.chunk_text,  # åªä¿å­˜å‰100å­—ç¬¦ç”¨äºæ—¥å¿—
                    "stock_code": doc.stock_code,
                    "company_name": doc.company_name,
                    "market": doc.market,
                    "doc_type": doc.doc_type,
                    "year": doc.year,
                    "quarter": doc.quarter,
                    "vectorized_at": chunk.vectorized_at.isoformat() if chunk.vectorized_at else None,
                })
            
            # æŒ‰æ‰¹æ¬¡åˆ†ç»„
            batches = []
            for i in range(0, len(chunk_list), batch_size):
                batch = chunk_list[i:i + batch_size]
                batches.append(batch)
            
            logger.info(f"åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹ {batch_size} ä¸ªåˆ†å—")
            
            return {
                "success": True,
                "total_chunks": len(chunk_list),
                "total_batches": len(batches),
                "batches": batches,
                "chunks": chunk_list,
            }
            
    except Exception as e:
        logger.error(f"æ‰«æå¾…å‘é‡åŒ–åˆ†å—å¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error_message": str(e),
            "total_chunks": 0,
            "total_batches": 0,
            "batches": [],
            "chunks": [],
        }


@op(
    config_schema={
        "force_revectorize": Field(
            bool,
            default_value=False,
            description="æ˜¯å¦å¼ºåˆ¶é‡æ–°å‘é‡åŒ–ï¼ˆåˆ é™¤æ—§å‘é‡ï¼‰"
        ),
    }
)
def vectorize_chunks_op(context, scan_result: Dict) -> Dict:
    """
    æ‰¹é‡æ‰§è¡Œå‘é‡åŒ–
    
    å¯¹æ‰«æåˆ°çš„åˆ†å—è¿›è¡Œå‘é‡åŒ–ï¼Œå¹¶ä¿å­˜åˆ° Milvus å’Œæ•°æ®åº“
    
    Args:
        scan_result: scan_unvectorized_chunks_op çš„è¿”å›ç»“æœ
        
    Returns:
        å‘é‡åŒ–ç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ scan_result æ˜¯å¦ä¸º None
    if scan_result is None:
        logger.error("scan_result ä¸º Noneï¼Œæ— æ³•ç»§ç»­å‘é‡åŒ–")
        return {
            "success": False,
            "error_message": "scan_result ä¸º None",
            "vectorized_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    # å®‰å…¨è·å– config
    config = context.op_config if hasattr(context, 'op_config') else {}
    force_revectorize = config.get("force_revectorize", False) if config else False
    
    if not scan_result.get("success"):
        logger.error(f"æ‰«æå¤±è´¥ï¼Œè·³è¿‡å‘é‡åŒ–: {scan_result.get('error_message')}")
        return {
            "success": False,
            "error_message": "æ‰«æå¤±è´¥",
            "vectorized_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    chunks = scan_result.get("chunks", [])
    if not chunks:
        logger.info("æ²¡æœ‰å¾…å‘é‡åŒ–çš„åˆ†å—")
        return {
            "success": True,
            "vectorized_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
        }
    
    logger.info(f"å¼€å§‹å‘é‡åŒ– {len(chunks)} ä¸ªåˆ†å—...")

    # æå–åˆ†å—IDåˆ—è¡¨
    chunk_ids = [chunk["chunk_id"] for chunk in chunks]

    try:
        # åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡ï¼ˆæ”¾åœ¨ try å—å†…ï¼Œä»¥ä¾¿æ•è·åˆå§‹åŒ–å¼‚å¸¸ï¼‰
        try:
            vectorizer = get_vectorizer()
        except Exception as init_error:
            logger.error(f"å‘é‡åŒ–æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {init_error}", exc_info=True)
            raise Exception(f"å‘é‡åŒ–æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(init_error)}ã€‚è¯·æ£€æŸ¥åµŒå…¥æ¨¡å‹é…ç½®ï¼ˆEMBEDDING_MODE, EMBEDDING_API_URLç­‰ï¼‰")

        # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆç”¨äºå®æ—¶è¿›åº¦æ—¥å¿—ï¼‰
        def on_batch_complete(batch_info):
            """æ¯ä¸ªæ‰¹æ¬¡å®Œæˆåè°ƒç”¨ï¼Œè®°å½•è¿›åº¦æ—¥å¿—"""
            batch_num = batch_info.get('batch_num', 0)
            total_batches = batch_info.get('total_batches', 0)
            processed = batch_info.get('processed', 0)
            total = batch_info.get('total', len(chunks))
            
            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            progress_pct = (processed / total * 100) if total > 0 else 0
            
            # ä½¿ç”¨ logger.info ç¡®ä¿åœ¨ Dagster UI ä¸­å¯è§ï¼ˆå‚è€ƒ parse_jobs å’Œ chunk_jobs çš„æ ¼å¼ï¼‰
            logger.info(
                f"ğŸ“¦ [{processed}/{total}] {progress_pct:.1f}% | "
                f"å‘é‡åŒ–æ‰¹æ¬¡: {batch_num}/{total_batches}"
            )

        # æ‰¹é‡å‘é‡åŒ–ï¼ˆä¼ å…¥è¿›åº¦å›è°ƒï¼‰
        result = vectorizer.vectorize_chunks(
            chunk_ids=chunk_ids,
            force_revectorize=force_revectorize,
            progress_callback=on_batch_complete
        )
        
        vectorized_count = result.get("vectorized_count", 0)
        failed_count = result.get("failed_count", 0)
        failed_chunks = result.get("failed_chunks", [])
        
        logger.info(
            f"å‘é‡åŒ–å®Œæˆ: æˆåŠŸ={vectorized_count}, å¤±è´¥={failed_count}"
        )

        # è®°å½•èµ„äº§ç‰©åŒ–ï¼ˆDagster æ•°æ®è¡€ç¼˜ï¼‰
        # æŒ‰æ–‡æ¡£åˆ†ç»„ï¼Œä¸ºæ¯ä¸ªæˆåŠŸå‘é‡åŒ–çš„æ–‡æ¡£è®°å½• AssetMaterialization
        failed_set = set(failed_chunks)
        document_chunks_map = {}  # document_id -> list of chunks
        
        for chunk in chunks:
            chunk_id = chunk["chunk_id"]
            if chunk_id in failed_set:
                continue  # è·³è¿‡å¤±è´¥çš„åˆ†å—
            
            doc_id = chunk["document_id"]
            if doc_id not in document_chunks_map:
                document_chunks_map[doc_id] = []
            document_chunks_map[doc_id].append(chunk)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£è®°å½• AssetMaterialization
        for doc_id, doc_chunks in document_chunks_map.items():
            if not doc_chunks:
                continue
            
            try:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªchunkçš„ä¿¡æ¯ï¼ˆæ‰€æœ‰chunkçš„æ–‡æ¡£ä¿¡æ¯åº”è¯¥ç›¸åŒï¼‰
                first_chunk = doc_chunks[0]
                market = first_chunk.get("market", "")
                doc_type = first_chunk.get("doc_type", "")
                stock_code = first_chunk.get("stock_code", "")
                company_name = first_chunk.get("company_name", "")
                vectorized_chunks_count = len(doc_chunks)
                
                # æ„å»ºèµ„äº§key: ["silver", "vectorized_chunks", market, doc_type, stock_code]
                asset_key = ["silver", "vectorized_chunks", market, doc_type, stock_code]
                
                # æ„å»ºçˆ¶èµ„äº§keyï¼ˆæŒ‡å‘chunked_documentsï¼‰
                parent_asset_key = ["silver", "chunked_documents", market, doc_type, stock_code]
                
                context.log_event(
                    AssetMaterialization(
                        asset_key=asset_key,
                        description=f"{company_name} å‘é‡åŒ–å®Œæˆ ({vectorized_chunks_count} ä¸ªåˆ†å—)",
                        metadata={
                            "document_id": MetadataValue.text(doc_id),
                            "stock_code": MetadataValue.text(stock_code),
                            "company_name": MetadataValue.text(company_name or ""),
                            "market": MetadataValue.text(market),
                            "doc_type": MetadataValue.text(doc_type),
                            "vectorized_chunks_count": MetadataValue.int(vectorized_chunks_count),
                            "vectorized_at": MetadataValue.text(datetime.now().isoformat()),
                            "parent_asset_key": MetadataValue.text("/".join(parent_asset_key)),
                        }
                    )
                )
            except Exception as e:
                logger.warning(f"è®°å½• AssetMaterialization äº‹ä»¶å¤±è´¥ (document_id={doc_id}): {e}")

        if failed_chunks:
            logger.warning(f"å¤±è´¥çš„åˆ†å—æ•°é‡: {len(failed_chunks)}")
            logger.warning("=" * 80)
            logger.warning("å¤±è´¥åˆ†å—è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å« chunk_textï¼‰:")
            logger.warning("=" * 80)
            
            # ä»æ•°æ®åº“æŸ¥è¯¢å¤±è´¥åˆ†å—çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬chunk_textï¼‰
            from src.storage.metadata.postgres_client import get_postgres_client
            from src.storage.metadata.models import DocumentChunk, Document
            import uuid as uuid_lib
            
            pg_client = get_postgres_client()
            try:
                with pg_client.get_session() as session:
                    # æ‰“å°æ‰€æœ‰å¤±è´¥çš„åˆ†å—ï¼ˆä¸åªæ˜¯å‰10ä¸ªï¼‰
                    for i, chunk_id in enumerate(failed_chunks, 1):
                        try:
                            chunk = session.query(DocumentChunk).filter(
                                DocumentChunk.id == uuid_lib.UUID(chunk_id)
                            ).first()
                            
                            if chunk:
                                chunk_text_preview = chunk.chunk_text[:200] if chunk.chunk_text else ""
                                doc = session.query(Document).filter(
                                    Document.id == chunk.document_id
                                ).first()
                                
                                logger.warning(
                                    f"\nå¤±è´¥åˆ†å— {i}/{len(failed_chunks)}:\n"
                                    f"  Chunk ID: {chunk_id}\n"
                                    f"  Document ID: {chunk.document_id}\n"
                                    f"  è‚¡ç¥¨ä»£ç : {doc.stock_code if doc else 'N/A'}\n"
                                    f"  å…¬å¸åç§°: {doc.company_name if doc else 'N/A'}\n"
                                    f"  Chunk Index: {chunk.chunk_index}\n"
                                    f"  Chunk Text (å‰200å­—ç¬¦):\n"
                                    f"  {chunk_text_preview}\n"
                                    f"  {'...' if len(chunk.chunk_text or '') > 200 else ''}"
                                )
                            else:
                                logger.warning(f"å¤±è´¥åˆ†å— {i}: Chunk ID {chunk_id} ä¸å­˜åœ¨")
                        except Exception as e:
                            logger.error(f"æŸ¥è¯¢å¤±è´¥åˆ†å— {chunk_id} ä¿¡æ¯å¼‚å¸¸: {e}")
            except Exception as e:
                logger.error(f"æŸ¥è¯¢å¤±è´¥åˆ†å—ä¿¡æ¯å¼‚å¸¸: {e}", exc_info=True)
            
            logger.warning("=" * 80)
        
        return {
            "success": True,
            "vectorized_count": vectorized_count,
            "failed_count": failed_count,
            "skipped_count": 0,
            "total_chunks": len(chunks),
            "failed_chunks": failed_chunks[:10],  # æœ€å¤šè¿”å›10ä¸ªå¤±è´¥è®°å½•
        }
        
    except Exception as e:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Dagster ä¸­æ–­å¼‚å¸¸
        error_type = type(e).__name__
        if "Interrupt" in error_type or "Interrupted" in error_type:
            logger.warning(f"âš ï¸ å‘é‡åŒ–è¢«ä¸­æ–­: {error_type}")
            raise
        
        error_msg = str(e)
        logger.error(f"å‘é‡åŒ–å¼‚å¸¸: {error_msg}", exc_info=True)
        
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®
        if "åˆå§‹åŒ–å¤±è´¥" in error_msg or "embedder" in error_msg.lower() or "api" in error_msg.lower():
            logger.error("=" * 80)
            logger.error("å‘é‡åŒ–æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œå¯èƒ½çš„åŸå› ï¼š")
            logger.error("1. åµŒå…¥æ¨¡å‹ API é…ç½®é”™è¯¯ï¼ˆEMBEDDING_API_URL, EMBEDDING_API_KEYï¼‰")
            logger.error("2. åµŒå…¥æ¨¡å‹ API æœåŠ¡ä¸å¯ç”¨ï¼ˆ404/500é”™è¯¯ï¼‰")
            logger.error("3. æœ¬åœ°æ¨¡å‹è·¯å¾„é”™è¯¯æˆ–æ¨¡å‹æ–‡ä»¶ç¼ºå¤±")
            logger.error("")
            logger.error("å»ºè®®è§£å†³æ–¹æ¡ˆï¼š")
            logger.error("- æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ EMBEDDING_MODE é…ç½®")
            logger.error("- å¦‚æœä½¿ç”¨ API æ¨¡å¼ï¼Œæ£€æŸ¥ EMBEDDING_API_URL å’Œ EMBEDDING_API_KEY")
            logger.error("- å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å¼ï¼Œæ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
            logger.error("=" * 80)
        
        return {
            "success": False,
            "error_message": error_msg,
            "vectorized_count": 0,
            "failed_count": len(chunks),
            "skipped_count": 0,
            "total_chunks": len(chunks),
        }


@op
def validate_vectorize_results_op(context, vectorize_results: Dict) -> Dict:
    """
    éªŒè¯å‘é‡åŒ–ç»“æœ
    
    æ£€æŸ¥å‘é‡åŒ–ç»“æœçš„è´¨é‡ï¼Œè®°å½•ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        vectorize_results: vectorize_chunks_op çš„è¿”å›ç»“æœ
        
    Returns:
        éªŒè¯ç»“æœç»Ÿè®¡
    """
    logger = get_dagster_logger()
    
    # æ£€æŸ¥ vectorize_results æ˜¯å¦ä¸º Noneï¼ˆå¯èƒ½æ˜¯ä¸Šæ¸¸è¢«ä¸­æ–­ï¼‰
    if vectorize_results is None:
        logger.warning("vectorize_results ä¸º Noneï¼Œå¯èƒ½æ˜¯ä¸Šæ¸¸æ­¥éª¤è¢«ä¸­æ–­ï¼Œè·³è¿‡éªŒè¯")
        return {
            "success": False,
            "validation_passed": False,
            "error_message": "vectorize_results ä¸º Noneï¼ˆå¯èƒ½è¢«ä¸­æ–­ï¼‰",
        }
    
    if not vectorize_results.get("success"):
        logger.warning("å‘é‡åŒ–ä½œä¸šå¤±è´¥ï¼Œè·³è¿‡éªŒè¯")
        return {
            "success": False,
            "validation_passed": False,
        }
    
    vectorized_count = vectorize_results.get("vectorized_count", 0)
    failed_count = vectorize_results.get("failed_count", 0)
    total_chunks = vectorize_results.get("total_chunks", 0)
    
    # è®¡ç®—æˆåŠŸç‡
    success_rate = vectorized_count / total_chunks if total_chunks > 0 else 0
    
    logger.info(f"å‘é‡åŒ–ç»“æœéªŒè¯:")
    logger.info(f"  æ€»åˆ†å—æ•°: {total_chunks}")
    logger.info(f"  æˆåŠŸå‘é‡åŒ–: {vectorized_count}")
    logger.info(f"  å‘é‡åŒ–å¤±è´¥: {failed_count}")
    logger.info(f"  æˆåŠŸç‡: {success_rate:.2%}")
    
    # éªŒè¯è§„åˆ™ï¼šæˆåŠŸç‡ >= 80% è®¤ä¸ºé€šè¿‡
    validation_passed = success_rate >= 0.8 if total_chunks > 0 else True
    
    if not validation_passed:
        logger.warning(f"âš ï¸ å‘é‡åŒ–æˆåŠŸç‡ {success_rate:.2%} ä½äºé˜ˆå€¼ 80%")
    
    # è®°å½•å¤±è´¥åˆ†å—ï¼ˆå¦‚æœæœ‰ï¼‰
    failed_chunks = vectorize_results.get("failed_chunks", [])
    if failed_chunks:
        logger.warning(f"å¤±è´¥åˆ†å—åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰:")
        for chunk_id in failed_chunks:
            logger.warning(f"  - chunk_id={chunk_id}")

    # è®°å½•æ±‡æ€»çš„ AssetMaterialization äº‹ä»¶ï¼ˆè´¨é‡æŒ‡æ ‡ï¼‰
    context.log_event(
        AssetMaterialization(
            asset_key=["quality_metrics", "vectorize_validation"],
            description=f"å‘é‡åŒ–æ•°æ®è´¨é‡æ£€æŸ¥: é€šè¿‡ç‡ {success_rate:.2%}",
            metadata={
                "total": MetadataValue.int(total_chunks),
                "vectorized": MetadataValue.int(vectorized_count),
                "failed": MetadataValue.int(failed_count),
                "success_rate": MetadataValue.float(success_rate),
                "validation_passed": MetadataValue.bool(validation_passed),
            }
        )
    )

    return {
        "success": True,
        "validation_passed": validation_passed,
        "total_chunks": total_chunks,
        "vectorized_count": vectorized_count,
        "failed_count": failed_count,
        "success_rate": success_rate,
        "failed_chunks_count": len(failed_chunks),
    }


# ==================== Dagster Jobs ====================

@job(
    config={
        "ops": {
            "scan_unvectorized_chunks_op": {
                "config": {
                    "batch_size": 50,
                    # limit ä¸è®¾ç½®è¡¨ç¤ºå¤„ç†å…¨éƒ¨æœªå‘é‡åŒ–çš„åˆ†å—
                    # market å’Œ doc_type æ˜¯å¯é€‰çš„ï¼Œä¸è®¾ç½®è¡¨ç¤ºæ‰€æœ‰ç±»å‹
                }
            },
            "vectorize_chunks_op": {
                "config": {
                    "force_revectorize": False,
                }
            }
        }
    },
    description="å‘é‡åŒ–ä½œä¸š - é»˜è®¤é…ç½®"
)
def vectorize_documents_job():
    """
    å‘é‡åŒ–ä½œä¸š

    å®Œæ•´æµç¨‹ï¼š
    1. æ‰«ææœªå‘é‡åŒ–åˆ†å—ï¼ˆvectorized_at IS NULLï¼‰
    2. æ‰¹é‡æ‰§è¡Œå‘é‡åŒ–ï¼ˆè°ƒç”¨ Vectorizerï¼‰
    3. éªŒè¯å‘é‡åŒ–ç»“æœ

    é»˜è®¤é…ç½®ï¼š
    - scan_unvectorized_chunks_op:
        - batch_size: 32 (æ¯æ‰¹å¤„ç†32ä¸ªåˆ†å—)
        - limit: ä¸è®¾ç½® (å¤„ç†å…¨éƒ¨æœªå‘é‡åŒ–çš„åˆ†å—)
    - vectorize_chunks_op:
        - force_revectorize: False (ä¸å¼ºåˆ¶é‡æ–°å‘é‡åŒ–)
    """
    scan_result = scan_unvectorized_chunks_op()
    vectorize_results = vectorize_chunks_op(scan_result)
    validate_vectorize_results_op(vectorize_results)


# ==================== Schedules ====================

@schedule(
    job=vectorize_documents_job,
    cron_schedule="0 */2 * * *",  # æ¯2å°æ—¶æ‰§è¡Œä¸€æ¬¡
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢ï¼Œéœ€è¦æ‰‹åŠ¨å¯ç”¨
)
def hourly_vectorize_schedule(context):
    """
    æ¯2å°æ—¶å®šæ—¶å‘é‡åŒ–ä½œä¸š
    """
    return RunRequest()


@schedule(
    job=vectorize_documents_job,
    cron_schedule="0 6 * * *",  # æ¯å¤©å‡Œæ™¨6ç‚¹æ‰§è¡Œï¼ˆåˆ†å—å®Œæˆåï¼‰
    default_status=DefaultScheduleStatus.STOPPED,  # é»˜è®¤åœæ­¢
)
def daily_vectorize_schedule(context):
    """
    æ¯æ—¥å®šæ—¶å‘é‡åŒ–ä½œä¸šï¼ˆåœ¨åˆ†å—ä½œä¸šä¹‹åæ‰§è¡Œï¼‰
    """
    return RunRequest()


# ==================== Sensors ====================

@sensor(
    job=vectorize_documents_job,
    default_status=DefaultSensorStatus.STOPPED,
)
def manual_trigger_vectorize_sensor(context):
    """
    æ‰‹åŠ¨è§¦å‘å‘é‡åŒ–ä¼ æ„Ÿå™¨
    å¯ä»¥é€šè¿‡ Dagster UI æ‰‹åŠ¨è§¦å‘
    """
    return RunRequest()
