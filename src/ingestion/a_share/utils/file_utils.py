# -*- coding: utf-8 -*-
"""
æ–‡ä»¶æ“ä½œå·¥å…·å‡½æ•°
"""

import os
import json
import csv
import time
import glob
import platform
import logging
from typing import Optional, List, Tuple

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from ..config import (
    CNINFO_STATIC, PROXIES, RETRY_TIMES, RETRY_STATUS, RETRY_BACKOFF,
    CSV_ENCODING, HEADERS_API
)
from .code_utils import normalize_code

logger = logging.getLogger(__name__)


def make_session(headers) -> requests.Session:
    """
    åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ HTTP Session
    
    Args:
        headers: HTTP è¯·æ±‚å¤´
        
    Returns:
        é…ç½®å¥½çš„ requests.Session
    """
    s = requests.Session()
    s.headers.update(headers)
    retry = Retry(
        total=RETRY_TIMES,
        status_forcelist=list(RETRY_STATUS),
        allowed_methods=frozenset(["GET", "POST"]),
        backoff_factor=RETRY_BACKOFF,
        raise_on_status=False
    )
    adapter = HTTPAdapter(max_retries=retry)
    s.mount("http://", adapter)
    s.mount("https://", adapter)

    # å…³é”®ï¼šè®¿é—®ä¸»é¡µå»ºç«‹ä¼šè¯ï¼Œè·å– JSESSIONID Cookie
    try:
        s.get("https://www.cninfo.com.cn/", timeout=10, proxies=PROXIES)
        logger.debug("ä¼šè¯å·²å»ºç«‹ï¼ˆè·å– JSESSIONIDï¼‰")
    except Exception as e:
        logger.warning(f"å»ºç«‹ä¼šè¯å¤±è´¥: {e}")

    return s


def ensure_dir(p: str):
    """
    ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
    
    Args:
        p: ç›®å½•è·¯å¾„
    """
    os.makedirs(p, exist_ok=True)


def pdf_url_from_adj(adj: str) -> str:
    """
    ä»é™„ä»¶URLæ„å»ºå®Œæ•´çš„PDF URL
    
    Args:
        adj: é™„ä»¶URLï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        
    Returns:
        å®Œæ•´çš„PDF URL
    """
    return CNINFO_STATIC + (adj or "").lstrip("/")


def load_json(path, default):
    """
    åŠ è½½JSONæ–‡ä»¶
    
    Args:
        path: æ–‡ä»¶è·¯å¾„
        default: é»˜è®¤å€¼ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥æ—¶è¿”å›ï¼‰
        
    Returns:
        è§£æåçš„JSONå¯¹è±¡æˆ–é»˜è®¤å€¼
    """
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default


def save_json(path, obj):
    """
    ä¿å­˜JSONæ–‡ä»¶ï¼Œå¸¦é‡è¯•æœºåˆ¶ï¼ˆè§£å†³Windowsæ–‡ä»¶é”å®šé—®é¢˜ï¼‰
    
    Args:
        path: æ–‡ä»¶è·¯å¾„
        obj: è¦ä¿å­˜çš„å¯¹è±¡
    """
    tmp = path + ".tmp"
    max_retries = 5
    retry_delay = 0.1

    for attempt in range(max_retries):
        try:
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with open(tmp, "w", encoding="utf-8") as f:
                json.dump(obj, f, ensure_ascii=False, indent=2)
            # ç¡®ä¿æ–‡ä»¶å·²å…³é—­
            time.sleep(0.01)

            # åŸå­æ›¿æ¢ï¼ˆWindowsä¸Šå¯èƒ½å¤±è´¥ï¼Œéœ€è¦é‡è¯•ï¼‰
            if os.path.exists(path):
                # Windows: å…ˆåˆ é™¤ç›®æ ‡æ–‡ä»¶ï¼Œå†é‡å‘½å
                if platform.system().lower().startswith("win"):
                    try:
                        os.remove(path)
                        time.sleep(0.02)  # ç­‰å¾…æ–‡ä»¶å¥æŸ„é‡Šæ”¾
                    except (PermissionError, OSError):
                        if attempt < max_retries - 1:
                            time.sleep(retry_delay * (2 ** attempt))
                            continue
                        raise
            os.rename(tmp, path)
            return
        except (PermissionError, OSError) as e:
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (2 ** attempt))
                continue
            # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•é”™è¯¯
            logger.warning(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥ï¼ˆ{path}ï¼‰: {e}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp):
                try:
                    os.remove(tmp)
                except:
                    pass
            raise


def read_tasks_from_csv(path: str, default_year: Optional[int] = None, default_quarter: Optional[str] = None):
    """
    è¯»å– CSV ä»»åŠ¡æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç 
    å°è¯•é¡ºåº: UTF-8-sig -> UTF-8 -> GBK (Windows) -> GB2312
    
    Args:
        path: CSVæ–‡ä»¶è·¯å¾„
        default_year: é»˜è®¤å¹´ä»½ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰yearåˆ—ï¼‰
        default_quarter: é»˜è®¤å­£åº¦ï¼ˆå¦‚æœCSVä¸­æ²¡æœ‰quarteråˆ—ï¼‰
        
    Returns:
        ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡ä¸º (code, name, year, quarter) å…ƒç»„
    """
    encodings = ["utf-8-sig", "utf-8"]
    if platform.system().lower().startswith("win"):
        encodings.extend(["gbk", "gb2312"])

    for enc in encodings:
        try:
            with open(path, "r", encoding=enc, newline="", errors="replace") as f:
                reader = csv.DictReader(f)
                tasks = []
                for row in reader:
                    raw_code = row.get("code", "").strip()
                    name = row.get("name", "").strip()
                    year_str = row.get("year", "").strip()
                    quarter = row.get("quarter", "").strip()

                    if not raw_code or not name:
                        continue

                    # è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ "1" -> "000001"ï¼‰
                    code = normalize_code(raw_code)
                    if not code:
                        logger.warning(f"æ— æ³•è§„èŒƒåŒ–è‚¡ç¥¨ä»£ç : {raw_code}ï¼Œè·³è¿‡")
                        continue

                    year = int(year_str) if year_str else default_year
                    if not year:
                        continue

                    if not quarter:
                        quarter = default_quarter or "Q4"

                    tasks.append((code, name, year, quarter))

                return tasks
        except UnicodeDecodeError:
            continue
        except Exception as e:
            logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥ ({enc}): {e}")
            break

    return []


def build_existing_pdf_cache(old_pdf_dir: Optional[str]) -> set:
    """
    æ‰«ææ—§PDFç›®å½•ï¼Œæ„å»ºå·²å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜ï¼ˆä¸€æ¬¡æ€§æ‰«æï¼‰

    Args:
        old_pdf_dir: æ—§PDFç›®å½•è·¯å¾„

    Returns:
        set of (code, year, quarter) tuples
    """
    if not old_pdf_dir or not os.path.exists(old_pdf_dir):
        return set()

    cache = set()
    print(f"ğŸ” æ‰«ææ—§PDFç›®å½•ï¼š{old_pdf_dir}")

    # æ‰«ææ‰€æœ‰äº¤æ˜“æ‰€ç›®å½•
    for exchange in ["SZ", "SH", "BJ"]:
        exchange_dir = os.path.join(old_pdf_dir, exchange)
        if not os.path.exists(exchange_dir):
            continue

        # ä½¿ç”¨globé€’å½’æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pattern = os.path.join(exchange_dir, "**", "*.pdf")
        pdf_files = glob.glob(pattern, recursive=True)

        for pdf_path in pdf_files:
            # è§£ææ–‡ä»¶åï¼šcode_year_quarter_date.pdf
            filename = os.path.basename(pdf_path)
            parts = filename.replace(".pdf", "").split("_")

            if len(parts) >= 3:
                code_raw = parts[0]
                year_str = parts[1]
                quarter = parts[2]

                try:
                    year = int(year_str)
                    code = normalize_code(code_raw)  # æ ‡å‡†åŒ–ä»£ç ï¼Œç¡®ä¿åŒ¹é…
                    cache.add((code, year, quarter))
                except ValueError:
                    continue

    print(f"âœ… æ‰¾åˆ° {len(cache)} ä¸ªå·²å­˜åœ¨çš„æŠ¥å‘Š")
    return cache


def check_pdf_exists_in_cache(cache: set, code: str, year: int, quarter: str) -> bool:
    """
    æ£€æŸ¥PDFæ˜¯å¦åœ¨ç¼“å­˜ä¸­ï¼ˆO(1)æŸ¥æ‰¾ï¼Œæ— I/Oï¼‰

    Args:
        cache: å·²å­˜åœ¨æ–‡ä»¶çš„ç¼“å­˜é›†åˆ
        code: è‚¡ç¥¨ä»£ç 
        year: å¹´ä»½
        quarter: å­£åº¦

    Returns:
        True å¦‚æœæ–‡ä»¶å­˜åœ¨äºç¼“å­˜ä¸­
    """
    return (code, year, quarter) in cache
