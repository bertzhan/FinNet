# -*- coding: utf-8 -*-
"""
Aè‚¡å®šæœŸæŠ¥å‘Šçˆ¬è™«å®ç°ï¼ˆCNINFOï¼‰
ç»§æ‰¿è‡ª CninfoBaseCrawlerï¼Œé›†æˆ storage å±‚
"""
import os
import tempfile
import csv
import multiprocessing
import queue
import threading
import time
from typing import List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor

# æ”¯æŒç›´æ¥è¿è¡Œå’Œä½œä¸ºæ¨¡å—å¯¼å…¥
try:
    from .base_cninfo_crawler import CninfoBaseCrawler
except ImportError:
    from src.ingestion.a_share.crawlers.base_cninfo_crawler import CninfoBaseCrawler

from src.ingestion.base.base_crawler import CrawlTask, CrawlResult
from src.common.constants import Market, DocType
from src.common.logger import get_logger

logger = get_logger(__name__)


class ReportCrawler(CninfoBaseCrawler):
    """
    Aè‚¡å®šæœŸæŠ¥å‘Šçˆ¬è™«å®ç°ï¼ˆCNINFO å·¨æ½®èµ„è®¯ç½‘ï¼‰
    
    ä½¿ç”¨æœ¬åœ°æ¨¡å—åŒ–çš„ä¸‹è½½é€»è¾‘
    å®ç° _download_file() æ–¹æ³•ï¼Œå…¶ä½™ç”±åŸºç±»è‡ªåŠ¨å¤„ç†
    """

    def __init__(
        self,
        enable_minio: bool = True,
        enable_postgres: bool = True,
        workers: int = 1
    ):
        """
        Args:
            enable_minio: æ˜¯å¦å¯ç”¨ MinIO ä¸Šä¼ 
            enable_postgres: æ˜¯å¦å¯ç”¨ PostgreSQL è®°å½•
            workers: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ1 è¡¨ç¤ºå•çº¿ç¨‹ï¼‰
        """
        super().__init__(
            market=Market.A_SHARE,
            enable_minio=enable_minio,
            enable_postgres=enable_postgres
        )
        self.workers = workers

    def _download_file(self, task: CrawlTask) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        ä¸‹è½½æ–‡ä»¶ï¼ˆé€šè¿‡è°ƒç”¨ç°æœ‰çˆ¬è™«é€»è¾‘ï¼‰

        Args:
            task: çˆ¬å–ä»»åŠ¡

        Returns:
            (æ˜¯å¦æˆåŠŸ, æœ¬åœ°æ–‡ä»¶è·¯å¾„, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # å¯¼å…¥ä»»åŠ¡å¤„ç†å™¨æ¨¡å—
            try:
                from ..processor.report_processor import process_single_task
            except ImportError:
                from src.ingestion.a_share.processor.report_processor import process_single_task

            # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºä¸‹è½½
            temp_dir = tempfile.mkdtemp(prefix='cninfo_download_')

            # å‡†å¤‡ä»»åŠ¡æ•°æ®
            quarter_str = f"Q{task.quarter}" if task.quarter else "Q4"
            shared_lock = multiprocessing.Lock()

            # orgIdç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨åŸºç±»æ–¹æ³•ï¼‰
            orgid_cache_file = self._get_cache_file_path('orgid_cache.json')
            code_change_cache_file = self._get_cache_file_path('code_change_cache.json')

            existing_pdf_cache = set()

            task_data = (
                task.stock_code,
                task.company_name,
                task.year,
                quarter_str,
                temp_dir,
                orgid_cache_file,
                code_change_cache_file,
                shared_lock,
                existing_pdf_cache
            )

            # è°ƒç”¨ç°æœ‰çˆ¬è™«é€»è¾‘
            success, failure_record = process_single_task(task_data)

            if not success:
                error_msg = failure_record[4] if failure_record and len(failure_record) > 4 else "ä¸‹è½½å¤±è´¥"
                self.logger.error(f"ä¸‹è½½å¤±è´¥: {task.stock_code} {task.year} Q{task.quarter} - {error_msg}")
                return False, None, error_msg

            # æŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶
            expected_path = self._get_expected_file_path(temp_dir, task)
            self.logger.debug(f"æœŸæœ›æ–‡ä»¶è·¯å¾„: {expected_path}")
            file_path = self._find_downloaded_file(temp_dir, task)

            if not file_path:
                # æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå¯èƒ½æ˜¯ä»¥ä¸‹æƒ…å†µï¼š
                # 1. checkpoint å·²å­˜åœ¨ï¼ˆè·³è¿‡ä¸‹è½½ï¼Œä½†æ–‡ä»¶ä¸åœ¨ temp_dirï¼‰
                # 2. ä¸‹è½½å¤±è´¥ä½†è¿”å›äº†æˆåŠŸï¼ˆå¼‚å¸¸æƒ…å†µï¼‰
                
                # æ£€æŸ¥ä¸´æ—¶ç›®å½•ä¸­æ˜¯å¦æœ‰ä»»ä½•æ–‡ä»¶
                temp_files = []
                if os.path.exists(temp_dir):
                    for root, dirs, files in os.walk(temp_dir):
                        for f in files:
                            if f.endswith('.pdf'):
                                temp_files.append(os.path.join(root, f))
                
                self.logger.warning(
                    f"æ–‡ä»¶æœªæ‰¾åˆ°: {task.stock_code} {task.year} Q{task.quarter}\n"
                    f"  æœŸæœ›è·¯å¾„: {expected_path}\n"
                    f"  ä¸´æ—¶ç›®å½•: {temp_dir}\n"
                    f"  ä¸´æ—¶ç›®å½•ä¸­çš„PDFæ–‡ä»¶æ•°: {len(temp_files)}\n"
                    f"  å‰3ä¸ªPDFæ–‡ä»¶: {temp_files[:3]}"
                )
                
                # æ£€æŸ¥ checkpoint
                from ..state.shared_state import SharedState
                checkpoint_file = os.path.join(temp_dir, "checkpoint.json")
                shared = SharedState(checkpoint_file, orgid_cache_file, code_change_cache_file, shared_lock)
                checkpoint = shared.load_checkpoint()
                key = f"{task.stock_code}-{task.year}-{quarter_str}"
                
                if checkpoint.get(key):
                    # checkpoint å­˜åœ¨ä½†æ–‡ä»¶ä¸åœ¨ temp_dirï¼Œè¯´æ˜æ˜¯ä¹‹å‰è¿è¡Œç•™ä¸‹çš„ checkpoint
                    # åˆ é™¤ checkpointï¼Œå¼ºåˆ¶é‡æ–°ä¸‹è½½
                    self.logger.warning(f"checkpointå­˜åœ¨ä½†æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œåˆ é™¤checkpointå¹¶é‡æ–°ä¸‹è½½: {task.stock_code} {task.year} Q{task.quarter}")
                    shared.remove_checkpoint(key)
                    # é‡æ–°è°ƒç”¨ä¸‹è½½
                    success, failure_record = process_single_task(task_data)
                    if not success:
                        error_msg = failure_record[4] if failure_record and len(failure_record) > 4 else "ä¸‹è½½å¤±è´¥"
                        self.logger.error(f"é‡æ–°ä¸‹è½½å¤±è´¥: {task.stock_code} {task.year} Q{task.quarter} - {error_msg}")
                        return False, None, error_msg
                    # å†æ¬¡æŸ¥æ‰¾æ–‡ä»¶
                    file_path = self._find_downloaded_file(temp_dir, task)
                    if not file_path:
                        error_msg = "é‡æ–°ä¸‹è½½åæ–‡ä»¶ä»æœªæ‰¾åˆ°"
                        self.logger.error(f"{error_msg}: {task.stock_code} {task.year} Q{task.quarter}")
                        return False, None, error_msg
                else:
                    # æ²¡æœ‰ checkpointï¼Œè¯´æ˜ä¸‹è½½å¤±è´¥
                    error_msg = "æ–‡ä»¶ä¸‹è½½å¤±è´¥æˆ–æœªæ‰¾åˆ°"
                    self.logger.error(
                        f"{error_msg}: {task.stock_code} {task.year} Q{task.quarter}\n"
                        f"  process_single_task è¿”å›äº†æˆåŠŸï¼Œä½†æ–‡ä»¶ä¸å­˜åœ¨\n"
                        f"  å¯èƒ½åŸå› ï¼š1) ä¸‹è½½å‡½æ•°è¿”å›æˆåŠŸä½†å®é™…æœªä¿å­˜æ–‡ä»¶ 2) æ–‡ä»¶ä¿å­˜è·¯å¾„ä¸åŒ¹é…"
                    )
                    return False, None, error_msg

            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {file_path}")
            return True, file_path, None

        except Exception as e:
            error_msg = f"ä¸‹è½½å¼‚å¸¸: {e}"
            self.logger.error(f"{error_msg}: {task.stock_code} {task.year} Q{task.quarter}", exc_info=True)
            return False, None, error_msg

    def _get_expected_file_path(self, output_root: str, task: CrawlTask) -> str:
        """
        è·å–æœŸæœ›çš„æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºè°ƒè¯•ï¼‰

        Args:
            output_root: è¾“å‡ºæ ¹ç›®å½•
            task: ä»»åŠ¡

        Returns:
            æœŸæœ›çš„æ–‡ä»¶è·¯å¾„
        """
        from src.storage.object_store.path_manager import PathManager
        from src.common.constants import Market, DocType
        
        quarter_num = task.quarter if task.quarter else 4
        
        if quarter_num == 4:
            doc_type = DocType.ANNUAL_REPORT
        elif quarter_num == 2:
            doc_type = DocType.INTERIM_REPORT
        else:
            doc_type = DocType.QUARTERLY_REPORT
        
        filename = "document.pdf"
        
        path_manager = PathManager()
        quarter_for_path = quarter_num if quarter_num not in [2, 4] else None
        minio_path = path_manager.get_bronze_path(
            market=Market.A_SHARE,
            doc_type=doc_type,
            stock_code=task.stock_code,
            year=task.year,
            quarter=quarter_for_path,
            filename=filename
        )
        
        return os.path.join(output_root, minio_path)

    def _find_downloaded_file(self, output_root: str, task: CrawlTask) -> Optional[str]:
        """
        æŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶ï¼ˆä½¿ç”¨ä¸MinIOä¸€è‡´çš„è·¯å¾„ç»“æ„ï¼‰

        Args:
            output_root: è¾“å‡ºæ ¹ç›®å½•
            task: ä»»åŠ¡

        Returns:
            æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        from src.storage.object_store.path_manager import PathManager
        from src.common.constants import Market, DocType
        
        # ä½¿ç”¨ PathManager ç”Ÿæˆä¸ MinIO ä¸€è‡´çš„è·¯å¾„ç»“æ„
        quarter_num = task.quarter if task.quarter else 4
        
        # æ ¹æ®å­£åº¦ç¡®å®šæ–‡æ¡£ç±»å‹
        if quarter_num == 4:
            doc_type = DocType.ANNUAL_REPORT
        elif quarter_num == 2:
            doc_type = DocType.INTERIM_REPORT
        else:
            doc_type = DocType.QUARTERLY_REPORT
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = "document.pdf"
        
        # ä½¿ç”¨ PathManager ç”Ÿæˆè·¯å¾„
        path_manager = PathManager()
        quarter_for_path = quarter_num if quarter_num not in [2, 4] else None
        minio_path = path_manager.get_bronze_path(
            market=Market.A_SHARE,
            doc_type=doc_type,
            stock_code=task.stock_code,
            year=task.year,
            quarter=quarter_for_path,
            filename=filename
        )
        
        # è½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè·¯å¾„
        file_path = os.path.join(output_root, minio_path)
        
        if os.path.exists(file_path):
            return file_path
        
        # å…¼å®¹æ—§æ ¼å¼ï¼šå°è¯•æŸ¥æ‰¾æ—§è·¯å¾„ç»“æ„
        exchanges = ['SZ', 'SH', 'BJ']
        for exchange in exchanges:
            code_dir = os.path.join(output_root, exchange, task.stock_code, str(task.year))
            if not os.path.exists(code_dir):
                continue
            
            old_file_path = os.path.join(code_dir, filename)
            if os.path.exists(old_file_path):
                return old_file_path
            
            # æ—§æ ¼å¼ï¼šå…¼å®¹æ—§æ–‡ä»¶åæ ¼å¼ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            import glob
            # å°è¯•æŸ¥æ‰¾æ—§æ ¼å¼çš„æ–‡ä»¶å
            old_pattern = f"{task.stock_code}_{task.year}_*.pdf"
            files = glob.glob(os.path.join(code_dir, old_pattern))
            if files:
                return files[0]

        return None

    def crawl_batch(self, tasks: List[CrawlTask]) -> List[CrawlResult]:
        """
        æ‰¹é‡çˆ¬å–

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            ç»“æœåˆ—è¡¨
        """
        if not tasks:
            return []

        # å¦‚æœåªæœ‰ä¸€ä¸ªä»»åŠ¡æˆ–ä¸å¯ç”¨å¤šè¿›ç¨‹ï¼Œä½¿ç”¨åŸºç±»çš„å•ä»»åŠ¡é€»è¾‘
        if len(tasks) == 1 or self.workers <= 1:
            return super().crawl_batch(tasks)

        # å¤šä»»åŠ¡ + å¤šè¿›ç¨‹ï¼šä½¿ç”¨ç°æœ‰çš„ run_multiprocessing
        return self._crawl_batch_multiprocessing(tasks)

    def _crawl_batch_multiprocessing(self, tasks: List[CrawlTask]) -> List[CrawlResult]:
        """
        ä½¿ç”¨å¤šè¿›ç¨‹æ‰¹é‡çˆ¬å– + ä¸»çº¿ç¨‹å¼‚æ­¥ä¸Šä¼ 
        
        ä¼˜åŒ–ï¼šä¸‹è½½å’Œä¸Šä¼ å¹¶è¡Œè¿›è¡Œï¼Œè€Œä¸æ˜¯ä¸²è¡Œç­‰å¾…
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            ç»“æœåˆ—è¡¨
        """
        try:
            # å¯¼å…¥ä»»åŠ¡å¤„ç†å™¨æ¨¡å—
            try:
                from ..processor.report_processor import run_multiprocessing
            except ImportError:
                from src.ingestion.a_share.processor.report_processor import run_multiprocessing

            # 0. æ£€æŸ¥æ•°æ®åº“ï¼Œè¿‡æ»¤å·²å­˜åœ¨çš„ä»»åŠ¡ï¼ˆé¿å…é‡å¤çˆ¬å–ï¼‰
            tasks_to_crawl = []
            skipped_results = {}  # {task_key: CrawlResult}
            
            if self.enable_postgres and self.pg_client:
                try:
                    from src.storage.metadata import crud
                    with self.pg_client.get_session() as session:
                        for task in tasks:
                            quarter_str = f"Q{task.quarter}" if task.quarter else "Q4"
                            task_key = (task.stock_code, task.year, quarter_str)
                            
                            existing_doc = crud.get_document_by_task(
                                session=session,
                                stock_code=task.stock_code,
                                market=task.market.value,
                                doc_type=task.doc_type.value,
                                year=task.year,
                                quarter=task.quarter
                            )
                            
                            if existing_doc:
                                # æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
                                self.logger.info(
                                    f"âœ… æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {task.stock_code} {task.year} Q{task.quarter} "
                                    f"(id={existing_doc.id}, path={existing_doc.minio_object_path})"
                                )
                                skipped_results[task_key] = CrawlResult(
                                    task=task,
                                    success=True,
                                    minio_object_path=existing_doc.minio_object_path,
                                    document_id=existing_doc.id,
                                    file_size=existing_doc.file_size,
                                    file_hash=existing_doc.file_hash,
                                    metadata=task.metadata
                                )
                            else:
                                # éœ€è¦çˆ¬å–çš„ä»»åŠ¡
                                tasks_to_crawl.append(task)
                except Exception as e:
                    # æ£€æŸ¥å¤±è´¥ä¸å½±å“çˆ¬å–æµç¨‹ï¼Œè®°å½•è­¦å‘Šåç»§ç»­
                    self.logger.warning(f"âš ï¸ æ£€æŸ¥æ•°æ®åº“æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œç»§ç»­çˆ¬å–æ‰€æœ‰ä»»åŠ¡: {e}")
                    tasks_to_crawl = tasks
            else:
                # æœªå¯ç”¨ PostgreSQLï¼Œçˆ¬å–æ‰€æœ‰ä»»åŠ¡
                tasks_to_crawl = tasks
            
            if not tasks_to_crawl:
                # æ‰€æœ‰ä»»åŠ¡éƒ½å·²å­˜åœ¨
                self.logger.info(f"âœ… æ‰€æœ‰ {len(tasks)} ä¸ªä»»åŠ¡éƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–")
                return list(skipped_results.values())
            
            if len(skipped_results) > 0:
                self.logger.info(f"è·³è¿‡ {len(skipped_results)} ä¸ªå·²å­˜åœ¨çš„ä»»åŠ¡ï¼Œå‰©ä½™ {len(tasks_to_crawl)} ä¸ªä»»åŠ¡éœ€è¦çˆ¬å–")

            # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶ï¼ˆåªåŒ…å«éœ€è¦çˆ¬å–çš„ä»»åŠ¡ï¼‰
            temp_csv = self._create_temp_csv(
                tasks_to_crawl,
                ['code', 'name', 'year', 'quarter'],
                lambda task: [
                    task.stock_code,
                    task.company_name,
                    task.year,
                    f"Q{task.quarter}" if task.quarter else "Q4"
                ]
            )

            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•å’Œå¤±è´¥è®°å½•æ–‡ä»¶
            temp_output = tempfile.mkdtemp(prefix='cninfo_batch_')
            temp_fail_csv = tempfile.NamedTemporaryFile(
                mode='w', suffix='.csv', delete=False, encoding='utf-8-sig', newline=''
            ).name

            # åˆ›å»ºä¸Šä¼ é˜Ÿåˆ—å’Œç»“æœå­—å…¸
            upload_queue = queue.Queue()
            upload_results = {}  # {task_key: CrawlResult}
            upload_lock = threading.Lock()
            processed_files = set()  # å·²å¤„ç†çš„æ–‡ä»¶è·¯å¾„é›†åˆ
            
            # åˆ›å»ºä¸Šä¼ çº¿ç¨‹æ± ï¼ˆä½¿ç”¨è¾ƒå°‘çš„çº¿ç¨‹ï¼Œé¿å…è¿‡åº¦å ç”¨èµ„æºï¼‰
            upload_workers = min(4, max(1, len(tasks_to_crawl) // 10))  # æ ¹æ®ä»»åŠ¡æ•°é‡åŠ¨æ€è°ƒæ•´
            self.logger.info(f"å¯åŠ¨ {upload_workers} ä¸ªä¸Šä¼ çº¿ç¨‹")
            
            upload_executor = ThreadPoolExecutor(max_workers=upload_workers)
            upload_futures = {}  # {task_key: future}
            
            def upload_file(task: CrawlTask, file_path: str):
                """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
                try:
                    self.logger.debug(f"å¼€å§‹ä¸Šä¼ : {task.stock_code} {task.year} Q{task.quarter}")
                    result = self._process_downloaded_file(file_path, task)
                    return result
                except Exception as e:
                    self.logger.error(f"ä¸Šä¼ å¼‚å¸¸: {task.stock_code} {task.year} Q{task.quarter} - {e}", exc_info=True)
                    return CrawlResult(
                        task=task,
                        success=False,
                        error_message=f"ä¸Šä¼ å¼‚å¸¸: {e}"
                    )
            
            try:
                # å¯åŠ¨ä¸‹è½½ï¼ˆåœ¨åå°çº¿ç¨‹ï¼‰
                download_exception = []
                download_thread = threading.Thread(
                    target=lambda: self._run_download_with_exception_handling(
                        run_multiprocessing,
                        temp_csv,
                        temp_output,
                        temp_fail_csv,
                        download_exception
                    ),
                    daemon=False
                )
                download_thread.start()
                self.logger.info(f"ä¸‹è½½çº¿ç¨‹å·²å¯åŠ¨ï¼Œworkers={self.workers}")
                
                # ä¸»çº¿ç¨‹è½®è¯¢æ£€æŸ¥ä¸‹è½½å®Œæˆçš„æ–‡ä»¶å¹¶å¼‚æ­¥ä¸Šä¼ 
                poll_interval = 0.5  # è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
                last_file_count = 0
                no_new_file_count = 0
                max_no_new_file_count = 10  # è¿ç»­10æ¬¡æ²¡æœ‰æ–°æ–‡ä»¶ï¼Œè®¤ä¸ºä¸‹è½½å®Œæˆ
                
                while download_thread.is_alive() or no_new_file_count < max_no_new_file_count:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹è½½å¼‚å¸¸
                    if download_exception:
                        raise download_exception[0]
                    
                    # æ£€æŸ¥æ–°ä¸‹è½½çš„æ–‡ä»¶
                    current_file_count = 0
                    for task in tasks_to_crawl:
                        quarter_str = f"Q{task.quarter}" if task.quarter else "Q4"
                        task_key = (task.stock_code, task.year, quarter_str)
                        
                        # è·³è¿‡å·²å¤„ç†çš„ä»»åŠ¡
                        if task_key in upload_results or task_key in upload_futures:
                            current_file_count += 1
                            continue
                        
                        # æŸ¥æ‰¾æ–‡ä»¶
                        file_path = self._find_downloaded_file(temp_output, task)
                        if file_path and os.path.exists(file_path) and file_path not in processed_files:
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸‹è½½å®Œæˆï¼ˆæ–‡ä»¶å¤§å°ç¨³å®šï¼‰
                            if self._is_file_stable(file_path):
                                # æ–‡ä»¶å·²ä¸‹è½½å®Œæˆï¼Œæäº¤åˆ°ä¸Šä¼ çº¿ç¨‹æ± 
                                processed_files.add(file_path)
                                future = upload_executor.submit(upload_file, task, file_path)
                                upload_futures[task_key] = future
                                current_file_count += 1
                                self.logger.debug(f"å‘ç°æ–°æ–‡ä»¶ï¼Œå·²æäº¤ä¸Šä¼ : {task.stock_code} {task.year} Q{task.quarter}")
                            # å¦‚æœæ–‡ä»¶ä¸ç¨³å®šï¼Œç­‰å¾…ä¸‹æ¬¡è½®è¯¢å†æ£€æŸ¥
                    
                    # æ£€æŸ¥å·²å®Œæˆçš„ä¸Šä¼ 
                    completed_futures = []
                    for task_key, future in upload_futures.items():
                        if future.done():
                            try:
                                result = future.result()
                                with upload_lock:
                                    upload_results[task_key] = result
                                completed_futures.append(task_key)
                                if result.success:
                                    self.logger.debug(f"ä¸Šä¼ å®Œæˆ: {task_key}")
                                else:
                                    self.logger.warning(f"ä¸Šä¼ å¤±è´¥: {task_key} - {result.error_message}")
                            except Exception as e:
                                self.logger.error(f"è·å–ä¸Šä¼ ç»“æœå¼‚å¸¸: {task_key} - {e}", exc_info=True)
                                with upload_lock:
                                    upload_results[task_key] = CrawlResult(
                                        task=next(t for t in tasks_to_crawl if (t.stock_code, t.year, f"Q{t.quarter}" if t.quarter else "Q4") == task_key),
                                        success=False,
                                        error_message=f"ä¸Šä¼ å¼‚å¸¸: {e}"
                                    )
                                completed_futures.append(task_key)
                    
                    # æ¸…ç†å·²å®Œæˆçš„ä¸Šä¼ ä»»åŠ¡
                    for task_key in completed_futures:
                        del upload_futures[task_key]
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ä»¶
                    if current_file_count > last_file_count:
                        no_new_file_count = 0
                        last_file_count = current_file_count
                    else:
                        no_new_file_count += 1

                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ä¸ªä»»åŠ¡æˆ–æ¯æ¬¡æœ‰æ–°æ–‡ä»¶æ—¶æ˜¾ç¤ºï¼‰
                    completed_count = len(upload_results)
                    total_count = len(tasks_to_crawl)
                    if current_file_count > last_file_count or completed_count % 10 == 0:
                        progress_pct = completed_count / total_count * 100 if total_count > 0 else 0
                        self.logger.info(
                            f"ğŸ“¦ è¿›åº¦: å·²å®Œæˆ {completed_count}/{total_count} ({progress_pct:.1f}%) | "
                            f"ä¸Šä¼ ä¸­: {len(upload_futures)}"
                        )

                    # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
                    time.sleep(poll_interval)
                
                # ç­‰å¾…ä¸‹è½½çº¿ç¨‹å®Œæˆ
                download_thread.join(timeout=300)  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
                if download_thread.is_alive():
                    self.logger.warning("ä¸‹è½½çº¿ç¨‹è¶…æ—¶ï¼Œç»§ç»­å¤„ç†å·²ä¸‹è½½çš„æ–‡ä»¶")
                
                # ç­‰å¾…æ‰€æœ‰ä¸Šä¼ å®Œæˆ
                self.logger.info(f"ç­‰å¾… {len(upload_futures)} ä¸ªä¸Šä¼ ä»»åŠ¡å®Œæˆ...")
                for task_key, future in upload_futures.items():
                    try:
                        result = future.result(timeout=60)  # æ¯ä¸ªä¸Šä¼ æœ€å¤šç­‰å¾…1åˆ†é’Ÿ
                        with upload_lock:
                            upload_results[task_key] = result
                    except Exception as e:
                        self.logger.error(f"ç­‰å¾…ä¸Šä¼ å®Œæˆå¼‚å¸¸: {task_key} - {e}", exc_info=True)
                        with upload_lock:
                            upload_results[task_key] = CrawlResult(
                                task=next(t for t in tasks_to_crawl if (t.stock_code, t.year, f"Q{t.quarter}" if t.quarter else "Q4") == task_key),
                                success=False,
                                error_message=f"ä¸Šä¼ è¶…æ—¶æˆ–å¼‚å¸¸: {e}"
                            )
                
                # å…³é—­ä¸Šä¼ çº¿ç¨‹æ± 
                upload_executor.shutdown(wait=True)
                
                # è¯»å–å¤±è´¥è®°å½•
                failed_tasks = set()
                if os.path.exists(temp_fail_csv):
                    failed_tasks = self._read_failed_tasks(
                        temp_fail_csv,
                        lambda row: (
                            row.get('code', '').strip(),
                            int(row.get('year', 0)),
                            row.get('quarter', '').strip()
                        )
                    )

                # æ„å»ºç»“æœåˆ—è¡¨ï¼ˆåŒ…å«è·³è¿‡çš„ä»»åŠ¡ï¼‰
                results = []
                for task in tasks:
                    quarter_str = f"Q{task.quarter}" if task.quarter else "Q4"
                    task_key = (task.stock_code, task.year, quarter_str)

                    # å¦‚æœä»»åŠ¡è¢«è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰ï¼Œç›´æ¥æ·»åŠ è·³è¿‡ç»“æœ
                    if task_key in skipped_results:
                        results.append(skipped_results[task_key])
                    elif task_key in failed_tasks:
                        # å¤±è´¥ä»»åŠ¡
                        results.append(CrawlResult(
                            task=task,
                            success=False,
                            error_message="çˆ¬å–å¤±è´¥ï¼Œè¯¦è§å¤±è´¥è®°å½•"
                        ))
                    elif task_key in upload_results:
                        # å·²ä¸Šä¼ çš„ä»»åŠ¡
                        results.append(upload_results[task_key])
                    else:
                        # æœªæ‰¾åˆ°æ–‡ä»¶æˆ–æœªå¤„ç†
                        file_path = self._find_downloaded_file(temp_output, task)
                        if file_path and os.path.exists(file_path):
                            # æ–‡ä»¶å­˜åœ¨ä½†æœªä¸Šä¼ ï¼Œå¯èƒ½æ˜¯è½®è¯¢æ—¶é—æ¼äº†ï¼Œç«‹å³ä¸Šä¼ 
                            self.logger.warning(f"å‘ç°é—æ¼çš„æ–‡ä»¶ï¼Œç«‹å³ä¸Šä¼ : {task.stock_code} {task.year} Q{task.quarter}")
                            temp_result = self._process_downloaded_file(file_path, task)
                            results.append(temp_result)
                        else:
                            results.append(CrawlResult(
                                task=task,
                                success=False,
                                error_message="æ–‡ä»¶æœªæ‰¾åˆ°æˆ–æœªå¤„ç†"
                            ))

                # ç»Ÿè®¡ç»“æœ
                success_count = sum(1 for r in results if r.success)
                upload_success_count = sum(1 for r in results if r.success and r.minio_object_path)
                self.logger.info(
                    f"æ‰¹é‡çˆ¬å–å®Œæˆ: æ€»è®¡ {len(results)}, æˆåŠŸ {success_count}, "
                    f"MinIOä¸Šä¼ æˆåŠŸ {upload_success_count}"
                )

                return results

            finally:
                # ç¡®ä¿ä¸Šä¼ çº¿ç¨‹æ± å…³é—­
                upload_executor.shutdown(wait=True)
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                self._cleanup_temp_files(temp_csv, temp_fail_csv)

        except Exception as e:
            self.logger.error(f"å¤šè¿›ç¨‹æ‰¹é‡çˆ¬å–å¤±è´¥: {e}", exc_info=True)
            # é™çº§åˆ°å•ä»»åŠ¡æ¨¡å¼
            return super().crawl_batch(tasks)
    
    def _run_download_with_exception_handling(
        self,
        run_multiprocessing_func,
        temp_csv: str,
        temp_output: str,
        temp_fail_csv: str,
        exception_list: list
    ):
        """
        åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œä¸‹è½½ï¼Œæ•è·å¼‚å¸¸
        
        Args:
            run_multiprocessing_func: run_multiprocessing å‡½æ•°
            temp_csv: ä¸´æ—¶CSVæ–‡ä»¶è·¯å¾„
            temp_output: ä¸´æ—¶è¾“å‡ºç›®å½•
            temp_fail_csv: å¤±è´¥è®°å½•CSVè·¯å¾„
            exception_list: å¼‚å¸¸åˆ—è¡¨ï¼ˆç”¨äºä¼ é€’å¼‚å¸¸ï¼‰
        """
        try:
            run_multiprocessing_func(
                input_csv=temp_csv,
                out_root=temp_output,
                fail_csv=temp_fail_csv,
                workers=self.workers,
                debug=False
            )
        except Exception as e:
            self.logger.error(f"ä¸‹è½½çº¿ç¨‹å¼‚å¸¸: {e}", exc_info=True)
            exception_list.append(e)


def main():
    """æµ‹è¯•å…¥å£"""
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ReportCrawler(
        enable_minio=True,
        enable_postgres=True,
        workers=4
    )

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    task = CrawlTask(
        stock_code="000001",
        company_name="å¹³å®‰é“¶è¡Œ",
        market=Market.A_SHARE,
        doc_type=DocType.ANNUAL_REPORT,
        year=2023,
        quarter=4
    )

    # æ‰§è¡Œçˆ¬å–
    result = crawler.crawl(task)

    # æ‰“å°ç»“æœ
    if result.success:
        print(f"âœ… çˆ¬å–æˆåŠŸï¼š{result.local_file_path}")
        print(f"   MinIO: {result.minio_object_path}")
        print(f"   æ•°æ®åº“ID: {result.document_id}")
    else:
        print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{result.error_message}")


if __name__ == '__main__':
    main()
