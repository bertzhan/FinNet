# -*- coding: utf-8 -*-
"""
Aè‚¡IPOæ‹›è‚¡è¯´æ˜ä¹¦çˆ¬è™«å®ç°ï¼ˆCNINFOï¼‰
ç»§æ‰¿è‡ª CninfoBaseCrawlerï¼Œé›†æˆ storage å±‚
"""
import os
import tempfile
import csv
import multiprocessing
import queue
import threading
import time
from typing import List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor

# æ”¯æŒç›´æ¥è¿è¡Œå’Œä½œä¸ºæ¨¡å—å¯¼å…¥
try:
    from .base_cninfo_crawler import CninfoBaseCrawler
except ImportError:
    from src.ingestion.a_share.crawlers.base_cninfo_crawler import CninfoBaseCrawler

from src.ingestion.base.base_crawler import CrawlTask, CrawlResult
from src.common.constants import Market, DocType
from src.common.logger import get_logger

logger = get_logger(__name__)


class CninfoIPOProspectusCrawler(CninfoBaseCrawler):
    """
    Aè‚¡IPOæ‹›è‚¡è¯´æ˜ä¹¦çˆ¬è™«å®ç°ï¼ˆCNINFO å·¨æ½®èµ„è®¯ç½‘ï¼‰

    ä½¿ç”¨æœ¬åœ°æ¨¡å—åŒ–çš„ä¸‹è½½é€»è¾‘
    å®ç° _download_file() æ–¹æ³•ï¼Œå…¶ä½™ç”±åŸºç±»è‡ªåŠ¨å¤„ç†
    """

    def __init__(
        self,
        enable_minio: bool = True,
        enable_postgres: bool = True,
        workers: int = 1
    ):
        """
        Args:
            enable_minio: æ˜¯å¦å¯ç”¨ MinIO ä¸Šä¼ 
            enable_postgres: æ˜¯å¦å¯ç”¨ PostgreSQL è®°å½•
            workers: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆ1 è¡¨ç¤ºå•çº¿ç¨‹ï¼‰
        """
        super().__init__(
            market=Market.A_SHARE,
            enable_minio=enable_minio,
            enable_postgres=enable_postgres
        )
        self.workers = workers

    def _download_file(self, task: CrawlTask) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        ä¸‹è½½æ–‡ä»¶ï¼ˆIPOæ‹›è‚¡è¯´æ˜ä¹¦ï¼‰

        Args:
            task: çˆ¬å–ä»»åŠ¡ï¼ˆIPOä¸éœ€è¦yearå’Œquarterï¼Œä½†CrawlTaskè¦æ±‚ï¼Œå¯ä»¥ä¼ å…¥ä»»æ„å€¼ï¼‰

        Returns:
            (æ˜¯å¦æˆåŠŸ, æœ¬åœ°æ–‡ä»¶è·¯å¾„, é”™è¯¯ä¿¡æ¯)
        """
        try:
            # å¯¼å…¥ä»»åŠ¡å¤„ç†å™¨æ¨¡å—
            try:
                from ..processor.ipo_processor import process_single_ipo_task
            except ImportError:
                from src.ingestion.a_share.processor.ipo_processor import process_single_ipo_task

            # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºä¸‹è½½
            temp_dir = tempfile.mkdtemp(prefix='cninfo_ipo_download_')

            # å‡†å¤‡ä»»åŠ¡æ•°æ®
            # IPOä»»åŠ¡æ ¼å¼ï¼š(code, name, out_root, checkpoint_file, orgid_cache_file)
            shared_lock = multiprocessing.Lock()

            # orgIdç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨åŸºç±»æ–¹æ³•ï¼‰
            orgid_cache_file = self._get_cache_file_path('orgid_cache_ipo.json')

            # checkpointæ–‡ä»¶è·¯å¾„
            checkpoint_file = os.path.join(temp_dir, 'checkpoint_ipo.json')

            task_data = (
                task.stock_code,
                task.company_name,
                temp_dir,
                checkpoint_file,
                orgid_cache_file
            )

            # è°ƒç”¨ä¸‹è½½é€»è¾‘
            success, failure_record = process_single_ipo_task(task_data)

            if not success:
                error_msg = failure_record[2] if failure_record and len(failure_record) > 2 else "ä¸‹è½½å¤±è´¥"
                self.logger.error(f"ä¸‹è½½å¤±è´¥: {task.stock_code} - {error_msg}")
                return False, None, error_msg

            # æŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶
            file_path = self._find_downloaded_file(temp_dir, task)

            if not file_path:
                error_msg = "æ–‡ä»¶ä¸‹è½½æˆåŠŸä½†æœªæ‰¾åˆ°"
                self.logger.error(f"{error_msg}: {task.stock_code}")
                return False, None, error_msg

            # å¦‚æœæ˜¯HTMLæ–‡ä»¶ï¼ŒéªŒè¯å†…å®¹
            if file_path.lower().endswith(('.html', '.htm')):
                is_valid, error_reason = self._validate_html_file(file_path)
                if not is_valid:
                    self.logger.warning(f"HTMLæ–‡ä»¶éªŒè¯å¤±è´¥: {task.stock_code} - {error_reason}")
                    return False, None, f"HTMLæ–‡ä»¶éªŒè¯å¤±è´¥: {error_reason}"

            # è¯»å–å‘å¸ƒæ—¥æœŸä¿¡æ¯å’ŒURLï¼ˆä»metadataæ–‡ä»¶ï¼‰
            metadata_file = file_path.replace('.pdf', '.meta.json').replace('.html', '.meta.json')
            if os.path.exists(metadata_file):
                try:
                    from ..utils.file_utils import load_json
                    metadata_info = load_json(metadata_file, {})
                    # å°†å‘å¸ƒæ—¥æœŸä¿¡æ¯å’ŒURLæ·»åŠ åˆ°task.metadata
                    task.metadata.update({
                        'publication_date': metadata_info.get('publication_date', ''),
                        'publication_year': metadata_info.get('publication_year', ''),
                        'publication_date_iso': metadata_info.get('publication_date_iso', ''),
                        'source_url': metadata_info.get('source_url', '')  # æ–‡æ¡£æ¥æºURL
                    })
                except Exception as e:
                    self.logger.warning(f"è¯»å–metadataæ–‡ä»¶å¤±è´¥: {e}")

            self.logger.info(f"ä¸‹è½½æˆåŠŸ: {file_path}")
            return True, file_path, None

        except Exception as e:
            error_msg = f"ä¸‹è½½å¼‚å¸¸: {e}"
            self.logger.error(f"{error_msg}: {task.stock_code}", exc_info=True)
            return False, None, error_msg

    def _find_downloaded_file(self, output_root: str, task: CrawlTask) -> Optional[str]:
        """
        æŸ¥æ‰¾ä¸‹è½½çš„æ–‡ä»¶ï¼ˆIPOç‰ˆæœ¬ï¼‰

        Args:
            output_root: è¾“å‡ºæ ¹ç›®å½•
            task: ä»»åŠ¡

        Returns:
            æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› None
        """
        # IPOæ–‡ä»¶æ ¼å¼ï¼ˆä¸MinIOä¸€è‡´ï¼‰ï¼š{output_root}/bronze/a_share/ipo_prospectus/{code}/document.pdf æˆ– document.html
        from src.storage.object_store.path_manager import PathManager
        from src.common.constants import Market, DocType
        
        path_manager = PathManager()
        
        # æ„å»ºIPOç›®å½•è·¯å¾„ï¼ˆä¸åŒ…å«æ–‡ä»¶åï¼‰
        ipo_dir = os.path.join(
            output_root,
            "bronze",
            Market.A_SHARE.value,
            DocType.IPO_PROSPECTUS.value,
            task.stock_code
        )
        
        if not os.path.exists(ipo_dir):
            return None

        import glob
        # IPOæ–‡ä»¶å‘½åï¼šç»Ÿä¸€ä¸º document.pdf æˆ– document.html
        # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼šdocument.pdf æˆ– document.html
        exact_pdf = os.path.join(ipo_dir, "document.pdf")
        if os.path.exists(exact_pdf):
            return exact_pdf
        
        exact_html = os.path.join(ipo_dir, "document.html")
        if os.path.exists(exact_html):
            return exact_html
        
        # å…¼å®¹æ—§æ ¼å¼ï¼š{code}_*.pdf æˆ– {code}_*.htmlï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        pattern = f"{task.stock_code}_*.pdf"
        pdf_files = glob.glob(os.path.join(ipo_dir, pattern))
        if pdf_files:
            return pdf_files[0]  # è¿”å›æœ€æ–°çš„PDFæ–‡ä»¶

        # ä¹Ÿæ£€æŸ¥HTMLæ–‡ä»¶
        pattern = f"{task.stock_code}_*.html"
        html_files = glob.glob(os.path.join(ipo_dir, pattern))
        if html_files:
            return html_files[0]  # è¿”å›æœ€æ–°çš„HTMLæ–‡ä»¶

        return None

    def _validate_html_file(self, file_path: str) -> Tuple[bool, Optional[str]]:
        """
        éªŒè¯HTMLæ–‡ä»¶æ˜¯å¦åŒ…å«æ‹›è‚¡è¯´æ˜ä¹¦ç›¸å…³å†…å®¹
        
        Args:
            file_path: HTMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯åŸå› )
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆ< 1KB è§†ä¸ºæ— æ•ˆï¼‰
            file_size = os.path.getsize(file_path)
            if file_size < 1024:  # 1KB
                return False, f"æ–‡ä»¶è¿‡å° ({file_size} bytes)ï¼Œå¯èƒ½æ˜¯ç©ºæ–‡ä»¶æˆ–é”™è¯¯é¡µé¢"
            
            # è¯»å–HTMLå†…å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()
            except UnicodeDecodeError:
                # å°è¯•å…¶ä»–ç¼–ç 
                try:
                    with open(file_path, 'r', encoding='gb2312') as f:
                        html_content = f.read()
                except:
                    with open(file_path, 'r', encoding='gbk') as f:
                        html_content = f.read()
            
            # ä½¿ç”¨BeautifulSoupæå–æ–‡æœ¬å†…å®¹
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
                for element in soup(["script", "style", "meta", "link", "noscript", "comment"]):
                    element.decompose()
                
                # æå–æ–‡æœ¬å†…å®¹
                text_content = soup.get_text(separator=' ', strip=True)
                
                # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼ˆ< 100 å­—ç¬¦è§†ä¸ºæ— æ•ˆï¼‰
                if len(text_content) < 100:
                    return False, f"æå–çš„æ–‡æœ¬å†…å®¹è¿‡çŸ­ ({len(text_content)} å­—ç¬¦)ï¼Œå¯èƒ½æ˜¯å¯¼èˆªé¡µé¢æˆ–åˆ—è¡¨é¡µé¢"
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«IPOå…³é”®è¯
                from ..config import IPO_KEYWORDS
                text_lower = text_content.lower()
                has_keyword = any(keyword in text_content or keyword in text_lower for keyword in IPO_KEYWORDS)
                
                if not has_keyword:
                    return False, f"HTMLå†…å®¹ä¸­æœªæ‰¾åˆ°æ‹›è‚¡è¯´æ˜ä¹¦å…³é”®è¯ ({', '.join(IPO_KEYWORDS)})"
                
                return True, None
                
            except Exception as e:
                self.logger.warning(f"è§£æHTMLæ–‡ä»¶å¤±è´¥: {file_path} - {e}")
                return False, f"è§£æHTMLæ–‡ä»¶å¤±è´¥: {str(e)}"
                
        except Exception as e:
            self.logger.error(f"éªŒè¯HTMLæ–‡ä»¶æ—¶å‡ºé”™: {file_path} - {e}", exc_info=True)
            return False, f"éªŒè¯HTMLæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"

    def crawl_batch(self, tasks: List[CrawlTask]) -> List[CrawlResult]:
        """
        æ‰¹é‡çˆ¬å–

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            ç»“æœåˆ—è¡¨
        """
        if not tasks:
            return []

        # å¦‚æœåªæœ‰ä¸€ä¸ªä»»åŠ¡æˆ–ä¸å¯ç”¨å¤šè¿›ç¨‹ï¼Œä½¿ç”¨åŸºç±»çš„å•ä»»åŠ¡é€»è¾‘
        if len(tasks) == 1 or self.workers <= 1:
            return super().crawl_batch(tasks)

        # å¤šä»»åŠ¡ + å¤šè¿›ç¨‹ï¼šä½¿ç”¨ç°æœ‰çš„ run_ipo_multiprocessing
        return self._crawl_batch_multiprocessing(tasks)

    def _crawl_batch_multiprocessing(self, tasks: List[CrawlTask]) -> List[CrawlResult]:
        """
        ä½¿ç”¨å¤šè¿›ç¨‹æ‰¹é‡çˆ¬å– + ä¸»çº¿ç¨‹å¼‚æ­¥ä¸Šä¼ ï¼ˆIPOç‰ˆæœ¬ï¼‰
        
        ä¼˜åŒ–ï¼šä¸‹è½½å’Œä¸Šä¼ å¹¶è¡Œè¿›è¡Œï¼Œè€Œä¸æ˜¯ä¸²è¡Œç­‰å¾…

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨

        Returns:
            ç»“æœåˆ—è¡¨
        """
        try:
            # å¯¼å…¥ä»»åŠ¡å¤„ç†å™¨æ¨¡å—
            try:
                from ..processor.ipo_processor import run_ipo_multiprocessing
            except ImportError:
                from src.ingestion.a_share.processor.ipo_processor import run_ipo_multiprocessing

            # 0. æ£€æŸ¥æ•°æ®åº“ï¼Œè¿‡æ»¤å·²å­˜åœ¨çš„ä»»åŠ¡ï¼ˆé¿å…é‡å¤çˆ¬å–ï¼‰
            tasks_to_crawl = []
            skipped_results = {}  # {stock_code: CrawlResult}
            
            if self.enable_postgres and self.pg_client:
                try:
                    from src.storage.metadata import crud
                    with self.pg_client.get_session() as session:
                        for task in tasks:
                            existing_doc = crud.get_document_by_task(
                                session=session,
                                stock_code=task.stock_code,
                                market=task.market.value,
                                doc_type=task.doc_type.value,
                                year=task.year,
                                quarter=task.quarter
                            )
                            
                            if existing_doc:
                                # æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½
                                self.logger.info(
                                    f"âœ… æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½ï¼ˆIPOï¼‰: {task.stock_code} "
                                    f"(id={existing_doc.id}, path={existing_doc.minio_object_path})"
                                )
                                skipped_results[task.stock_code] = CrawlResult(
                                    task=task,
                                    success=True,
                                    minio_object_path=existing_doc.minio_object_path,
                                    document_id=existing_doc.id,
                                    file_size=existing_doc.file_size,
                                    file_hash=existing_doc.file_hash,
                                    metadata=task.metadata
                                )
                            else:
                                # éœ€è¦çˆ¬å–çš„ä»»åŠ¡
                                tasks_to_crawl.append(task)
                except Exception as e:
                    # æ£€æŸ¥å¤±è´¥ä¸å½±å“çˆ¬å–æµç¨‹ï¼Œè®°å½•è­¦å‘Šåç»§ç»­
                    self.logger.warning(f"âš ï¸ æ£€æŸ¥æ•°æ®åº“æ—¶å‘ç”Ÿå¼‚å¸¸ï¼Œç»§ç»­çˆ¬å–æ‰€æœ‰ä»»åŠ¡: {e}")
                    tasks_to_crawl = tasks
            else:
                # æœªå¯ç”¨ PostgreSQLï¼Œçˆ¬å–æ‰€æœ‰ä»»åŠ¡
                tasks_to_crawl = tasks
            
            if not tasks_to_crawl:
                # æ‰€æœ‰ä»»åŠ¡éƒ½å·²å­˜åœ¨
                self.logger.info(f"âœ… æ‰€æœ‰ {len(tasks)} ä¸ªIPOä»»åŠ¡éƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–")
                return list(skipped_results.values())
            
            if len(skipped_results) > 0:
                self.logger.info(f"è·³è¿‡ {len(skipped_results)} ä¸ªå·²å­˜åœ¨çš„ä»»åŠ¡ï¼Œå‰©ä½™ {len(tasks_to_crawl)} ä¸ªä»»åŠ¡éœ€è¦çˆ¬å–")

            # åˆ›å»ºä¸´æ—¶CSVæ–‡ä»¶ï¼ˆåªåŒ…å«éœ€è¦çˆ¬å–çš„ä»»åŠ¡ï¼‰
            temp_csv = self._create_temp_csv(
                tasks_to_crawl,
                ['code', 'name'],
                lambda task: [task.stock_code, task.company_name]
            )

            # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•å’Œå¤±è´¥è®°å½•æ–‡ä»¶
            temp_output = tempfile.mkdtemp(prefix='cninfo_ipo_batch_')
            temp_fail_csv = tempfile.NamedTemporaryFile(
                mode='w', suffix='.csv', delete=False, encoding='utf-8-sig', newline=''
            ).name

            # åˆ›å»ºä¸Šä¼ é˜Ÿåˆ—å’Œç»“æœå­—å…¸
            upload_results = {}  # {stock_code: CrawlResult}
            upload_lock = threading.Lock()
            processed_files = set()  # å·²å¤„ç†çš„æ–‡ä»¶è·¯å¾„é›†åˆ
            
            # åˆ›å»ºä¸Šä¼ çº¿ç¨‹æ± 
            upload_workers = min(4, max(1, len(tasks) // 10))
            self.logger.info(f"å¯åŠ¨ {upload_workers} ä¸ªä¸Šä¼ çº¿ç¨‹ï¼ˆIPOï¼‰")
            
            upload_executor = ThreadPoolExecutor(max_workers=upload_workers)
            upload_futures = {}  # {stock_code: future}
            
            def upload_file(task: CrawlTask, file_path: str):
                """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
                try:
                    # å¦‚æœæ˜¯HTMLæ–‡ä»¶ï¼Œå…ˆéªŒè¯å†…å®¹
                    if file_path.lower().endswith(('.html', '.htm')):
                        is_valid, error_reason = self._validate_html_file(file_path)
                        if not is_valid:
                            self.logger.warning(f"HTMLæ–‡ä»¶éªŒè¯å¤±è´¥ï¼ˆIPOï¼‰: {task.stock_code} - {error_reason}")
                            return CrawlResult(
                                task=task,
                                success=False,
                                error_message=f"HTMLæ–‡ä»¶éªŒè¯å¤±è´¥: {error_reason}"
                            )
                    
                    # è¯»å–å‘å¸ƒæ—¥æœŸä¿¡æ¯å’ŒURLï¼ˆä»metadataæ–‡ä»¶ï¼‰
                    metadata_file = file_path.replace('.pdf', '.meta.json').replace('.html', '.meta.json')
                    if os.path.exists(metadata_file):
                        try:
                            from ..utils.file_utils import load_json
                            metadata_info = load_json(metadata_file, {})
                            # å°†å‘å¸ƒæ—¥æœŸä¿¡æ¯å’ŒURLæ·»åŠ åˆ°task.metadata
                            task.metadata.update({
                                'publication_date': metadata_info.get('publication_date', ''),
                                'publication_year': metadata_info.get('publication_year', ''),
                                'publication_date_iso': metadata_info.get('publication_date_iso', ''),
                                'source_url': metadata_info.get('source_url', '')
                            })
                        except Exception as e:
                            self.logger.warning(f"è¯»å–metadataæ–‡ä»¶å¤±è´¥: {e}")
                    
                    self.logger.debug(f"å¼€å§‹ä¸Šä¼ ï¼ˆIPOï¼‰: {task.stock_code}")
                    result = self._process_downloaded_file(
                        file_path, 
                        task, 
                        extract_year_from_filename=True
                    )
                    return result
                except Exception as e:
                    self.logger.error(f"ä¸Šä¼ å¼‚å¸¸ï¼ˆIPOï¼‰: {task.stock_code} - {e}", exc_info=True)
                    return CrawlResult(
                        task=task,
                        success=False,
                        error_message=f"ä¸Šä¼ å¼‚å¸¸: {e}"
                    )
            
            try:
                # å¯åŠ¨ä¸‹è½½ï¼ˆåœ¨åå°çº¿ç¨‹ï¼‰
                download_exception = []
                download_thread = threading.Thread(
                    target=lambda: self._run_download_with_exception_handling(
                        run_ipo_multiprocessing,
                        temp_csv,
                        temp_output,
                        temp_fail_csv,
                        download_exception
                    ),
                    daemon=False
                )
                download_thread.start()
                self.logger.info(f"ä¸‹è½½çº¿ç¨‹å·²å¯åŠ¨ï¼ˆIPOï¼‰ï¼Œworkers={self.workers}")
                
                # ä¸»çº¿ç¨‹è½®è¯¢æ£€æŸ¥ä¸‹è½½å®Œæˆçš„æ–‡ä»¶å¹¶å¼‚æ­¥ä¸Šä¼ 
                poll_interval = 0.5
                last_file_count = 0
                no_new_file_count = 0
                max_no_new_file_count = 10
                
                while download_thread.is_alive() or no_new_file_count < max_no_new_file_count:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹è½½å¼‚å¸¸
                    if download_exception:
                        raise download_exception[0]
                    
                    # æ£€æŸ¥æ–°ä¸‹è½½çš„æ–‡ä»¶
                    current_file_count = 0
                    for task in tasks_to_crawl:
                        stock_code = task.stock_code
                        
                        # è·³è¿‡å·²å¤„ç†çš„ä»»åŠ¡
                        if stock_code in upload_results or stock_code in upload_futures:
                            current_file_count += 1
                            continue
                        
                        # æŸ¥æ‰¾æ–‡ä»¶
                        file_path = self._find_downloaded_file(temp_output, task)
                        if file_path and os.path.exists(file_path) and file_path not in processed_files:
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸‹è½½å®Œæˆï¼ˆæ–‡ä»¶å¤§å°ç¨³å®šï¼‰
                            if self._is_file_stable(file_path):
                                # æ–‡ä»¶å·²ä¸‹è½½å®Œæˆï¼Œæäº¤åˆ°ä¸Šä¼ çº¿ç¨‹æ± 
                                processed_files.add(file_path)
                                future = upload_executor.submit(upload_file, task, file_path)
                                upload_futures[stock_code] = future
                                current_file_count += 1
                                self.logger.debug(f"å‘ç°æ–°æ–‡ä»¶ï¼Œå·²æäº¤ä¸Šä¼ ï¼ˆIPOï¼‰: {stock_code}")
                            # å¦‚æœæ–‡ä»¶ä¸ç¨³å®šï¼Œç­‰å¾…ä¸‹æ¬¡è½®è¯¢å†æ£€æŸ¥
                    
                    # æ£€æŸ¥å·²å®Œæˆçš„ä¸Šä¼ 
                    completed_futures = []
                    for stock_code, future in upload_futures.items():
                        if future.done():
                            try:
                                result = future.result()
                                with upload_lock:
                                    upload_results[stock_code] = result
                                completed_futures.append(stock_code)
                                if result.success:
                                    self.logger.debug(f"ä¸Šä¼ å®Œæˆï¼ˆIPOï¼‰: {stock_code}")
                                else:
                                    self.logger.warning(f"ä¸Šä¼ å¤±è´¥ï¼ˆIPOï¼‰: {stock_code} - {result.error_message}")
                            except Exception as e:
                                self.logger.error(f"è·å–ä¸Šä¼ ç»“æœå¼‚å¸¸ï¼ˆIPOï¼‰: {stock_code} - {e}", exc_info=True)
                                with upload_lock:
                                    upload_results[stock_code] = CrawlResult(
                                        task=next(t for t in tasks_to_crawl if t.stock_code == stock_code),
                                        success=False,
                                        error_message=f"ä¸Šä¼ å¼‚å¸¸: {e}"
                                    )
                                completed_futures.append(stock_code)
                    
                    # æ¸…ç†å·²å®Œæˆçš„ä¸Šä¼ ä»»åŠ¡
                    for stock_code in completed_futures:
                        del upload_futures[stock_code]
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ–‡ä»¶
                    if current_file_count > last_file_count:
                        no_new_file_count = 0
                        last_file_count = current_file_count
                    else:
                        no_new_file_count += 1

                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ä¸ªä»»åŠ¡æˆ–æ¯æ¬¡æœ‰æ–°æ–‡ä»¶æ—¶æ˜¾ç¤ºï¼‰
                    completed_count = len(upload_results)
                    total_count = len(tasks_to_crawl)
                    if current_file_count > last_file_count or completed_count % 10 == 0:
                        progress_pct = completed_count / total_count * 100 if total_count > 0 else 0
                        self.logger.info(
                            f"ğŸ“¦ è¿›åº¦ (IPO): å·²å®Œæˆ {completed_count}/{total_count} ({progress_pct:.1f}%) | "
                            f"ä¸Šä¼ ä¸­: {len(upload_futures)}"
                        )

                    # ç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥
                    time.sleep(poll_interval)
                
                # ç­‰å¾…ä¸‹è½½çº¿ç¨‹å®Œæˆ
                download_thread.join(timeout=300)
                if download_thread.is_alive():
                    self.logger.warning("ä¸‹è½½çº¿ç¨‹è¶…æ—¶ï¼ˆIPOï¼‰ï¼Œç»§ç»­å¤„ç†å·²ä¸‹è½½çš„æ–‡ä»¶")
                
                # ç­‰å¾…æ‰€æœ‰ä¸Šä¼ å®Œæˆ
                self.logger.info(f"ç­‰å¾… {len(upload_futures)} ä¸ªä¸Šä¼ ä»»åŠ¡å®Œæˆï¼ˆIPOï¼‰...")
                for stock_code, future in upload_futures.items():
                    try:
                        result = future.result(timeout=60)
                        with upload_lock:
                            upload_results[stock_code] = result
                    except Exception as e:
                        self.logger.error(f"ç­‰å¾…ä¸Šä¼ å®Œæˆå¼‚å¸¸ï¼ˆIPOï¼‰: {stock_code} - {e}", exc_info=True)
                        with upload_lock:
                            upload_results[stock_code] = CrawlResult(
                                task=next(t for t in tasks_to_crawl if t.stock_code == stock_code),
                                success=False,
                                error_message=f"ä¸Šä¼ è¶…æ—¶æˆ–å¼‚å¸¸: {e}"
                            )
                
                # å…³é—­ä¸Šä¼ çº¿ç¨‹æ± 
                upload_executor.shutdown(wait=True)
                
                # è¯»å–å¤±è´¥è®°å½•
                failed_tasks = set()
                if os.path.exists(temp_fail_csv):
                    failed_tasks = self._read_failed_tasks(
                        temp_fail_csv,
                        lambda row: row.get('code', '').strip()
                    )

                # æ„å»ºç»“æœåˆ—è¡¨ï¼ˆåŒ…å«è·³è¿‡çš„ä»»åŠ¡ï¼‰
                results = []
                for task in tasks:
                    stock_code = task.stock_code
                    
                    # å¦‚æœä»»åŠ¡è¢«è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰ï¼Œç›´æ¥æ·»åŠ è·³è¿‡ç»“æœ
                    if stock_code in skipped_results:
                        results.append(skipped_results[stock_code])
                    elif stock_code in failed_tasks:
                        # å¤±è´¥ä»»åŠ¡
                        results.append(CrawlResult(
                            task=task,
                            success=False,
                            error_message="çˆ¬å–å¤±è´¥ï¼Œè¯¦è§å¤±è´¥è®°å½•"
                        ))
                    elif stock_code in upload_results:
                        # å·²ä¸Šä¼ çš„ä»»åŠ¡
                        results.append(upload_results[stock_code])
                    else:
                        # æœªæ‰¾åˆ°æ–‡ä»¶æˆ–æœªå¤„ç†
                        file_path = self._find_downloaded_file(temp_output, task)
                        if file_path and os.path.exists(file_path):
                            # å¦‚æœæ˜¯HTMLæ–‡ä»¶ï¼Œå…ˆéªŒè¯å†…å®¹
                            if file_path.lower().endswith(('.html', '.htm')):
                                is_valid, error_reason = self._validate_html_file(file_path)
                                if not is_valid:
                                    self.logger.warning(f"HTMLæ–‡ä»¶éªŒè¯å¤±è´¥ï¼ˆé—æ¼æ–‡ä»¶ï¼ŒIPOï¼‰: {stock_code} - {error_reason}")
                                    results.append(CrawlResult(
                                        task=task,
                                        success=False,
                                        error_message=f"HTMLæ–‡ä»¶éªŒè¯å¤±è´¥: {error_reason}"
                                    ))
                                    continue
                            
                            # æ–‡ä»¶å­˜åœ¨ä½†æœªä¸Šä¼ ï¼Œç«‹å³ä¸Šä¼ 
                            self.logger.warning(f"å‘ç°é—æ¼çš„æ–‡ä»¶ï¼Œç«‹å³ä¸Šä¼ ï¼ˆIPOï¼‰: {stock_code}")
                            # è¯»å–metadata
                            metadata_file = file_path.replace('.pdf', '.meta.json').replace('.html', '.meta.json')
                            if os.path.exists(metadata_file):
                                try:
                                    from ..utils.file_utils import load_json
                                    metadata_info = load_json(metadata_file, {})
                                    task.metadata.update({
                                        'publication_date': metadata_info.get('publication_date', ''),
                                        'publication_year': metadata_info.get('publication_year', ''),
                                        'publication_date_iso': metadata_info.get('publication_date_iso', ''),
                                        'source_url': metadata_info.get('source_url', '')
                                    })
                                except Exception as e:
                                    self.logger.warning(f"è¯»å–metadataæ–‡ä»¶å¤±è´¥: {e}")
                            
                            temp_result = self._process_downloaded_file(
                                file_path, 
                                task, 
                                extract_year_from_filename=True
                            )
                            results.append(temp_result)
                        else:
                            results.append(CrawlResult(
                                task=task,
                                success=False,
                                error_message="æ–‡ä»¶æœªæ‰¾åˆ°æˆ–æœªå¤„ç†"
                            ))

                # ç»Ÿè®¡ç»“æœ
                success_count = sum(1 for r in results if r.success)
                upload_success_count = sum(1 for r in results if r.success and r.minio_object_path)
                self.logger.info(
                    f"æ‰¹é‡çˆ¬å–å®Œæˆï¼ˆIPOï¼‰: æ€»è®¡ {len(results)}, æˆåŠŸ {success_count}, "
                    f"MinIOä¸Šä¼ æˆåŠŸ {upload_success_count}"
                )

                return results

            finally:
                # ç¡®ä¿ä¸Šä¼ çº¿ç¨‹æ± å…³é—­
                upload_executor.shutdown(wait=True)
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                self._cleanup_temp_files(temp_csv, temp_fail_csv)

        except Exception as e:
            self.logger.error(f"å¤šè¿›ç¨‹æ‰¹é‡çˆ¬å–å¤±è´¥ï¼ˆIPOï¼‰: {e}", exc_info=True)
            # é™çº§åˆ°å•ä»»åŠ¡æ¨¡å¼
            return super().crawl_batch(tasks)
    
    def _run_download_with_exception_handling(
        self,
        run_multiprocessing_func,
        temp_csv: str,
        temp_output: str,
        temp_fail_csv: str,
        exception_list: list
    ):
        """
        åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œä¸‹è½½ï¼Œæ•è·å¼‚å¸¸ï¼ˆIPOç‰ˆæœ¬ï¼‰
        
        Args:
            run_multiprocessing_func: run_ipo_multiprocessing å‡½æ•°
            temp_csv: ä¸´æ—¶CSVæ–‡ä»¶è·¯å¾„
            temp_output: ä¸´æ—¶è¾“å‡ºç›®å½•
            temp_fail_csv: å¤±è´¥è®°å½•CSVè·¯å¾„
            exception_list: å¼‚å¸¸åˆ—è¡¨ï¼ˆç”¨äºä¼ é€’å¼‚å¸¸ï¼‰
        """
        try:
            run_multiprocessing_func(
                input_csv=temp_csv,
                out_root=temp_output,
                fail_csv=temp_fail_csv,
                workers=self.workers,
                debug=False
            )
        except Exception as e:
            self.logger.error(f"ä¸‹è½½çº¿ç¨‹å¼‚å¸¸ï¼ˆIPOï¼‰: {e}", exc_info=True)
            exception_list.append(e)


def main():
    """æµ‹è¯•å…¥å£"""
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = CninfoIPOProspectusCrawler(
        enable_minio=True,
        enable_postgres=True,
        workers=4
    )

    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡ï¼ˆIPOä¸éœ€è¦yearå’Œquarterï¼‰
    task = CrawlTask(
        stock_code="688111",
        company_name="é‡‘å±±åŠå…¬",
        market=Market.A_SHARE,
        doc_type=DocType.IPO_PROSPECTUS,
        year=None,
        quarter=None
    )

    # æ‰§è¡Œçˆ¬å–
    result = crawler.crawl(task)

    # æ‰“å°ç»“æœ
    if result.success:
        print(f"âœ… çˆ¬å–æˆåŠŸï¼š{result.local_file_path}")
        print(f"   MinIO: {result.minio_object_path}")
        print(f"   æ•°æ®åº“ID: {result.document_id}")
    else:
        print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{result.error_message}")


if __name__ == '__main__':
    main()
